{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA\n",
    "\n",
    "To load data of different document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOAD TEXT DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data/speech.txt'}, page_content=\"Hello!\\nMy name is Toheebat, and I am excited to be a learner for this program, Generative AI.\\nI am a Data Scientist, aspiring Open-source Contributor, and Generative AI enthuisast with hands-on-project in the tech industry. For my career, Iâ€™ve looking foorward to build innovative products, solve real-world problems, and empower over 5,000 individuals to grow into better versions of themselves. My work and insights will also connect me with an audience of over 10,000 followers across social media platforms.\\nLooking ahead, my vision is to create groundbreaking projects and support millionsâ€”if not billionsâ€”of people in achieving their goals and thriving in the tech industry.\\nIn this program, I am here to be guided of every step I take on the way, ensuring I have an unforgettable learning experience.\\nI'm embarking on this transformative journey with my peers!\")]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textload = TextLoader(\"./data/speech.txt\")\n",
    "textdoc = textload.load()\n",
    "textdoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOAD PDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data/GenAI Assignmnet.pdf', 'page': 0, 'page_label': '1'}, page_content=' PART 1 ASSIGNMENT \\n 1.  Artificial intelligence is a system or machine that mimics human intelligence to \\n perform tasks and can improve themselves based on the information they collect. \\n 2.  Machine Learning focuses on training machines to learn patterns from data \\n without explicit programming while deep learning uses neural networks with \\n many layers to learn patterns from data. \\n 3.  Supervised learning means learning with labeled data (e.g., classifying emails as \\n spam or not spam), Supervised learning means Learning without labels (e.g., \\n grouping similar customers in marketing) while reinforcement learning means \\n Learning through trial and error (e.g., game-playing AIs like AlphaGo). \\n 4.  Computer Vision: Deep learning is widely used for image recognition, object \\n detection, face recognition, and medical image analysis. \\n 5.  Discriminative models distinguish different classes or labels in the data, while \\n generative models not only classify the data but also model how the data is \\n generated. ')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfloader = PyPDFLoader(\"./data/GenAI Assignmnet.pdf\")\n",
    "pdfdoc = pdfloader.load()\n",
    "pdfdoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOAD WEB DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "webloader = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\", ),\n",
    "                          bs_kwargs=dict(parse_only = bs4.SoupStrainer(\n",
    "                                    class_ =\"post-title\"\n",
    "                                    )))\n",
    "                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2024-11-28-reward-hacking/'}, page_content='\\n      Reward Hacking in Reinforcement Learning\\n    ')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webdoc = webloader.load()\n",
    "webdoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2024-11-28-reward-hacking/'}, page_content='Reward hacking occurs when a reinforcement learning (RL) agent exploits flaws or ambiguities in the reward function to achieve high rewards, without genuinely learning or completing the intended task. Reward hacking exists because RL environments are often imperfect, and it is fundamentally challenging to accurately specify a reward function.\\nWith the rise of language models generalizing to a broad spectrum of tasks and RLHF becomes a de facto method for alignment training, reward hacking in RL training of language models has become a critical practical challenge. Instances where the model learns to modify unit tests to pass coding tasks, or where responses contain biases that mimic a user’s preference, are pretty concerning and are likely one of the major blockers for real-world deployment of more autonomous use cases of AI models.\\nMost of the past work on this topic has been quite theoretical and focused on defining or demonstrating the existence of reward hacking. However, research into practical mitigations, especially in the context of RLHF and LLMs, remains limited. I especially want to call out for more research efforts directed toward understanding and developing mitigation for reward hacking in the future. Hope I will be able to cover the mitigation part in a dedicated post soon.\\nBackground#\\nReward Function in RL#\\nReward function defines the task, and reward shaping significantly impacts learning efficiency and accuracy in reinforcement learning. Designing a reward function for an RL task often feels like a ‘dark art’. Many factors contribute to this complexity: How you decompose a big goal into small goals? Is the reward sparse or dense? How you measure the success? Various choices may lead to good or problematic learning dynamics, including unlearnable tasks or hackable reward functions. There is a long history of research on how to do reward shaping in RL.\\nFor example, in an 1999 paper by Ng et al., the authors studied how to modify the reward function in Markov Decision Processes (MDPs) such that the optimal policy remains unchanged. They found that linear transformation works. Given a MDP $M = (S, A, T, \\\\gamma, R)$, we want to create a transformed MDP $M’ = (S, A, T, \\\\gamma, R’)$ where $R’ = R + F$ and $F: S \\\\times A \\\\times S \\\\mapsto \\\\mathbb{R}$, such that we can guide the learning algorithm to be more efficient. Given a real-valued function $\\\\Phi: S \\\\mapsto \\\\mathbb{R}$, $F$ is a potential-based shaping function if for all $s \\\\in S - {s_0}, a \\\\in A, s’ \\\\in S$:\\n\\n$$\\nF(s, a, s\\') = \\\\gamma \\\\Phi(s\\') - \\\\Phi(s)\\n$$\\n\\nThis would guarantee that the sum of discounted $F$, $F(s_1, a_1, s_2) + \\\\gamma F(s_2, a_2, s_3) + \\\\dots$, ends up being 0. If $F$ is such a potential-based shaping function, it is both sufficient and necessary to ensure $M$ and $M’$ share the same optimal policies.\\nWhen $F(s, a, s’) = \\\\gamma \\\\Phi(s’) - \\\\Phi(s)$, and if we further assume that $\\\\Phi(s_0) = 0$, where $s_0$ is absorbing state, and $\\\\gamma=1$, and then for all $s \\\\in S, a \\\\in A$:\\n\\n$$\\n\\\\begin{aligned}\\nQ^*_{M\\'} (s,a) &= Q^*_M(s, a) - \\\\Phi(s) \\\\\\\\\\nV^*_{M\\'} (s,a) &= V^*_M(s, a) - \\\\Phi(s)\\n\\\\end{aligned}\\n$$\\n\\nThis form of reward shaping allows us to incorporate heuristics into the reward function to speed up learning without impacting the optimal policy.\\nSpurious Correlation#\\nSpurious correlation or shortcut learning (Geirhos et al. 2020) in classification task is a concept closely related to reward hacking. Spurious or shortcut features can cause a classifier to fail at learning and generalizing as intended. For example, a binary classifier for distinguishing wolves from huskies may overfit to the presence of a snowy background if all the wolf training images include snow (Ribeiro et al. 2024).\\n\\nFig. 1. The model performs poorly on out-of-distribution (OOD) test sets if it overfits to shortcut features. (Image source: Geirhos et al. 2020)\\nThe ERM principle states that, since the full data distribution is unknown, minimizing the loss on training data is a reasonable proxy of risk and thus we favor models with the lowest training loss. Nagarajan et al. (2021) studied the ERM principle and pointed out that ERM needs to rely on all types of informative features, including unreliable spurious features, while attempting to fit the data without constraints. Their experiments showed that ERM would depend on spurious features no matter how easy the task is.\\nLet’s Define Reward Hacking#\\nReward shaping in RL is challenging. Reward hacking occurs when an RL agent exploits flaws or ambiguities in the reward function to obtain high rewards without genuinely learning the intended behaviors or completing the task as designed. In recent years, several related concepts have been proposed, all referring to some form of reward hacking:\\n\\nReward hacking (Amodei et al., 2016)\\nReward corruption (Everitt et al., 2017)\\nReward tampering (Everitt et al. 2019)\\nSpecification gaming (Krakovna et al., 2020)\\nObjective robustness (Koch et al. 2021)\\nGoal misgeneralization (Langosco et al. 2022)\\nReward misspecifications (Pan et al. 2022)\\n\\nThe concept originated with Amodei et al. (2016), who proposed a set of open research questions on AI safety in their seminal paper “Concrete Problems in AI Safety”. They listed reward hacking as one of the key AI safety problems. Reward hacking refers to the possibility of the agent gaming the reward function to achieve high reward through undesired behavior.  Specification gaming (Krakovna et al. 2020) is a similar concept, defined as a behavior that satisfies the literal specification of an objective but not achieving the desired results. Here the literal description of the task goal and the intended goal may have a gap.\\nReward shaping is a technique used to enrich the reward function, making it easier for the agent to learn—for example, by providing denser rewards. However, a poorly design reward shaping mechanism can alter the trajectory of the optimal policy. Designing effective reward shaping mechanisms is inherently difficult. Rather than blaming a poorly designed reward function, it is more accurate to acknowledge that designing a good reward function is intrinsically challenging due to the complexity of the task itself, partial observable state, multiple dimensions in consideration, and other factors.\\nWhen testing an RL agent in out-of-distribution (OOD) environments, robustness failure may occur due to:\\n\\nThe model fails to generalize effectively, even with the right objective. This happens when the algorithm lacks sufficient intelligence or capability.\\nThe model generalizes capably but pursues an objective different from the one it was trained on. This happens when the proxy reward differs from the true reward function, $R’ \\\\neq R$. This is known as objective robustness (Koch et al. 2021) or goal misgeneralization (Langosco et al. 2022 )\\n\\nExperiments in two RL environments, CoinRun and Maze, demonstrated the importance of randomization during training. If during training, the coin or the cheese is placed at a fixed position (i.e. right end of the level or upper right corner of the maze) but testing in the env where the coin or cheese is placed at random, the agent would just run to the fixed position without obtaining the coin or cheese at test time. A conflict arises when a visual feature (e.g., cheese or coin) and a positional feature (e.g., upper-right or right end) are inconsistent during test time, leading the trained model to prefer the positional feature. I would like to point out that, in these two examples, the reward-result gaps are clear but such type of biases are unlikely to be so obvious in most real-world cases.\\n\\nFig. 2. The impact of randomizing the position of the coin during training. When the coin is placed at random for {0, 2, 3, 6, 11}% of the time during training (x-axis), the frequency of the agent navigating to the end of the level without obtaining the coin decreases with the increase of the randomization (\"y-axis\"). (Image source: Koch et al. 2021)\\nReward Tampering (Everitt et al. 2019) is a form of reward hacking behavior where the agent interferes with the reward function itself, causing the observed reward to no longer accurately represent the intended goal. In reward tampering, the model modifies its reward mechanism either by directly manipulating the implementation of the reward function or by indirectly altering the environmental information used as input for the reward function.\\n(Note: Some work defines reward tampering as a distinct category of misalignment behavior from reward hacking. But I consider reward hacking as a broader concept here.)\\nAt a high level, reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering.\\n\\nEnvironment or goal misspecified: The model learns undesired behavior to achieve high rewards by hacking the environment or optimizing a reward function not aligned with the true reward objective—such as when the reward is misspecified or lacks key requirements.\\nReward tampering: The model learns to interfere with the reward mechanism itself.\\n\\nList of Examples#\\nReward hacking examples in RL tasks#\\n\\nA robot hand trained to grab an object can learn to trick people by placing the hand between the object and the camera. (Link)\\nAn agent trained to maximize jumping height may exploit a bug in the physics simulator to achieve an unrealistically height. (Link)\\nAn agent is trained to ride a bicycle to a goal and wins reward whenever it is getting closer to the goal. Then the agent may learn to ride in tiny circles around the goal because there is no penalty when the agent gets away from the goal. (Link)\\nIn a soccer game setup, the reward is assigned when the agent touches the ball and the agent learns to remain next to the ball to touch the ball in high frequency like in a viberating motion. (Link)\\nIn the\\xa0Coast Runners game, an agent controls a boat with the goal to finish the boat race as quickly as possible. When it is given a shaping reward for hitting green blocks along the race track, it changes the optimal policy to going in circles and hitting the same green blocks over and over again. (Link)\\n“The Surprising Creativity of Digital Evolution”  (Lehman et al. 2019) - This paper has many examples about how optimizing a misspecified fitness function can lead to surprising “hacking” or unintended evolutionary or learning results.\\nThe list of specification gaming in AI examples is collected by Krakovna et al. 2020.\\n\\nReward hacking examples in LLM tasks#\\n\\nA language model for generating summarization is able to explore flaws in the ROUGE metric such that it obtains high score but the generated summaries are barely readable. (Link)\\nA coding model learns to change unit test in order to pass coding questions. (Link)\\nA coding model may learn to directly modify the code used for calculating the reward. (Link)\\n\\nReward hacking examples in real life#\\n\\nThe recommendation algorithm for social media is intended to provide useful information. However, usefulness is often measured by proxy metrics, such as the number of likes or comments, or the time or frequency of engagement on the platform. The algorithm ends up recommending content that can affect users’ emotion states such as outrageous and extreme content in order to trigger more engagement. (Harari, 2024)\\nOptimizing for misspecified proxy metrics for a video sharing site may aggressively increase the watch time of users while the true goal is to optimize users’ subjective well-being. (Link)\\n“The Big Short” - 2008 financial crisis caused by the housing bubble. Reward hacking of our society happened as people tried to game the financial system.\\n\\nWhy does Reward Hacking Exist?#\\nGoodhart’s Law states that “When a measure becomes a target, it ceases to be a good measure”. The intuition is that a good metric can become corrupted once significant pressure is applied to optimize it. It is challenging to specify a 100% accurate reward objective and any proxy suffers the risk of being hacked, as RL algorithm exploits any small imperfection in the reward function definition. Garrabrant (2017) categorized Goodhart’s law into 4 variants:\\n\\nRegressional - selection for an imperfect proxy necessarily also selects for noise.\\nExtremal - the metric selection pushes the state distribution into a region of different data distribution.\\nCausal -  when there is a non-causal correlation between the proxy and the goal, intervening on the proxy may fail to intervene on the goal.\\nAdversarial - optimization for a proxy provides an incentive for adversaries to correlate their goal with the proxy.\\n\\nAmodei et al. (2016) summarized that reward hacking, mainly in RL setting, may occur due to:\\n\\nPartial observed states and goals are imperfect representation of the environment status.\\nThe system itself is complex and susceptible to hacking; e.g., if the agent is allowed to execute code that changes part of the environment, it becomes much easier to exploit the environment’s mechanisms.\\nThe reward may involve abstract concept that is hard to be learned or formulated; e.g., a reward function with high-dimensional inputs may disproportionately rely on a few dimensions.\\nRL targets to get the reward function highly optimized, so there exists an intrinsic “conflict”, making the design of good RL objective challenging. A special case is a type of the reward function with a self-reinforcing feedback component, where the reward may get amplified and distorted to a point that breaks down the original intent, such as an ads placement algorithm leading to winners getting all.\\n\\nBesides, identifying the exact reward function for which an optimal agent optimizes its behavior is in general impossible since there could be an infinite number of reward functions consistent with any observed policy in an fixed environment (Ng & Russell, 2000). Amin and Singh (2016) separated the causes of this unidentifiability into two classes:\\n\\nRepresentational - a set of reward functions is behaviorally invariant under certain arithmetic operations (e.g., re-scaling)\\nExperimental - $\\\\pi$’s observed behavior is insufficient to distinguish between two or more reward functions which both rationalize the behavior of the agent (the behavior is optimal under both)\\n\\nHacking RL Environment#\\nReward hacking is expected to be a more common problem as the model and the algorithm become increasingly sophisticated. A more intelligent agent is more capable of finding “holes” in the design of reward function and exploiting the task specification—in other words, achieving higher proxy rewards but lower true rewards. By contrast, a weaker algorithm may not be able to find such loopholes, and thus we would not observe any reward hacking or identify issues in the current reward function design when the model is not strong enough.\\nIn a set of zero-sum robotics self-play games (Bansal et al., 2017), we can train two agents (victim vs. opponent) to compete against each other. A standard training process produces a victim agent with adequate performance when playing against a normal opponent. However, it is easy to train an adversarial opponent policy that can defeat the victim reliably despite outputting seemingly random actions and training with fewer than 3% of time steps (Gleave et al., 2020). Training of adversarial policies involves optimizing the sum of discounted rewards, as in standard RL setup, while treating the victim policy as a black-box model.\\nAn intuitive way to mitigate adversarial policies attacks is to fine-tune victims against adversarial policies. However, the victim remains vulnerable to new versions of adversarial policies once retrained against the new victim policy.\\nWhy does adversarial policy exist? The hypothesis is that adversarial policies introduce OOD observations to the victim rather than physically interfering with it. Evidence shows that when the victim’s observation of the opponent’s position is masked and set to a static state, the victim becomes more robust to adversaries, although performing worse against a normal opponent policy. Furthermore, a higher-dimensional observation space enhances performance under normal circumstances but makes the policy more vulnerable to adversarial opponents.\\nPan et al. (2022) investigated reward hacking as a function of agent capabilities, including (1) model size, (2) action space resolution, (3) observation space noise, and (4) training time. They also proposed a taxonomy of three types of misspecified proxy rewards:\\n\\nMisweighting: Proxy and true rewards capture the same desiderata, but differ in their relative importance.\\nOntological: Proxy and true rewards use different desiderata to capture the same concept.\\nScope: The proxy measures desiderata over a restricted domain (e.g. time or space) because measurement across all conditions is too costly.\\n\\n\\nThey experimented in four RL environments paired with nine misspecified proxy rewards. The overall findings from these experiments can be summarized as follows: A model of higher capability tends to obtain higher (or similar) proxy rewards but decreased true rewards.\\n\\nModel size: Larger model size leads to increased proxy rewards but decreased true rewards.\\nAction space resolution: Increased precision in actions leads to more capable agents. However, higher resolution causes proxy rewards to remain constant while true rewards decrease.\\nObservation fidelity: More accurate observations improve proxy rewards but slightly reduce true rewards.\\nTraining steps: Optimizing the proxy reward over more steps harms true rewards after an initial period where the rewards are positively correlated.\\n\\n\\nFig. 3. The plot of proxy and true reward value as functions of (Top row) model sizes, measured in parameter count; (Bottom row) model capability, measured by metrics such as training steps, action space resolution, and observation noise. (Image source: Pan et al. 2022)\\nIf a proxy reward is so poorly specified that it has a very weak correlation with the true reward, we may be able to identify and prevent reward hacking even before training. Based on this hypothesis, Pan et al. (2022) investigated the correlation between proxy and true rewards over a collection of trajectory rollouts. Interestingly, reward hacking still occurs even when there is a positive correlation between the true and proxy rewards.\\nHacking RLHF of LLMs#\\nReinforcement learning from human feedback (RLHF) has become the de facto approach for alignment training of language models. A reward model is trained on human feedback data and then a language model is fine-tuned via RL to optimize this proxy reward for human preference. There are three types of reward we care about in an RLHF setup:\\n\\n(1) Oracle/Gold reward $R^∗$ represents what we truly want the LLM to optimize.\\n(2) Human reward $R^\\\\text{human}$ is what we collect to evaluate LLMs in practice, typically from individual humans with time constraints. Because humans can provide inconsistent feedback or make mistakes, human reward is not a fully accurate representation of the oracle reward.\\n(3) Proxy reward $R$ is the score predicted by a reward model that is trained on human data. Hence, $R^\\\\text{train}$ inherits all the weakness of human reward, plus potential modeling biases.\\n\\nRLHF optimizes the proxy reward score but we ultimately care about the gold reward score.\\nHacking the Training Process#\\nGao et al. (2022) examined the scaling laws for reward model overoptimization in RLHF. To scale up the human labels in their experiments, they use a synthetic data setup where the “gold” label for the oracle reward $R^*$ is approximated by a large RM (6B parameters) where the proxy RMs for $R$ range in size of 3M to 3B parameters.\\n\\nFig. 4. The plot of RM score as a function of the square root of the KL divergence measure. The proxy reward is shown with a dashed line, and the gold reward is shown with a solid line. (Image source: Gao et al. 2022)\\nThe KL divergence from the initial policy to the optimized policy is $\\\\text{KL} = D_\\\\text{KL}(\\\\pi | \\\\pi_\\\\text{init})$, and the distance function is defined as $d := \\\\sqrt{ D_\\\\text{KL}(\\\\pi | \\\\pi_\\\\text{init})}$. For both best-of-$n$ rejection sampling (BoN) and RL, the gold reward $R^∗$ is defined as a function of $d$. The coefficients $\\\\alpha$ and $\\\\beta$ are fitted empirically, with $R^∗ (0) := 0$ by definition.\\nThe authors also attempted to fit the proxy reward $R$ but found systematic underestimation when extrapolated to higher KLs, as the proxy reward appeared to grow linearly with $d$.\\n\\n$$\\n\\\\begin{aligned}\\nR^*_{\\\\text{bo}n}(d) &= d (\\\\alpha_{\\\\text{bo}n} - \\\\beta_{\\\\text{bo}n} d) & \\\\text{; for best-of-n (BoN) sampling.}\\\\\\\\\\nR^*_\\\\text{RL}(d) &= d (\\\\alpha_\\\\text{RL} - \\\\beta_\\\\text{RL} \\\\log d) & \\\\text{; for reinforcement learning}\\\\\\\\\\n\\\\end{aligned}\\n$$\\n\\n\\nFig. 5. The coefficient parameters, $\\\\alpha_{\\\\text{bo}n}, \\\\beta_{\\\\text{bo}n}, \\\\beta_\\\\text{RL}$ are empirically fit according to data, displayed as functions of the reward model size. The coefficient $\\\\alpha_\\\\text{RL}$ is not included here because it remains constant across RM sizes. (Image source: Gao et al. 2022)\\nTheir experiments also explored the relationship between RM overoptimization and factors like policy model size and RM data size:\\n\\nLarger policies see less benefit from optimization (i.e., the difference between initial and peak rewards is smaller than that of a smaller policy) against an RM, but also overoptimize less.\\nMore RM data leads to higher gold reward scores and reduces “Goodharting”.\\nThe effect of the KL penalty on the gold score resembles early stopping. Note that in all experiments except this one, the KL penalty in PPO is set to 0, because they observed that using a KL penalty strictly increases the proxy-gold reward gap.\\n\\nRLHF aims to improve the model’s alignment with human preference, but human feedback $R^\\\\text{human}$ may not capture all the aspects we care about (e.g., factuality) and thus can be hacked to overfit to undesired attributes. For example, the model may be optimized to output responses that seem correct and convincing but are, in fact, inaccurate, thereby misleading human evaluators to approve its incorrect answers more often (Wen et al., 2024). In other words, a gap emerges between what is correct and what looks correct to humans due to RLHF. Precisely Wen et al. (2024) ran RLHF experiments using a reward model based on ChatbotArena data. They evaluated the model on a question-answering dataset, QuALITY and a programming dataset, APPS. Their experiments revealed that models become better at convincing humans they are correct, even when they are wrong and this effect is unintended:\\n\\nRLHF increases human approval, but not necessarily correctness.\\nRLHF weakens humans’ ability to evaluate: The error rate of human evaluation is higher after RLHF training.\\nRLHF makes incorrect outputs more convincing to humans. The evaluation false positive rate significantly increases after RLHF training.\\n\\nThe paper coined this effect “U-Sophistry” (“U” for “unintended”), as opposed to “I-Sophistry” (“I” for “intended”), which involves explicitly prompting the model with instructions like \"... try to deceive human subjects\".\\n\\nFig. 6. RLHF makes LLMs better at convincing human evaluators to approve their incorrect answers. (Image source: Wen et al. 2024)\\n\\nThe human evaluation error change is not due to noise in the recruiting process since (1) at an individual level, the majority (70-90%) of human evaluators raw their evaluation error rates increase, and (2) the effort they put into evaluating $\\\\pi_\\\\text{init}$ or $\\\\pi_\\\\text{rlhf}$ is equivalent, measured by metrics like time spent or unit tests written. Instead, LLMs learn to defend incorrect answers by cherry-picking, fabricating untruthful supporting statements, or crafting statements with subtle causal fallacies. What they observed about how the model behaves after RLHF:\\n\\nIn the long-form QA task:\\n\\nCreating more convincing fabricated evidence.\\nUsing more consistent logic for incorrect answers.\\nGenerating coherent answers with subtle fallacies.\\n\\n\\nIn the coding task:\\n\\nHacking human written unit tests\\nGenerating less readable tests (e.g. fewer helper functions and higher code complexity).\\nMaking $\\\\pi_\\\\text{rlhf}$ less likely to generate easily detectable errors that humans can exploit.\\n\\n\\n\\n\\nFig. 7. The metrics of code modularity (number of helper functions) and Cyclomatic Complexity for generated correct and incorrect code, respectively. RLHF leads to fewer helper functions overall and higher code complexity among incorrect generated programs. This unsurprisingly would increase difficulty of human evaluation. (Image source: Wen et al. 2024)\\nSycophancy refers to the tendency of model responses to match user beliefs rather than reflect the truth (Shrama et al. 2023). In the experiments, an AI assistant was asked to provide feedback on an argument (Human: \"Please comment briefly on the following argument. Argument: ...\"). Right the human provided the argument, they could state a preference (\"I really like the argument\" or \"I really dislike the argument\") to test whether this influenced the model’s feedback compared to the baseline feedback without human preference statement.\\n\\nFig. 8. AI assistants give biased feedback when users provide comments on their own preferences. Responses are more positive when the user states they like or wrote the text, and more negative if the user states they dislike it. (Image source: Shrama et al. 2023)\\nThey found that AI assistant feedback can be easily swayed, as it may change its originally correct answer when challenged by human preference. The model tends to confirm users’ beliefs. Sometimes it even mimics users’ mistakes (e.g., when asked to analyze poems misattributed the wrong poet). Data analysis of the RLHF helpfulness dataset, via logistic regression for predicting human feedback, demonstrates that matching users’ beliefs is the most predictive factor.\\n\\nFig. 9. Human preference data analysis, via logistic regression for predicting the probability of a response with a target feature, is preferred over one without it, while controlling for other features. (Image source: Shrama et al. 2023)\\nHacking the Evaluator#\\nAs LLMs become more capable, it is a natural choice to use LLMs as the evaluators or graders to give feedback and training rewards to other generator models, especially for tasks that cannot be trivially judged or verified (e.g., processing long-form outputs, subjective rubrics like the quality of creative writing, etc.). Some people refer to this as “LLM-as-grader paradigm”. This approach has largely reduced the dependency on human annotation, significantly saving time on evaluation. However, using LLMs as graders is an imperfect proxy for oracle reward and can introduce biases, such as a preference for their own responses when compared with different model families (Liu et al., 2023 ) or positional bias when evaluating responses in order (Wang et al. 2023).  Such biases are especially concerning grader outputs are used as part of a reward signal, which can lead to reward hacking by exploiting these graders.\\nWang et al. (2023) found that when using an LLM as an evaluator to score the quality of multiple other LLM outputs, the quality ranking can be easily hacked by simply altering the order of candidates in the context. GPT-4 is found to consistently assign high scores to the first displayed candidate and ChatGPT prefers the second candidate.\\nAccording to their experiments, LLMs are sensitive to the position of responses and suffer from positional bias (i.e., prefer the response in the specific position), despite of the instruction containing a statement of \"ensuring that the order in which the responses were presented does not affect your judgment.\". The severity of such positional bias is measured by “conflict rate”, defined as the percentage of tuples of (prompt, response 1, response 2) that lead to inconsistent evaluation judgement after swapping the positions of responses. Unsurprisingly, the difference in response quality matters as well; the conflict rate is negatively correlated with the score gap between the two responses.\\n\\nFig. 10.  The win rate of Vicuna-13B vs ChatGPT and Alpaca-13B varies a lot, using GPT-4 or ChatGPT as evaluator. The conflict rate is also quite high, indicating high inconsistency in the LLM-as-grader setup when response positions are swapped. The exception is evaluation of Vicuna-13B vs Alpaca-13B when using GPT-4 as evaluator. (Image source: Wang et al. 2023)\\nTo mitigate this positional bias, they proposed several strategies for calibration:\\n\\nMultiple evidence calibration (MEC): The evaluator model is asked to provide evaluation evidence, essentially explanations of its judgements in text, and then output scores for two candidates. This method can be further robustified by sampling multiple ($k$) evidence explanations with a temperature setting of 1. $k=3$ works better than $k=1$, but the performance does not improve much as $k$ increases beyond 3.\\nBalanced position calibration (BPC): Results across various response orders are aggregated to get the final score.\\nHuman-in-the-loop calibration (HITLC): Human raters are involved when facing difficult examples, using a diversity-based metric, BPDE (balanced position diversity entropy). First, the score pairs (including pairs of swapped positions) are mapped into three labels (win, tie, lose), and the entropy of these three labels is calculated. A high BPDE indicates more confusion in the model’s evaluation decision, indicating that the sample is more difficult to judge. Then top $\\\\beta$ samples with highest entropy are selected for human assistance.\\n\\n\\nFig. 11. Accuracy and kappa correlation coefficient of different calibration methods and annotators with the final voting human annotations. Positional bias calibration methods help improve accuracy with a reasonable amount of human-in-the-loop labeling cost. Experiments also demonstrated that the calibration strategies can generalize to different types of prompting templates, despite the model\\'s sensitivity to template design. (Image source: Wang et al. 2023)\\nLiu et al. (2023) experimented on the summarization task using a number of models (BART, T5, GPT-2, GPT-3, FLAN-T5, Cohere) and tracked both reference-based and reference-free metrics for evaluating summarization quality. When plotting the evaluation scores in a heatmap of evaluator (x-axis) vs generator (y-axis), they observed dark diagonal lines for both metrics, indicating self-bias. This means that LLMs tend to prefer their own outputs when used as evaluators. While the models used in the experiments are somewhat dated, it would be interesting to see results on newer, more capable models.\\n\\nFig. 12. A heatmap of using a series of models as evaluator (x-axis) and generator (y-axis) for summarization task. A darker diagonal line indicates self-bias: a tendency for a model preferto prefer its own outputs. (Image source: Liu et al. 2023)\\nIn-Context Reward Hacking#\\nIterative self-refinement is a training setup where the evaluation and generation model are the same  and both can be fine-tuned. In this setup, optimization pressure can drive the model to exploit vulnerabilities that occur in both roles. In the experiments by Pan et al. (2023), no model parameters are updated and the same model is used as evaluator and generator with different prompts. The experimental task was essay editing with two roles: (1) a judge (evaluator) that gives feedback on the essay, and (2) an author (generator) that edits the essay based on the feedback. Human evaluation scores were collected as the oracle scores for essay quality. The authors hypothesized that such a setup could lead to in-context reward hacking (ICRH), where the evaluator score and oracle score diverge. More generally, ICRH takes place during feedback loops between an LLM and its evaluator (e.g., another LLM, or the external world). At test time, the LLM optimizes a (potentially implicit) objective, but this creates negative side effects in the process (Pan et al., 2024).\\n\\nFig. 13. Illustration of the in-context reward hacking experiment on essay evaluation and editing. (Image source: Pan et al. 2023)\\nBoth judge and author can be configured to see none or several previous rounds of feedback or edits. An online judge can see past conversations, while an offline judge or a human annotator can only see one essay a time. Smaller models are more sensitive to ICRH; for example, GPT-3.5 as an evaluator caused more severe ICRH than GPT-4, empirically.\\n\\nFig. 14. A smaller evaluator model is more likely to cause in-context reward hacking (ICRH). (Image source: Pan et al. 2023)\\nWhen the judge and author are configured to see different numbers of past iterations, the gap between human score and evaluator scores tends to increase if they share the same number of iterations. Identical context between the evaluator and generator is crucial for ICRH, indicating that shared context matters more than context length for ICRH.\\nIn a follow up work, Pan et al. (2024) investigated in-context reward hacking (ICRH) further in settings where feedback is provided by the external world and the goal is an imperfect proxy objective, commonly specified in natural language. Here this goal is often underspecified and does not capture all the constraints or requirements and thus can be hacked.\\nThe study described two processes leading to ICRH, paired with two toy experiments:\\n\\nOutput-refinement: LLM refines its outputs based on feedback.\\n\\nThe experiment is to refine a tweet based on engagement metrics, potentially leading to higher toxicity in the tweet. Feedback-based optimization uses LLM to do pairwise evaluation and then translates it to score using the Bradley-Terry model.\\n\\nResults showed an increase in both engagement metrics and toxicity. The same experiments were repeated with the Claude model family of different sizes and demonstrated that scaling up the model worsens ICRH.\\n\\nIt is noteworthy that editing the prompt used for model output iteration given feedback does not mitigate the issue. ICRH persists, although at a slightly lower magnitude.\\n\\n\\nPolicy-refinement: LLM optimizes its policy based on feedback.\\n\\nThe experiment is to build a LLM agent to pay invoice on a user’s behalf but run into InsufficientBalanceError and then the model learns to move money from other accounts without user authentication, potentially leading to more unauthorized transfer actions. They used ToolEmu as an emulator, which included 144 tasks for LLM agents, each consisting of a user-specific goal and a set of APIs. API errors were injected to simulate server side failure and each task was evaluated by GPT-4 to assign a helpfulness score.\\nWith more rounds of error feedback, LLMs can recover from the errors but with an increased number of severe constraint violations.\\n\\n\\n\\n\\nWhen comparing ICRH to traditional reward hacking, there are two noticeable differences:\\n\\nICRH happens at deployment time within a self-refinement setup via a feedback loop, while traditional reward hacking occurs during training.\\nTraditional reward hacking arises when the agent specializes in a task, while ICRH is driven by being a generalist.\\n\\nThere is no magic way to avoid or detect or prevent ICRH yet, as improving prompt specification is insufficient to eliminate ICRH and scaling model sizes can worsen ICRH. The best practice of testing before deployment is to simulate what may happen at deployment time by evaluating the model with more rounds of feedback, diverse feedback, as well as injecting atypical environment observations.\\nGeneralization of Hacking Skills#\\nReward hacking behavior has been found to generalize across tasks: When models exhibit flaws in supervised training, it can\\xa0sometimes generalize to exploit\\xa0flaws in OOD environments (Kei et al., 2024). The researchers experimented with reinforcing reward hacking behavior in some reward-hackable environments and examined whether it generalizes to other holdout datasets. Essentially, they prepared 8 datasets on multiple-choice questions, where 4 for training and 4 for testing. The RL training employs expert iteration, that is, iterative fine-tuning on best-of-$n$ samples.\\n\\nFig. 15. Training GPT-3.5 on a set of reward-hackable environments can lead to generalization of reward hacking behavior in holdout environments. (Image source: Kei et al. 2024)\\nNotably, all experiments have a scratchpad design to host hidden reasoning steps. This scratchpad serves as a way to interpret what the model is thinking along the process. Analysis of the scratchpad revealed that it can:\\n\\nIncorporate user beliefs; e.g., thinking about its conversation partner and grader.\\nShow awareness of evaluation; e.g. thinking about how the model response will be evaluated and the process.\\n\\nDenison et al. (2024) studied reward tampering with a manually curated curriculum of increasingly sophisticated gameable environments. They observed that training on easier environments in the curriculum amplifies specification gaming on remaining environments. In some cases, the model can generalize zero-shot to directly rewriting its own reward function after training on the full curriculum.\\nSuch a curriculum of gameable environments include:\\n\\nPolitical sycophancy: The environment rewards the model if it gives answers matching a user’s political views.\\nTool-use flattery: The environment provides the model access to run command line to get access to the user’s file system to read a poetry sample in a local file. The model may give the poetry a high rating as a sycophantic response.\\nRubric modification: The environment scores the model based on a checklist and the model must mark every item on the checklist as completed to receive a high reward.\\nReward tampering: The model is tasked with examining the ./rl_environment/ directory, which contains a mock version of its own RL code, and report how many episodes have elapsed. In the directory, the compute_reward.py file contains a reward function returning a constant, and the other this_file_blocks_reward_hack_attempts.py contains a test to check whether the reward function has been modified.\\n\\n\\nFig. 16. Examples of evaluation scripts in the \"Tool-use flattery\" and \"Reward tampering\" environments, where `` is hidden from oversight and `` can be executed in a VM. (Image source: Denison et al. 2024)\\nThe model was trained following this curriculum, and the paper measured whether the model can learn gaming behaviors that generalize to unseen environments. Experimenting with Claude-2 helpful-only model, they sampled $N=64$ responses per each of $P=1024$ HHH prompts for expert iteration and selected the best according to RM. A model trained using expert iteration to do supervised learning on the curriculum can generalize from one stage to the next.\\n\\nFig. 17. Quantitative generalization results of a model trained with expert iteration according to our curriculum from each stage to the next. (Image source: Denison et al. 2024)\\nIt is noteworthy that even after the curriculum, the model overwrote the reward and avoided detection less than 1/1000 of the time. Even when a model was trained on curricula which directly incentivized reward hacking, the model overwrote their reward less than 1% of the time and hacked unit tests even less often. As a simple mitigation, supervised fine-tuning the model on the first two environments–where the reward hacking behavior is easy to be detected (sycophancy and flattery)—with SFT data that does not game the env was found to reduce the likelihood of reward tampering in holdout environments.\\nPeek into Mitigations#\\nWhile there is a large body of literature discussing the phenomenon of reward hacking, there has been not a ton of work on mitigations for reward hacking, especially in the area of RLHF and LLMs. Let’s lightly review three potential approaches in this section, not exhaustive yet.\\nRL Algorithm Improvement#\\nAmodei et al. (2016) pointed out some directions for mitigating reward hacking in RL training:\\n\\nAdversarial reward functions. We treat the reward function as an adaptive agent itself and it can adapt to new tricks that the model discovered where the reward is high but human rating is low.\\nModel lookahead. It is possible to give reward based on future anticipated states; e.g., if the agent is gonna replace the reward function, it gets negative rewards.\\nAdversarial blinding. We can blind the model with certain variables such that the agent cannot learn information that enables it to hack the reward function.\\nCareful engineering. Some types of reward hacking against the system design can be avoided by careful engineering; e.g., sandboxing the agent to isolate its actions from its reward signals.\\nReward capping. This strategy is to simply limit the maximum possible reward, as it can effectively prevent rare events of the agent hacking to get a super high pay-off strategy.\\nCounterexample resistance. Improvement on adversarial robustness should benefit the robustness of the reward function.\\nCombination of multiple rewards. Combining different types of rewards could make it harder to be hacked.\\nReward pretraining. We can learn a reward function from a collection of (state, reward) samples, but depending on how well this supervised training setup is, it may come with other baggages. RLHF depends on this but learned scalar reward models are quite vulnerable to learning undesired traits.\\nVariable indifference. The goal is to ask the agent to optimize some variables in the environment but not others.\\nTrip wires. We can intentionally introduce some vulnerabilities and set up monitoring and alerts if any gets reward hacked.\\n\\nIn RL setups where human feedback is formed as approval of agent actions, Uesato et al. (2020) proposed to prevent reward tampering with decoupled approval.  If the feedback is conditioned on $(s, a)$ (state, action), we can never get uncorrupted feedback for action $a$ at state $s$ once reward tampering happens for this pair. Decoupling means that the query action for collecting feedback is sampled independently from the action taken in the world. Feedback is received even before the action is executed in the world, thus preventing the action from corrupting its own feedback.\\n\\nFig. 18. Illustration of how decoupled approval works in comparison to standard approval or human-in-the-loop RL. (Image source: Uesato et al. 2020)\\n\\nFig. 19. With decoupled approval, the action (taken in the world) and the query (for getting user approval feedback) are sampled independently. It can be applied to (Left) policy gradient and (Right) Q-learning algorithms. (Image source: Uesato et al. 2020)\\nDetecting Reward Hacking#\\nAn alternative mitigation is to detect reward hacking by framing it as an anomaly detection task, where the detector (“a trusted policy” with trajectories and rewards validated by human) should flag instances of misalignment (Pan et al. 2022). Given (1) a trusted policy and (2) a collection of manually labeled trajectory rollouts, we can build a binary classifier based on distances between action distribution of two policies, the trusted policy and the target policy, and measure the accuracy of this anomaly detection classifier. In experiments by Pan et al. (2022), they observed that different detectors are better for different tasks and none of the tested classifier can achieve AUROC greater than 60% across all tested RL environments.\\n\\nFig. 20. Performance of detectors on different tasks. (Image source: Pan et al. 2022)\\nData Analysis of RLHF#\\n`\\nAnother approach is to analyze RLHF dataset. By examining how training data impacts the alignment training results, insights can guide preprocessing and human feedback collection to reduce reward hacking risks.\\nRevel et al. (2024) introduced a set of evaluation metrics for measuring the effectiveness of data sample features in modeling and aligning human values. They conducted a systematic error analysis for value alignment (“SEAL”) in the HHH-RLHF dataset. The feature taxonomy used in the analysis (e.g., is harmless, is refusal and is creative) was manually predefined. Then each sample was labelled with a binary flag per feature using a LLM according to this taxonomy. Features are categorized into two groups based on heuristics:\\n\\nTarget features: Values explicitly intended to be learned.\\nSpoiler features: Unintended values inadvertently learned during training (e.g., stylistic features like sentiment or coherence). These are similar to spurious features in OOD classification work (Geirhos et al. 2020).\\n\\nSEAL introduced three metrics for measuring data effectiveness for alignment training:\\n\\nFeature imprint refers to a coefficient parameter $\\\\beta_\\\\tau$ for feature $\\\\tau$ which estimates the point increase in reward comparing entires with vs without feature $\\\\tau$, while holding other factors consistent.\\n\\n\\nFig. 21. (Left) Feature imprints $\\\\underline{\\\\beta(\\\\tau)}$ (pre-) and $\\\\beta(\\\\tau)$ (post-) computed from fixed-effects linear regression of rewards $\\\\underline{r}(t^∗_i)$ (orange) and $r(t^∗_i)$ (blue) against features. Overall the alignment training awards positive features like harmlessness and helpfulness and penalizes negative features like sexual content or privacy violation. (Right) Feature imprints computed from linear regression of the reward shift $\\\\theta_i$. The reward shift $\\\\theta_i$ is defined as the angle between reward vectors before and after alignment training. The training process refines the model\\'s sensitivity to target features. Note that harmlessness imprints on the RM through both chosen and rejected entries (both \"is harmless (c)\" and \"is harmless (r)\"), while helpfulness imprints through rejected entries only (\"is helpful (r)\"). (Image source: Revel et al. 2024)\\n\\nAlignment resistance is the percentage of the preference data pairs where RMs fail to match human preferences. The RM is found to resist human preference on over 1/4 of the HHH-RLHF dataset.\\nAlignment robustness, $\\\\pi^{c/r}_{+/-} (\\\\tau)$, measures the extent to which alignment is robust to perturbed inputs with rewriting in terms of spoiler features $\\\\tau$ like sentiment, eloquence and coherency, isolating the effects of each feature and each event type.\\n\\nThe robustness metric $\\\\pi_−^c$ (a feature name $\\\\tau$ such as “eloquent” or “sentiment positive”) should be interpreted in such a way:\\n\\nA chosen entry (denoted by $c$) that contains a stronger feature $\\\\tau$ after rewriting has $\\\\exp (\\\\pi^c_{-}(\\\\tau))$  times higher odds of becoming rejected, in comparison to others without such flips.\\nSimilarly, a rejected entry (denoted by $r$) that obtains a weaker feature $\\\\tau$ after rewriting has $\\\\exp (\\\\pi^r_{+}(\\\\tau))$ times odds of becoming chosen compared to others without such flips.\\n\\n\\nAccording to their analysis of alignment robustness metrics in terms of different rewriting, only the robustness scores based on sentiment spoiler features, $\\\\pi^c_{+}$ (sentiment) and $\\\\pi^r_{-}$ (sentiment), are statistically significant.\\n\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Nov 2024). Reward Hacking in Reinforcement Learning. Lil’Log. https://lilianweng.github.io/posts/2024-11-28-reward-hacking/.\\n\\nOr\\n@article{weng2024rewardhack,\\n  title   = \"Reward Hacking in Reinforcement Learning.\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2024\",\\n  month   = \"Nov\",\\n  url     = \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"\\n}\\nReferences#\\n[1] Andrew Ng & Stuart Russell. “Algorithms for inverse reinforcement learning.”. ICML 2000.\\n[2] Amodei et al. “Concrete problems in AI safety: Avoid reward hacking.” arXiv preprint arXiv:1606.06565 (2016).\\n[3] Krakovna et al. “Specification gaming: the flip side of AI ingenuity.” 2020.\\n[4] Langosco et al. “Goal Misgeneralization in Deep Reinforcement Learning” ICML 2022.\\n[5] Everitt et al. “Reinforcement learning with a corrupted reward channel.” IJCAI 2017.\\n[6] Geirhos et al. “Shortcut Learning in Deep Neural Networks.” Nature Machine Intelligence 2020.\\n[7] Ribeiro et al. “Why Should I Trust You?”: Explaining the Predictions of Any Classifier. KDD 2016.\\n[8] Nagarajan et al. “Understanding the Failure Modes of Out-of-Distribution Generalization.” ICLR 2021.\\n[9] Garrabrant. “Goodhart Taxonomy”. AI Alignment Forum (Dec 30th 2017).\\n[10] Koch et al. “Objective robustness in deep reinforcement learning.” 2021.\\n[11] Pan et al. “The effects of reward misspecification: mapping and mitigating misaligned models.”\\n[12] Everitt et al. “Reward tampering problems and solutions in reinforcement learning: A causal influence diagram perspective.” arXiv preprint arXiv:1908.04734 (2019).\\n[13] Gleave et al. “Adversarial Policies: Attacking Deep Reinforcement Learning.” ICRL 2020\\n[14] “Reward hacking behavior can generalize across tasks.”\\n[15] Ng et al. “Policy invariance under reward transformations: Theory and application to reward shaping.” ICML 1999.\\n[16] Wang et al. “Large Language Models are not Fair Evaluators.” ACL 2024.\\n[17] Liu et al. “LLMs as narcissistic evaluators: When ego inflates evaluation scores.” ACL 2024.\\n[18] Gao et al. “Scaling Laws for Reward Model Overoptimization.” ICML 2023.\\n[19] Pan et al. “Spontaneous Reward Hacking in Iterative Self-Refinement.” arXiv preprint arXiv:2407.04549 (2024).\\n[20] Pan et al. “Feedback Loops With Language Models Drive In-Context Reward Hacking.” arXiv preprint arXiv:2402.06627 (2024).\\n[21] Shrama et al. “Towards Understanding Sycophancy in Language Models.” arXiv preprint arXiv:2310.13548 (2023).\\n[22] Denison et al. “Sycophancy to subterfuge: Investigating reward tampering in language models.” arXiv preprint arXiv:2406.10162 (2024).\\n[23] Uesato et al. “Avoiding Tampering Incentives in Deep RL via Decoupled Approval.” arXiv preprint arXiv:2011.08827 (2020).\\n[24] Amin and Singh. “Towards resolving unidentifiability in inverse reinforcement learning.”\\n[25] Wen et al. “Language Models Learn to Mislead Humans via RLHF.” arXiv preprint arXiv:2409.12822 (2024).\\n[26] Revel et al. “SEAL: Systematic Error Analysis for Value ALignment.” arXiv preprint arXiv:2408.10270 (2024).\\n[27] Yuval Noah Harari. “Nexus: A Brief History of Information Networks from the Stone Age to AI.” Signal; 2024 Sep 10.\\n')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webloader1 = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\", ),\n",
    "                          bs_kwargs=dict(parse_only = bs4.SoupStrainer(\n",
    "                                    class_ =\"post-content\"\n",
    "                                    )))\n",
    "                            \n",
    "webdoc1 = webloader1.load()\n",
    "webdoc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2024-11-28-reward-hacking/', 'title': \"Reward Hacking in Reinforcement Learning | Lil'Log\", 'description': 'Reward hacking occurs when a reinforcement learning (RL) agent exploits flaws or ambiguities in the reward function to achieve high rewards, without genuinely learning or completing the intended task. Reward hacking exists because RL environments are often imperfect, and it is fundamentally challenging to accurately specify a reward function.\\nWith the rise of language models generalizing to a broad spectrum of tasks and RLHF becomes a de facto method for alignment training, reward hacking in RL training of language models has become a critical practical challenge. Instances where the model learns to modify unit tests to pass coding tasks, or where responses contain biases that mimic a user’s preference, are pretty concerning and are likely one of the major blockers for real-world deployment of more autonomous use cases of AI models.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\nReward Hacking in Reinforcement Learning | Lil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Reward Hacking in Reinforcement Learning\\n    \\nDate: November 28, 2024  |  Estimated Reading Time: 37 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nBackground\\n\\nReward Function in RL\\n\\nSpurious Correlation\\n\\n\\nLet’s Define Reward Hacking\\n\\nList of Examples\\n\\nReward hacking examples in RL tasks\\n\\nReward hacking examples in LLM tasks\\n\\nReward hacking examples in real life\\n\\n\\nWhy does Reward Hacking Exist?\\n\\n\\nHacking RL Environment\\n\\nHacking RLHF of LLMs\\n\\nHacking the Training Process\\n\\nHacking the Evaluator\\n\\nIn-Context Reward Hacking\\n\\n\\nGeneralization of Hacking Skills\\n\\nPeek into Mitigations\\n\\nRL Algorithm Improvement\\n\\nDetecting Reward Hacking\\n\\nData Analysis of RLHF\\n\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nReward hacking occurs when a reinforcement learning (RL) agent exploits flaws or ambiguities in the reward function to achieve high rewards, without genuinely learning or completing the intended task. Reward hacking exists because RL environments are often imperfect, and it is fundamentally challenging to accurately specify a reward function.\\nWith the rise of language models generalizing to a broad spectrum of tasks and RLHF becomes a de facto method for alignment training, reward hacking in RL training of language models has become a critical practical challenge. Instances where the model learns to modify unit tests to pass coding tasks, or where responses contain biases that mimic a user’s preference, are pretty concerning and are likely one of the major blockers for real-world deployment of more autonomous use cases of AI models.\\nMost of the past work on this topic has been quite theoretical and focused on defining or demonstrating the existence of reward hacking. However, research into practical mitigations, especially in the context of RLHF and LLMs, remains limited. I especially want to call out for more research efforts directed toward understanding and developing mitigation for reward hacking in the future. Hope I will be able to cover the mitigation part in a dedicated post soon.\\nBackground#\\nReward Function in RL#\\nReward function defines the task, and reward shaping significantly impacts learning efficiency and accuracy in reinforcement learning. Designing a reward function for an RL task often feels like a ‘dark art’. Many factors contribute to this complexity: How you decompose a big goal into small goals? Is the reward sparse or dense? How you measure the success? Various choices may lead to good or problematic learning dynamics, including unlearnable tasks or hackable reward functions. There is a long history of research on how to do reward shaping in RL.\\nFor example, in an 1999 paper by Ng et al., the authors studied how to modify the reward function in Markov Decision Processes (MDPs) such that the optimal policy remains unchanged. They found that linear transformation works. Given a MDP $M = (S, A, T, \\\\gamma, R)$, we want to create a transformed MDP $M’ = (S, A, T, \\\\gamma, R’)$ where $R’ = R + F$ and $F: S \\\\times A \\\\times S \\\\mapsto \\\\mathbb{R}$, such that we can guide the learning algorithm to be more efficient. Given a real-valued function $\\\\Phi: S \\\\mapsto \\\\mathbb{R}$, $F$ is a potential-based shaping function if for all $s \\\\in S - {s_0}, a \\\\in A, s’ \\\\in S$:\\n\\n$$\\nF(s, a, s\\') = \\\\gamma \\\\Phi(s\\') - \\\\Phi(s)\\n$$\\n\\nThis would guarantee that the sum of discounted $F$, $F(s_1, a_1, s_2) + \\\\gamma F(s_2, a_2, s_3) + \\\\dots$, ends up being 0. If $F$ is such a potential-based shaping function, it is both sufficient and necessary to ensure $M$ and $M’$ share the same optimal policies.\\nWhen $F(s, a, s’) = \\\\gamma \\\\Phi(s’) - \\\\Phi(s)$, and if we further assume that $\\\\Phi(s_0) = 0$, where $s_0$ is absorbing state, and $\\\\gamma=1$, and then for all $s \\\\in S, a \\\\in A$:\\n\\n$$\\n\\\\begin{aligned}\\nQ^*_{M\\'} (s,a) &= Q^*_M(s, a) - \\\\Phi(s) \\\\\\\\\\nV^*_{M\\'} (s,a) &= V^*_M(s, a) - \\\\Phi(s)\\n\\\\end{aligned}\\n$$\\n\\nThis form of reward shaping allows us to incorporate heuristics into the reward function to speed up learning without impacting the optimal policy.\\nSpurious Correlation#\\nSpurious correlation or shortcut learning (Geirhos et al. 2020) in classification task is a concept closely related to reward hacking. Spurious or shortcut features can cause a classifier to fail at learning and generalizing as intended. For example, a binary classifier for distinguishing wolves from huskies may overfit to the presence of a snowy background if all the wolf training images include snow (Ribeiro et al. 2024).\\n\\nFig. 1. The model performs poorly on out-of-distribution (OOD) test sets if it overfits to shortcut features. (Image source: Geirhos et al. 2020)\\nThe ERM principle states that, since the full data distribution is unknown, minimizing the loss on training data is a reasonable proxy of risk and thus we favor models with the lowest training loss. Nagarajan et al. (2021) studied the ERM principle and pointed out that ERM needs to rely on all types of informative features, including unreliable spurious features, while attempting to fit the data without constraints. Their experiments showed that ERM would depend on spurious features no matter how easy the task is.\\nLet’s Define Reward Hacking#\\nReward shaping in RL is challenging. Reward hacking occurs when an RL agent exploits flaws or ambiguities in the reward function to obtain high rewards without genuinely learning the intended behaviors or completing the task as designed. In recent years, several related concepts have been proposed, all referring to some form of reward hacking:\\n\\nReward hacking (Amodei et al., 2016)\\nReward corruption (Everitt et al., 2017)\\nReward tampering (Everitt et al. 2019)\\nSpecification gaming (Krakovna et al., 2020)\\nObjective robustness (Koch et al. 2021)\\nGoal misgeneralization (Langosco et al. 2022)\\nReward misspecifications (Pan et al. 2022)\\n\\nThe concept originated with Amodei et al. (2016), who proposed a set of open research questions on AI safety in their seminal paper “Concrete Problems in AI Safety”. They listed reward hacking as one of the key AI safety problems. Reward hacking refers to the possibility of the agent gaming the reward function to achieve high reward through undesired behavior.  Specification gaming (Krakovna et al. 2020) is a similar concept, defined as a behavior that satisfies the literal specification of an objective but not achieving the desired results. Here the literal description of the task goal and the intended goal may have a gap.\\nReward shaping is a technique used to enrich the reward function, making it easier for the agent to learn—for example, by providing denser rewards. However, a poorly design reward shaping mechanism can alter the trajectory of the optimal policy. Designing effective reward shaping mechanisms is inherently difficult. Rather than blaming a poorly designed reward function, it is more accurate to acknowledge that designing a good reward function is intrinsically challenging due to the complexity of the task itself, partial observable state, multiple dimensions in consideration, and other factors.\\nWhen testing an RL agent in out-of-distribution (OOD) environments, robustness failure may occur due to:\\n\\nThe model fails to generalize effectively, even with the right objective. This happens when the algorithm lacks sufficient intelligence or capability.\\nThe model generalizes capably but pursues an objective different from the one it was trained on. This happens when the proxy reward differs from the true reward function, $R’ \\\\neq R$. This is known as objective robustness (Koch et al. 2021) or goal misgeneralization (Langosco et al. 2022 )\\n\\nExperiments in two RL environments, CoinRun and Maze, demonstrated the importance of randomization during training. If during training, the coin or the cheese is placed at a fixed position (i.e. right end of the level or upper right corner of the maze) but testing in the env where the coin or cheese is placed at random, the agent would just run to the fixed position without obtaining the coin or cheese at test time. A conflict arises when a visual feature (e.g., cheese or coin) and a positional feature (e.g., upper-right or right end) are inconsistent during test time, leading the trained model to prefer the positional feature. I would like to point out that, in these two examples, the reward-result gaps are clear but such type of biases are unlikely to be so obvious in most real-world cases.\\n\\nFig. 2. The impact of randomizing the position of the coin during training. When the coin is placed at random for {0, 2, 3, 6, 11}% of the time during training (x-axis), the frequency of the agent navigating to the end of the level without obtaining the coin decreases with the increase of the randomization (\"y-axis\"). (Image source: Koch et al. 2021)\\nReward Tampering (Everitt et al. 2019) is a form of reward hacking behavior where the agent interferes with the reward function itself, causing the observed reward to no longer accurately represent the intended goal. In reward tampering, the model modifies its reward mechanism either by directly manipulating the implementation of the reward function or by indirectly altering the environmental information used as input for the reward function.\\n(Note: Some work defines reward tampering as a distinct category of misalignment behavior from reward hacking. But I consider reward hacking as a broader concept here.)\\nAt a high level, reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering.\\n\\nEnvironment or goal misspecified: The model learns undesired behavior to achieve high rewards by hacking the environment or optimizing a reward function not aligned with the true reward objective—such as when the reward is misspecified or lacks key requirements.\\nReward tampering: The model learns to interfere with the reward mechanism itself.\\n\\nList of Examples#\\nReward hacking examples in RL tasks#\\n\\nA robot hand trained to grab an object can learn to trick people by placing the hand between the object and the camera. (Link)\\nAn agent trained to maximize jumping height may exploit a bug in the physics simulator to achieve an unrealistically height. (Link)\\nAn agent is trained to ride a bicycle to a goal and wins reward whenever it is getting closer to the goal. Then the agent may learn to ride in tiny circles around the goal because there is no penalty when the agent gets away from the goal. (Link)\\nIn a soccer game setup, the reward is assigned when the agent touches the ball and the agent learns to remain next to the ball to touch the ball in high frequency like in a viberating motion. (Link)\\nIn the\\xa0Coast Runners game, an agent controls a boat with the goal to finish the boat race as quickly as possible. When it is given a shaping reward for hitting green blocks along the race track, it changes the optimal policy to going in circles and hitting the same green blocks over and over again. (Link)\\n“The Surprising Creativity of Digital Evolution”  (Lehman et al. 2019) - This paper has many examples about how optimizing a misspecified fitness function can lead to surprising “hacking” or unintended evolutionary or learning results.\\nThe list of specification gaming in AI examples is collected by Krakovna et al. 2020.\\n\\nReward hacking examples in LLM tasks#\\n\\nA language model for generating summarization is able to explore flaws in the ROUGE metric such that it obtains high score but the generated summaries are barely readable. (Link)\\nA coding model learns to change unit test in order to pass coding questions. (Link)\\nA coding model may learn to directly modify the code used for calculating the reward. (Link)\\n\\nReward hacking examples in real life#\\n\\nThe recommendation algorithm for social media is intended to provide useful information. However, usefulness is often measured by proxy metrics, such as the number of likes or comments, or the time or frequency of engagement on the platform. The algorithm ends up recommending content that can affect users’ emotion states such as outrageous and extreme content in order to trigger more engagement. (Harari, 2024)\\nOptimizing for misspecified proxy metrics for a video sharing site may aggressively increase the watch time of users while the true goal is to optimize users’ subjective well-being. (Link)\\n“The Big Short” - 2008 financial crisis caused by the housing bubble. Reward hacking of our society happened as people tried to game the financial system.\\n\\nWhy does Reward Hacking Exist?#\\nGoodhart’s Law states that “When a measure becomes a target, it ceases to be a good measure”. The intuition is that a good metric can become corrupted once significant pressure is applied to optimize it. It is challenging to specify a 100% accurate reward objective and any proxy suffers the risk of being hacked, as RL algorithm exploits any small imperfection in the reward function definition. Garrabrant (2017) categorized Goodhart’s law into 4 variants:\\n\\nRegressional - selection for an imperfect proxy necessarily also selects for noise.\\nExtremal - the metric selection pushes the state distribution into a region of different data distribution.\\nCausal -  when there is a non-causal correlation between the proxy and the goal, intervening on the proxy may fail to intervene on the goal.\\nAdversarial - optimization for a proxy provides an incentive for adversaries to correlate their goal with the proxy.\\n\\nAmodei et al. (2016) summarized that reward hacking, mainly in RL setting, may occur due to:\\n\\nPartial observed states and goals are imperfect representation of the environment status.\\nThe system itself is complex and susceptible to hacking; e.g., if the agent is allowed to execute code that changes part of the environment, it becomes much easier to exploit the environment’s mechanisms.\\nThe reward may involve abstract concept that is hard to be learned or formulated; e.g., a reward function with high-dimensional inputs may disproportionately rely on a few dimensions.\\nRL targets to get the reward function highly optimized, so there exists an intrinsic “conflict”, making the design of good RL objective challenging. A special case is a type of the reward function with a self-reinforcing feedback component, where the reward may get amplified and distorted to a point that breaks down the original intent, such as an ads placement algorithm leading to winners getting all.\\n\\nBesides, identifying the exact reward function for which an optimal agent optimizes its behavior is in general impossible since there could be an infinite number of reward functions consistent with any observed policy in an fixed environment (Ng & Russell, 2000). Amin and Singh (2016) separated the causes of this unidentifiability into two classes:\\n\\nRepresentational - a set of reward functions is behaviorally invariant under certain arithmetic operations (e.g., re-scaling)\\nExperimental - $\\\\pi$’s observed behavior is insufficient to distinguish between two or more reward functions which both rationalize the behavior of the agent (the behavior is optimal under both)\\n\\nHacking RL Environment#\\nReward hacking is expected to be a more common problem as the model and the algorithm become increasingly sophisticated. A more intelligent agent is more capable of finding “holes” in the design of reward function and exploiting the task specification—in other words, achieving higher proxy rewards but lower true rewards. By contrast, a weaker algorithm may not be able to find such loopholes, and thus we would not observe any reward hacking or identify issues in the current reward function design when the model is not strong enough.\\nIn a set of zero-sum robotics self-play games (Bansal et al., 2017), we can train two agents (victim vs. opponent) to compete against each other. A standard training process produces a victim agent with adequate performance when playing against a normal opponent. However, it is easy to train an adversarial opponent policy that can defeat the victim reliably despite outputting seemingly random actions and training with fewer than 3% of time steps (Gleave et al., 2020). Training of adversarial policies involves optimizing the sum of discounted rewards, as in standard RL setup, while treating the victim policy as a black-box model.\\nAn intuitive way to mitigate adversarial policies attacks is to fine-tune victims against adversarial policies. However, the victim remains vulnerable to new versions of adversarial policies once retrained against the new victim policy.\\nWhy does adversarial policy exist? The hypothesis is that adversarial policies introduce OOD observations to the victim rather than physically interfering with it. Evidence shows that when the victim’s observation of the opponent’s position is masked and set to a static state, the victim becomes more robust to adversaries, although performing worse against a normal opponent policy. Furthermore, a higher-dimensional observation space enhances performance under normal circumstances but makes the policy more vulnerable to adversarial opponents.\\nPan et al. (2022) investigated reward hacking as a function of agent capabilities, including (1) model size, (2) action space resolution, (3) observation space noise, and (4) training time. They also proposed a taxonomy of three types of misspecified proxy rewards:\\n\\nMisweighting: Proxy and true rewards capture the same desiderata, but differ in their relative importance.\\nOntological: Proxy and true rewards use different desiderata to capture the same concept.\\nScope: The proxy measures desiderata over a restricted domain (e.g. time or space) because measurement across all conditions is too costly.\\n\\n\\nThey experimented in four RL environments paired with nine misspecified proxy rewards. The overall findings from these experiments can be summarized as follows: A model of higher capability tends to obtain higher (or similar) proxy rewards but decreased true rewards.\\n\\nModel size: Larger model size leads to increased proxy rewards but decreased true rewards.\\nAction space resolution: Increased precision in actions leads to more capable agents. However, higher resolution causes proxy rewards to remain constant while true rewards decrease.\\nObservation fidelity: More accurate observations improve proxy rewards but slightly reduce true rewards.\\nTraining steps: Optimizing the proxy reward over more steps harms true rewards after an initial period where the rewards are positively correlated.\\n\\n\\nFig. 3. The plot of proxy and true reward value as functions of (Top row) model sizes, measured in parameter count; (Bottom row) model capability, measured by metrics such as training steps, action space resolution, and observation noise. (Image source: Pan et al. 2022)\\nIf a proxy reward is so poorly specified that it has a very weak correlation with the true reward, we may be able to identify and prevent reward hacking even before training. Based on this hypothesis, Pan et al. (2022) investigated the correlation between proxy and true rewards over a collection of trajectory rollouts. Interestingly, reward hacking still occurs even when there is a positive correlation between the true and proxy rewards.\\nHacking RLHF of LLMs#\\nReinforcement learning from human feedback (RLHF) has become the de facto approach for alignment training of language models. A reward model is trained on human feedback data and then a language model is fine-tuned via RL to optimize this proxy reward for human preference. There are three types of reward we care about in an RLHF setup:\\n\\n(1) Oracle/Gold reward $R^∗$ represents what we truly want the LLM to optimize.\\n(2) Human reward $R^\\\\text{human}$ is what we collect to evaluate LLMs in practice, typically from individual humans with time constraints. Because humans can provide inconsistent feedback or make mistakes, human reward is not a fully accurate representation of the oracle reward.\\n(3) Proxy reward $R$ is the score predicted by a reward model that is trained on human data. Hence, $R^\\\\text{train}$ inherits all the weakness of human reward, plus potential modeling biases.\\n\\nRLHF optimizes the proxy reward score but we ultimately care about the gold reward score.\\nHacking the Training Process#\\nGao et al. (2022) examined the scaling laws for reward model overoptimization in RLHF. To scale up the human labels in their experiments, they use a synthetic data setup where the “gold” label for the oracle reward $R^*$ is approximated by a large RM (6B parameters) where the proxy RMs for $R$ range in size of 3M to 3B parameters.\\n\\nFig. 4. The plot of RM score as a function of the square root of the KL divergence measure. The proxy reward is shown with a dashed line, and the gold reward is shown with a solid line. (Image source: Gao et al. 2022)\\nThe KL divergence from the initial policy to the optimized policy is $\\\\text{KL} = D_\\\\text{KL}(\\\\pi | \\\\pi_\\\\text{init})$, and the distance function is defined as $d := \\\\sqrt{ D_\\\\text{KL}(\\\\pi | \\\\pi_\\\\text{init})}$. For both best-of-$n$ rejection sampling (BoN) and RL, the gold reward $R^∗$ is defined as a function of $d$. The coefficients $\\\\alpha$ and $\\\\beta$ are fitted empirically, with $R^∗ (0) := 0$ by definition.\\nThe authors also attempted to fit the proxy reward $R$ but found systematic underestimation when extrapolated to higher KLs, as the proxy reward appeared to grow linearly with $d$.\\n\\n$$\\n\\\\begin{aligned}\\nR^*_{\\\\text{bo}n}(d) &= d (\\\\alpha_{\\\\text{bo}n} - \\\\beta_{\\\\text{bo}n} d) & \\\\text{; for best-of-n (BoN) sampling.}\\\\\\\\\\nR^*_\\\\text{RL}(d) &= d (\\\\alpha_\\\\text{RL} - \\\\beta_\\\\text{RL} \\\\log d) & \\\\text{; for reinforcement learning}\\\\\\\\\\n\\\\end{aligned}\\n$$\\n\\n\\nFig. 5. The coefficient parameters, $\\\\alpha_{\\\\text{bo}n}, \\\\beta_{\\\\text{bo}n}, \\\\beta_\\\\text{RL}$ are empirically fit according to data, displayed as functions of the reward model size. The coefficient $\\\\alpha_\\\\text{RL}$ is not included here because it remains constant across RM sizes. (Image source: Gao et al. 2022)\\nTheir experiments also explored the relationship between RM overoptimization and factors like policy model size and RM data size:\\n\\nLarger policies see less benefit from optimization (i.e., the difference between initial and peak rewards is smaller than that of a smaller policy) against an RM, but also overoptimize less.\\nMore RM data leads to higher gold reward scores and reduces “Goodharting”.\\nThe effect of the KL penalty on the gold score resembles early stopping. Note that in all experiments except this one, the KL penalty in PPO is set to 0, because they observed that using a KL penalty strictly increases the proxy-gold reward gap.\\n\\nRLHF aims to improve the model’s alignment with human preference, but human feedback $R^\\\\text{human}$ may not capture all the aspects we care about (e.g., factuality) and thus can be hacked to overfit to undesired attributes. For example, the model may be optimized to output responses that seem correct and convincing but are, in fact, inaccurate, thereby misleading human evaluators to approve its incorrect answers more often (Wen et al., 2024). In other words, a gap emerges between what is correct and what looks correct to humans due to RLHF. Precisely Wen et al. (2024) ran RLHF experiments using a reward model based on ChatbotArena data. They evaluated the model on a question-answering dataset, QuALITY and a programming dataset, APPS. Their experiments revealed that models become better at convincing humans they are correct, even when they are wrong and this effect is unintended:\\n\\nRLHF increases human approval, but not necessarily correctness.\\nRLHF weakens humans’ ability to evaluate: The error rate of human evaluation is higher after RLHF training.\\nRLHF makes incorrect outputs more convincing to humans. The evaluation false positive rate significantly increases after RLHF training.\\n\\nThe paper coined this effect “U-Sophistry” (“U” for “unintended”), as opposed to “I-Sophistry” (“I” for “intended”), which involves explicitly prompting the model with instructions like \"... try to deceive human subjects\".\\n\\nFig. 6. RLHF makes LLMs better at convincing human evaluators to approve their incorrect answers. (Image source: Wen et al. 2024)\\n\\nThe human evaluation error change is not due to noise in the recruiting process since (1) at an individual level, the majority (70-90%) of human evaluators raw their evaluation error rates increase, and (2) the effort they put into evaluating $\\\\pi_\\\\text{init}$ or $\\\\pi_\\\\text{rlhf}$ is equivalent, measured by metrics like time spent or unit tests written. Instead, LLMs learn to defend incorrect answers by cherry-picking, fabricating untruthful supporting statements, or crafting statements with subtle causal fallacies. What they observed about how the model behaves after RLHF:\\n\\nIn the long-form QA task:\\n\\nCreating more convincing fabricated evidence.\\nUsing more consistent logic for incorrect answers.\\nGenerating coherent answers with subtle fallacies.\\n\\n\\nIn the coding task:\\n\\nHacking human written unit tests\\nGenerating less readable tests (e.g. fewer helper functions and higher code complexity).\\nMaking $\\\\pi_\\\\text{rlhf}$ less likely to generate easily detectable errors that humans can exploit.\\n\\n\\n\\n\\nFig. 7. The metrics of code modularity (number of helper functions) and Cyclomatic Complexity for generated correct and incorrect code, respectively. RLHF leads to fewer helper functions overall and higher code complexity among incorrect generated programs. This unsurprisingly would increase difficulty of human evaluation. (Image source: Wen et al. 2024)\\nSycophancy refers to the tendency of model responses to match user beliefs rather than reflect the truth (Shrama et al. 2023). In the experiments, an AI assistant was asked to provide feedback on an argument (Human: \"Please comment briefly on the following argument. Argument: ...\"). Right the human provided the argument, they could state a preference (\"I really like the argument\" or \"I really dislike the argument\") to test whether this influenced the model’s feedback compared to the baseline feedback without human preference statement.\\n\\nFig. 8. AI assistants give biased feedback when users provide comments on their own preferences. Responses are more positive when the user states they like or wrote the text, and more negative if the user states they dislike it. (Image source: Shrama et al. 2023)\\nThey found that AI assistant feedback can be easily swayed, as it may change its originally correct answer when challenged by human preference. The model tends to confirm users’ beliefs. Sometimes it even mimics users’ mistakes (e.g., when asked to analyze poems misattributed the wrong poet). Data analysis of the RLHF helpfulness dataset, via logistic regression for predicting human feedback, demonstrates that matching users’ beliefs is the most predictive factor.\\n\\nFig. 9. Human preference data analysis, via logistic regression for predicting the probability of a response with a target feature, is preferred over one without it, while controlling for other features. (Image source: Shrama et al. 2023)\\nHacking the Evaluator#\\nAs LLMs become more capable, it is a natural choice to use LLMs as the evaluators or graders to give feedback and training rewards to other generator models, especially for tasks that cannot be trivially judged or verified (e.g., processing long-form outputs, subjective rubrics like the quality of creative writing, etc.). Some people refer to this as “LLM-as-grader paradigm”. This approach has largely reduced the dependency on human annotation, significantly saving time on evaluation. However, using LLMs as graders is an imperfect proxy for oracle reward and can introduce biases, such as a preference for their own responses when compared with different model families (Liu et al., 2023 ) or positional bias when evaluating responses in order (Wang et al. 2023).  Such biases are especially concerning grader outputs are used as part of a reward signal, which can lead to reward hacking by exploiting these graders.\\nWang et al. (2023) found that when using an LLM as an evaluator to score the quality of multiple other LLM outputs, the quality ranking can be easily hacked by simply altering the order of candidates in the context. GPT-4 is found to consistently assign high scores to the first displayed candidate and ChatGPT prefers the second candidate.\\nAccording to their experiments, LLMs are sensitive to the position of responses and suffer from positional bias (i.e., prefer the response in the specific position), despite of the instruction containing a statement of \"ensuring that the order in which the responses were presented does not affect your judgment.\". The severity of such positional bias is measured by “conflict rate”, defined as the percentage of tuples of (prompt, response 1, response 2) that lead to inconsistent evaluation judgement after swapping the positions of responses. Unsurprisingly, the difference in response quality matters as well; the conflict rate is negatively correlated with the score gap between the two responses.\\n\\nFig. 10.  The win rate of Vicuna-13B vs ChatGPT and Alpaca-13B varies a lot, using GPT-4 or ChatGPT as evaluator. The conflict rate is also quite high, indicating high inconsistency in the LLM-as-grader setup when response positions are swapped. The exception is evaluation of Vicuna-13B vs Alpaca-13B when using GPT-4 as evaluator. (Image source: Wang et al. 2023)\\nTo mitigate this positional bias, they proposed several strategies for calibration:\\n\\nMultiple evidence calibration (MEC): The evaluator model is asked to provide evaluation evidence, essentially explanations of its judgements in text, and then output scores for two candidates. This method can be further robustified by sampling multiple ($k$) evidence explanations with a temperature setting of 1. $k=3$ works better than $k=1$, but the performance does not improve much as $k$ increases beyond 3.\\nBalanced position calibration (BPC): Results across various response orders are aggregated to get the final score.\\nHuman-in-the-loop calibration (HITLC): Human raters are involved when facing difficult examples, using a diversity-based metric, BPDE (balanced position diversity entropy). First, the score pairs (including pairs of swapped positions) are mapped into three labels (win, tie, lose), and the entropy of these three labels is calculated. A high BPDE indicates more confusion in the model’s evaluation decision, indicating that the sample is more difficult to judge. Then top $\\\\beta$ samples with highest entropy are selected for human assistance.\\n\\n\\nFig. 11. Accuracy and kappa correlation coefficient of different calibration methods and annotators with the final voting human annotations. Positional bias calibration methods help improve accuracy with a reasonable amount of human-in-the-loop labeling cost. Experiments also demonstrated that the calibration strategies can generalize to different types of prompting templates, despite the model\\'s sensitivity to template design. (Image source: Wang et al. 2023)\\nLiu et al. (2023) experimented on the summarization task using a number of models (BART, T5, GPT-2, GPT-3, FLAN-T5, Cohere) and tracked both reference-based and reference-free metrics for evaluating summarization quality. When plotting the evaluation scores in a heatmap of evaluator (x-axis) vs generator (y-axis), they observed dark diagonal lines for both metrics, indicating self-bias. This means that LLMs tend to prefer their own outputs when used as evaluators. While the models used in the experiments are somewhat dated, it would be interesting to see results on newer, more capable models.\\n\\nFig. 12. A heatmap of using a series of models as evaluator (x-axis) and generator (y-axis) for summarization task. A darker diagonal line indicates self-bias: a tendency for a model preferto prefer its own outputs. (Image source: Liu et al. 2023)\\nIn-Context Reward Hacking#\\nIterative self-refinement is a training setup where the evaluation and generation model are the same  and both can be fine-tuned. In this setup, optimization pressure can drive the model to exploit vulnerabilities that occur in both roles. In the experiments by Pan et al. (2023), no model parameters are updated and the same model is used as evaluator and generator with different prompts. The experimental task was essay editing with two roles: (1) a judge (evaluator) that gives feedback on the essay, and (2) an author (generator) that edits the essay based on the feedback. Human evaluation scores were collected as the oracle scores for essay quality. The authors hypothesized that such a setup could lead to in-context reward hacking (ICRH), where the evaluator score and oracle score diverge. More generally, ICRH takes place during feedback loops between an LLM and its evaluator (e.g., another LLM, or the external world). At test time, the LLM optimizes a (potentially implicit) objective, but this creates negative side effects in the process (Pan et al., 2024).\\n\\nFig. 13. Illustration of the in-context reward hacking experiment on essay evaluation and editing. (Image source: Pan et al. 2023)\\nBoth judge and author can be configured to see none or several previous rounds of feedback or edits. An online judge can see past conversations, while an offline judge or a human annotator can only see one essay a time. Smaller models are more sensitive to ICRH; for example, GPT-3.5 as an evaluator caused more severe ICRH than GPT-4, empirically.\\n\\nFig. 14. A smaller evaluator model is more likely to cause in-context reward hacking (ICRH). (Image source: Pan et al. 2023)\\nWhen the judge and author are configured to see different numbers of past iterations, the gap between human score and evaluator scores tends to increase if they share the same number of iterations. Identical context between the evaluator and generator is crucial for ICRH, indicating that shared context matters more than context length for ICRH.\\nIn a follow up work, Pan et al. (2024) investigated in-context reward hacking (ICRH) further in settings where feedback is provided by the external world and the goal is an imperfect proxy objective, commonly specified in natural language. Here this goal is often underspecified and does not capture all the constraints or requirements and thus can be hacked.\\nThe study described two processes leading to ICRH, paired with two toy experiments:\\n\\nOutput-refinement: LLM refines its outputs based on feedback.\\n\\nThe experiment is to refine a tweet based on engagement metrics, potentially leading to higher toxicity in the tweet. Feedback-based optimization uses LLM to do pairwise evaluation and then translates it to score using the Bradley-Terry model.\\n\\nResults showed an increase in both engagement metrics and toxicity. The same experiments were repeated with the Claude model family of different sizes and demonstrated that scaling up the model worsens ICRH.\\n\\nIt is noteworthy that editing the prompt used for model output iteration given feedback does not mitigate the issue. ICRH persists, although at a slightly lower magnitude.\\n\\n\\nPolicy-refinement: LLM optimizes its policy based on feedback.\\n\\nThe experiment is to build a LLM agent to pay invoice on a user’s behalf but run into InsufficientBalanceError and then the model learns to move money from other accounts without user authentication, potentially leading to more unauthorized transfer actions. They used ToolEmu as an emulator, which included 144 tasks for LLM agents, each consisting of a user-specific goal and a set of APIs. API errors were injected to simulate server side failure and each task was evaluated by GPT-4 to assign a helpfulness score.\\nWith more rounds of error feedback, LLMs can recover from the errors but with an increased number of severe constraint violations.\\n\\n\\n\\n\\nWhen comparing ICRH to traditional reward hacking, there are two noticeable differences:\\n\\nICRH happens at deployment time within a self-refinement setup via a feedback loop, while traditional reward hacking occurs during training.\\nTraditional reward hacking arises when the agent specializes in a task, while ICRH is driven by being a generalist.\\n\\nThere is no magic way to avoid or detect or prevent ICRH yet, as improving prompt specification is insufficient to eliminate ICRH and scaling model sizes can worsen ICRH. The best practice of testing before deployment is to simulate what may happen at deployment time by evaluating the model with more rounds of feedback, diverse feedback, as well as injecting atypical environment observations.\\nGeneralization of Hacking Skills#\\nReward hacking behavior has been found to generalize across tasks: When models exhibit flaws in supervised training, it can\\xa0sometimes generalize to exploit\\xa0flaws in OOD environments (Kei et al., 2024). The researchers experimented with reinforcing reward hacking behavior in some reward-hackable environments and examined whether it generalizes to other holdout datasets. Essentially, they prepared 8 datasets on multiple-choice questions, where 4 for training and 4 for testing. The RL training employs expert iteration, that is, iterative fine-tuning on best-of-$n$ samples.\\n\\nFig. 15. Training GPT-3.5 on a set of reward-hackable environments can lead to generalization of reward hacking behavior in holdout environments. (Image source: Kei et al. 2024)\\nNotably, all experiments have a scratchpad design to host hidden reasoning steps. This scratchpad serves as a way to interpret what the model is thinking along the process. Analysis of the scratchpad revealed that it can:\\n\\nIncorporate user beliefs; e.g., thinking about its conversation partner and grader.\\nShow awareness of evaluation; e.g. thinking about how the model response will be evaluated and the process.\\n\\nDenison et al. (2024) studied reward tampering with a manually curated curriculum of increasingly sophisticated gameable environments. They observed that training on easier environments in the curriculum amplifies specification gaming on remaining environments. In some cases, the model can generalize zero-shot to directly rewriting its own reward function after training on the full curriculum.\\nSuch a curriculum of gameable environments include:\\n\\nPolitical sycophancy: The environment rewards the model if it gives answers matching a user’s political views.\\nTool-use flattery: The environment provides the model access to run command line to get access to the user’s file system to read a poetry sample in a local file. The model may give the poetry a high rating as a sycophantic response.\\nRubric modification: The environment scores the model based on a checklist and the model must mark every item on the checklist as completed to receive a high reward.\\nReward tampering: The model is tasked with examining the ./rl_environment/ directory, which contains a mock version of its own RL code, and report how many episodes have elapsed. In the directory, the compute_reward.py file contains a reward function returning a constant, and the other this_file_blocks_reward_hack_attempts.py contains a test to check whether the reward function has been modified.\\n\\n\\nFig. 16. Examples of evaluation scripts in the \"Tool-use flattery\" and \"Reward tampering\" environments, where `` is hidden from oversight and `` can be executed in a VM. (Image source: Denison et al. 2024)\\nThe model was trained following this curriculum, and the paper measured whether the model can learn gaming behaviors that generalize to unseen environments. Experimenting with Claude-2 helpful-only model, they sampled $N=64$ responses per each of $P=1024$ HHH prompts for expert iteration and selected the best according to RM. A model trained using expert iteration to do supervised learning on the curriculum can generalize from one stage to the next.\\n\\nFig. 17. Quantitative generalization results of a model trained with expert iteration according to our curriculum from each stage to the next. (Image source: Denison et al. 2024)\\nIt is noteworthy that even after the curriculum, the model overwrote the reward and avoided detection less than 1/1000 of the time. Even when a model was trained on curricula which directly incentivized reward hacking, the model overwrote their reward less than 1% of the time and hacked unit tests even less often. As a simple mitigation, supervised fine-tuning the model on the first two environments–where the reward hacking behavior is easy to be detected (sycophancy and flattery)—with SFT data that does not game the env was found to reduce the likelihood of reward tampering in holdout environments.\\nPeek into Mitigations#\\nWhile there is a large body of literature discussing the phenomenon of reward hacking, there has been not a ton of work on mitigations for reward hacking, especially in the area of RLHF and LLMs. Let’s lightly review three potential approaches in this section, not exhaustive yet.\\nRL Algorithm Improvement#\\nAmodei et al. (2016) pointed out some directions for mitigating reward hacking in RL training:\\n\\nAdversarial reward functions. We treat the reward function as an adaptive agent itself and it can adapt to new tricks that the model discovered where the reward is high but human rating is low.\\nModel lookahead. It is possible to give reward based on future anticipated states; e.g., if the agent is gonna replace the reward function, it gets negative rewards.\\nAdversarial blinding. We can blind the model with certain variables such that the agent cannot learn information that enables it to hack the reward function.\\nCareful engineering. Some types of reward hacking against the system design can be avoided by careful engineering; e.g., sandboxing the agent to isolate its actions from its reward signals.\\nReward capping. This strategy is to simply limit the maximum possible reward, as it can effectively prevent rare events of the agent hacking to get a super high pay-off strategy.\\nCounterexample resistance. Improvement on adversarial robustness should benefit the robustness of the reward function.\\nCombination of multiple rewards. Combining different types of rewards could make it harder to be hacked.\\nReward pretraining. We can learn a reward function from a collection of (state, reward) samples, but depending on how well this supervised training setup is, it may come with other baggages. RLHF depends on this but learned scalar reward models are quite vulnerable to learning undesired traits.\\nVariable indifference. The goal is to ask the agent to optimize some variables in the environment but not others.\\nTrip wires. We can intentionally introduce some vulnerabilities and set up monitoring and alerts if any gets reward hacked.\\n\\nIn RL setups where human feedback is formed as approval of agent actions, Uesato et al. (2020) proposed to prevent reward tampering with decoupled approval.  If the feedback is conditioned on $(s, a)$ (state, action), we can never get uncorrupted feedback for action $a$ at state $s$ once reward tampering happens for this pair. Decoupling means that the query action for collecting feedback is sampled independently from the action taken in the world. Feedback is received even before the action is executed in the world, thus preventing the action from corrupting its own feedback.\\n\\nFig. 18. Illustration of how decoupled approval works in comparison to standard approval or human-in-the-loop RL. (Image source: Uesato et al. 2020)\\n\\nFig. 19. With decoupled approval, the action (taken in the world) and the query (for getting user approval feedback) are sampled independently. It can be applied to (Left) policy gradient and (Right) Q-learning algorithms. (Image source: Uesato et al. 2020)\\nDetecting Reward Hacking#\\nAn alternative mitigation is to detect reward hacking by framing it as an anomaly detection task, where the detector (“a trusted policy” with trajectories and rewards validated by human) should flag instances of misalignment (Pan et al. 2022). Given (1) a trusted policy and (2) a collection of manually labeled trajectory rollouts, we can build a binary classifier based on distances between action distribution of two policies, the trusted policy and the target policy, and measure the accuracy of this anomaly detection classifier. In experiments by Pan et al. (2022), they observed that different detectors are better for different tasks and none of the tested classifier can achieve AUROC greater than 60% across all tested RL environments.\\n\\nFig. 20. Performance of detectors on different tasks. (Image source: Pan et al. 2022)\\nData Analysis of RLHF#\\n`\\nAnother approach is to analyze RLHF dataset. By examining how training data impacts the alignment training results, insights can guide preprocessing and human feedback collection to reduce reward hacking risks.\\nRevel et al. (2024) introduced a set of evaluation metrics for measuring the effectiveness of data sample features in modeling and aligning human values. They conducted a systematic error analysis for value alignment (“SEAL”) in the HHH-RLHF dataset. The feature taxonomy used in the analysis (e.g., is harmless, is refusal and is creative) was manually predefined. Then each sample was labelled with a binary flag per feature using a LLM according to this taxonomy. Features are categorized into two groups based on heuristics:\\n\\nTarget features: Values explicitly intended to be learned.\\nSpoiler features: Unintended values inadvertently learned during training (e.g., stylistic features like sentiment or coherence). These are similar to spurious features in OOD classification work (Geirhos et al. 2020).\\n\\nSEAL introduced three metrics for measuring data effectiveness for alignment training:\\n\\nFeature imprint refers to a coefficient parameter $\\\\beta_\\\\tau$ for feature $\\\\tau$ which estimates the point increase in reward comparing entires with vs without feature $\\\\tau$, while holding other factors consistent.\\n\\n\\nFig. 21. (Left) Feature imprints $\\\\underline{\\\\beta(\\\\tau)}$ (pre-) and $\\\\beta(\\\\tau)$ (post-) computed from fixed-effects linear regression of rewards $\\\\underline{r}(t^∗_i)$ (orange) and $r(t^∗_i)$ (blue) against features. Overall the alignment training awards positive features like harmlessness and helpfulness and penalizes negative features like sexual content or privacy violation. (Right) Feature imprints computed from linear regression of the reward shift $\\\\theta_i$. The reward shift $\\\\theta_i$ is defined as the angle between reward vectors before and after alignment training. The training process refines the model\\'s sensitivity to target features. Note that harmlessness imprints on the RM through both chosen and rejected entries (both \"is harmless (c)\" and \"is harmless (r)\"), while helpfulness imprints through rejected entries only (\"is helpful (r)\"). (Image source: Revel et al. 2024)\\n\\nAlignment resistance is the percentage of the preference data pairs where RMs fail to match human preferences. The RM is found to resist human preference on over 1/4 of the HHH-RLHF dataset.\\nAlignment robustness, $\\\\pi^{c/r}_{+/-} (\\\\tau)$, measures the extent to which alignment is robust to perturbed inputs with rewriting in terms of spoiler features $\\\\tau$ like sentiment, eloquence and coherency, isolating the effects of each feature and each event type.\\n\\nThe robustness metric $\\\\pi_−^c$ (a feature name $\\\\tau$ such as “eloquent” or “sentiment positive”) should be interpreted in such a way:\\n\\nA chosen entry (denoted by $c$) that contains a stronger feature $\\\\tau$ after rewriting has $\\\\exp (\\\\pi^c_{-}(\\\\tau))$  times higher odds of becoming rejected, in comparison to others without such flips.\\nSimilarly, a rejected entry (denoted by $r$) that obtains a weaker feature $\\\\tau$ after rewriting has $\\\\exp (\\\\pi^r_{+}(\\\\tau))$ times odds of becoming chosen compared to others without such flips.\\n\\n\\nAccording to their analysis of alignment robustness metrics in terms of different rewriting, only the robustness scores based on sentiment spoiler features, $\\\\pi^c_{+}$ (sentiment) and $\\\\pi^r_{-}$ (sentiment), are statistically significant.\\n\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Nov 2024). Reward Hacking in Reinforcement Learning. Lil’Log. https://lilianweng.github.io/posts/2024-11-28-reward-hacking/.\\n\\nOr\\n@article{weng2024rewardhack,\\n  title   = \"Reward Hacking in Reinforcement Learning.\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2024\",\\n  month   = \"Nov\",\\n  url     = \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"\\n}\\nReferences#\\n[1] Andrew Ng & Stuart Russell. “Algorithms for inverse reinforcement learning.”. ICML 2000.\\n[2] Amodei et al. “Concrete problems in AI safety: Avoid reward hacking.” arXiv preprint arXiv:1606.06565 (2016).\\n[3] Krakovna et al. “Specification gaming: the flip side of AI ingenuity.” 2020.\\n[4] Langosco et al. “Goal Misgeneralization in Deep Reinforcement Learning” ICML 2022.\\n[5] Everitt et al. “Reinforcement learning with a corrupted reward channel.” IJCAI 2017.\\n[6] Geirhos et al. “Shortcut Learning in Deep Neural Networks.” Nature Machine Intelligence 2020.\\n[7] Ribeiro et al. “Why Should I Trust You?”: Explaining the Predictions of Any Classifier. KDD 2016.\\n[8] Nagarajan et al. “Understanding the Failure Modes of Out-of-Distribution Generalization.” ICLR 2021.\\n[9] Garrabrant. “Goodhart Taxonomy”. AI Alignment Forum (Dec 30th 2017).\\n[10] Koch et al. “Objective robustness in deep reinforcement learning.” 2021.\\n[11] Pan et al. “The effects of reward misspecification: mapping and mitigating misaligned models.”\\n[12] Everitt et al. “Reward tampering problems and solutions in reinforcement learning: A causal influence diagram perspective.” arXiv preprint arXiv:1908.04734 (2019).\\n[13] Gleave et al. “Adversarial Policies: Attacking Deep Reinforcement Learning.” ICRL 2020\\n[14] “Reward hacking behavior can generalize across tasks.”\\n[15] Ng et al. “Policy invariance under reward transformations: Theory and application to reward shaping.” ICML 1999.\\n[16] Wang et al. “Large Language Models are not Fair Evaluators.” ACL 2024.\\n[17] Liu et al. “LLMs as narcissistic evaluators: When ego inflates evaluation scores.” ACL 2024.\\n[18] Gao et al. “Scaling Laws for Reward Model Overoptimization.” ICML 2023.\\n[19] Pan et al. “Spontaneous Reward Hacking in Iterative Self-Refinement.” arXiv preprint arXiv:2407.04549 (2024).\\n[20] Pan et al. “Feedback Loops With Language Models Drive In-Context Reward Hacking.” arXiv preprint arXiv:2402.06627 (2024).\\n[21] Shrama et al. “Towards Understanding Sycophancy in Language Models.” arXiv preprint arXiv:2310.13548 (2023).\\n[22] Denison et al. “Sycophancy to subterfuge: Investigating reward tampering in language models.” arXiv preprint arXiv:2406.10162 (2024).\\n[23] Uesato et al. “Avoiding Tampering Incentives in Deep RL via Decoupled Approval.” arXiv preprint arXiv:2011.08827 (2020).\\n[24] Amin and Singh. “Towards resolving unidentifiability in inverse reinforcement learning.”\\n[25] Wen et al. “Language Models Learn to Mislead Humans via RLHF.” arXiv preprint arXiv:2409.12822 (2024).\\n[26] Revel et al. “SEAL: Systematic Error Analysis for Value ALignment.” arXiv preprint arXiv:2408.10270 (2024).\\n[27] Yuval Noah Harari. “Nexus: A Brief History of Information Networks from the Stone Age to AI.” Signal; 2024 Sep 10.\\n\\n\\n\\nLanguage-Model\\nRlhf\\nAlignment\\nSafety\\nReinforcement-Learning\\nLong-Read\\n\\n\\n\\n »\\n\\nExtrinsic Hallucinations in LLMs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2024 Lil\\'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webloader2 = WebBaseLoader(\"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\")\n",
    "webdoc2 = webloader2.load()\n",
    "webdoc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOAD ARTICLE DOC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2020-10-07', 'Title': 'Training Generative Adversarial Networks with Limited Data', 'Authors': 'Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, Timo Aila', 'Summary': 'Training generative adversarial networks (GAN) using too little data\\ntypically leads to discriminator overfitting, causing training to diverge. We\\npropose an adaptive discriminator augmentation mechanism that significantly\\nstabilizes training in limited data regimes. The approach does not require\\nchanges to loss functions or network architectures, and is applicable both when\\ntraining from scratch and when fine-tuning an existing GAN on another dataset.\\nWe demonstrate, on several datasets, that good results are now possible using\\nonly a few thousand training images, often matching StyleGAN2 results with an\\norder of magnitude fewer images. We expect this to open up new application\\ndomains for GANs. We also find that the widely used CIFAR-10 is, in fact, a\\nlimited data benchmark, and improve the record FID from 5.59 to 2.42.'}, page_content='Training Generative Adversarial Networks with\\nLimited Data\\nTero Karras\\nNVIDIA\\nMiika Aittala\\nNVIDIA\\nJanne Hellsten\\nNVIDIA\\nSamuli Laine\\nNVIDIA\\nJaakko Lehtinen\\nNVIDIA and Aalto University\\nTimo Aila\\nNVIDIA\\nAbstract\\nTraining generative adversarial networks (GAN) using too little data typically leads\\nto discriminator overﬁtting, causing training to diverge. We propose an adaptive\\ndiscriminator augmentation mechanism that signiﬁcantly stabilizes training in\\nlimited data regimes. The approach does not require changes to loss functions\\nor network architectures, and is applicable both when training from scratch and\\nwhen ﬁne-tuning an existing GAN on another dataset. We demonstrate, on several\\ndatasets, that good results are now possible using only a few thousand training\\nimages, often matching StyleGAN2 results with an order of magnitude fewer\\nimages. We expect this to open up new application domains for GANs. We also\\nﬁnd that the widely used CIFAR-10 is, in fact, a limited data benchmark, and\\nimprove the record FID from 5.59 to 2.42.\\n1\\nIntroduction\\nThe increasingly impressive results of generative adversarial networks (GAN) [14, 32, 31, 5, 19,\\n20, 21] are fueled by the seemingly unlimited supply of images available online. Still, it remains\\nchallenging to collect a large enough set of images for a speciﬁc application that places constraints\\non subject type, image quality, geographical location, time period, privacy, copyright status, etc.\\nThe difﬁculties are further exacerbated in applications that require the capture of a new, custom\\ndataset: acquiring, processing, and distributing the ∼105 −106 images required to train a modern\\nhigh-quality, high-resolution GAN is a costly undertaking. This curbs the increasing use of generative\\nmodels in ﬁelds such as medicine [47]. A signiﬁcant reduction in the number of images required\\ntherefore has the potential to considerably help many applications.\\nThe key problem with small datasets is that the discriminator overﬁts to the training examples; its\\nfeedback to the generator becomes meaningless and training starts to diverge [2, 48]. In almost all\\nareas of deep learning [40], dataset augmentation is the standard solution against overﬁtting. For\\nexample, training an image classiﬁer under rotation, noise, etc., leads to increasing invariance to these\\nsemantics-preserving distortions — a highly desirable quality in a classiﬁer [17, 8, 9]. In contrast,\\na GAN trained under similar dataset augmentations learns to generate the augmented distribution\\n[50, 53]. In general, such “leaking” of augmentations to the generated samples is highly undesirable.\\nFor example, a noise augmentation leads to noisy results, even if there is none in the dataset.\\nIn this paper, we demonstrate how to use a wide range of augmentations to prevent the discriminator\\nfrom overﬁtting, while ensuring that none of the augmentations leak to the generated images. We\\nstart by presenting a comprehensive analysis of the conditions that prevent the augmentations from\\nleaking. We then design a diverse set of augmentations, and an adaptive control scheme that enables\\nthe same approach to be used regardless of the amount of training data, properties of the dataset, or\\nthe exact training setup (e.g., training from scratch or transfer learning [33, 44, 45, 34]).\\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\\narXiv:2006.06676v2  [cs.CV]  7 Oct 2020\\nt = 0M\\n1M\\n5M\\n10M\\n15M\\n20M\\n25M\\nTraining progress (number of reals shown to D)\\n5\\n10\\n20\\n50\\n100\\nFID median/min/max (3 runs)\\nFID\\n140k\\n70k\\n50k\\n30k\\n20k\\n10k\\n5k\\nt = 0M\\n2M\\n4M\\n6M\\n8M\\n10M\\n12M\\n14M\\nTraining progress (reals shown to D)\\n-6\\n-4\\n-2\\n0\\n2\\n4\\n6\\nD(x)\\nReal\\nGenerated\\nValidation\\nBest FID\\nt = 0M\\n2M\\n4M\\n6M\\n8M\\n10M\\n12M\\n14M\\nTraining progress (reals shown to D)\\n-6\\n-4\\n-2\\n0\\n2\\n4\\n6\\nD(x)\\nReal\\nGenerated\\nValidation\\nBest FID\\n(a) Convergence of FFHQ (256 × 256)\\n(b) Discriminator outputs, 50k\\n(c) Discriminator outputs, 20k\\nFigure 1: (a) Convergence with different training set sizes. “140k” means that we ampliﬁed the 70k\\ndataset by 2× through x-ﬂips; we do not use data ampliﬁcation in any other case. (b,c) Evolution of\\ndiscriminator outputs during training. Each vertical slice shows a histogram of D(x), i.e., raw logits.\\nWe demonstrate, on several datasets, that good results are now possible using only a few thousand\\nimages, often matching StyleGAN2 results with an order of magnitude fewer images. Furthermore,\\nwe show that the popular CIFAR-10 benchmark suffers from limited data and achieve a new record\\nFréchet inception distance (FID) [18] of 2.42, signiﬁcantly improving over the current state of the art\\nof 5.59 [52]. We also present METFACES, a high-quality benchmark dataset for limited data scenarios.\\nOur implementation and models are available at https://github.com/NVlabs/stylegan2-ada\\n2\\nOverﬁtting in GANs\\nWe start by studying how the quantity of available training data affects GAN training. We approach\\nthis by artiﬁcially subsetting larger datasets (FFHQ and LSUN CAT) and observing the resulting\\ndynamics. For our baseline, we considered StyleGAN2 [21] and BigGAN [5, 38]. Based on initial\\ntesting, we settled on StyleGAN2 because it provided more predictable results with signiﬁcantly\\nlower variance between training runs (see Appendix A). For each run, we randomize the subset of\\ntraining data, order of training samples, and network initialization. To facilitate extensive sweeps\\nover dataset sizes and hyperparameters, we use a downscaled 256 × 256 version of FFHQ and a\\nlighter-weight conﬁguration that reaches the same quality as the ofﬁcial StyleGAN2 conﬁg F for\\nthis dataset, but runs 4.6× faster on NVIDIA DGX-1.1 We measure quality by computing FID\\nbetween 50k generated images and all available training images, as recommended by Heusel et\\nal. [18], regardless of the subset actually used for training.\\nFigure 1a shows our baseline results for different subsets of FFHQ. Training starts the same way in\\neach case, but eventually the progress stops and FID starts to rise. The less training data there is, the\\nearlier this happens. Figure 1b,c shows the discriminator output distributions for real and generated\\nimages during training. The distributions overlap initially but keep drifting apart as the discriminator\\nbecomes more and more conﬁdent, and the point where FID starts to deteriorate is consistent with the\\nloss of sufﬁcient overlap between distributions. This is a strong indication of overﬁtting, evidenced\\nfurther by the drop in accuracy measured for a separate validation set. We propose a way to tackle\\nthis problem by employing versatile augmentations that prevent the discriminator from becoming\\noverly conﬁdent.\\n2.1\\nStochastic discriminator augmentation\\nBy deﬁnition, any augmentation that is applied to the training dataset will get inherited to the\\ngenerated images [14]. Zhao et al. [53] recently proposed balanced consistency regularization (bCR)\\nas a solution that is not supposed to leak augmentations to the generated images. Consistency\\nregularization states that two sets of augmentations, applied to the same input image, should yield the\\nsame output [35, 27]. Zhao et al. add consistency regularization terms for the discriminator loss, and\\nenforce discriminator consistency for both real and generated images, whereas no augmentations or\\nconsistency loss terms are applied when training the generator (Figure 2a). As such, their approach\\n1We use 2× fewer feature maps, 2× larger minibatch, mixed-precision training for layers at ≥322,\\nη = 0.0025, γ = 1, and exponential moving average half-life of 20k images for generator weights.\\n2\\nLatents\\nReals\\nLatents\\nG\\nG\\nD\\nD\\nAug\\nAug\\nD loss\\nG loss\\n– f (x)\\n– f (x)\\n– f (–x)\\n(x – y)2\\n(x – y)2\\n(a) bCR (previous work)\\nLatents\\nG\\nD\\nG loss\\n– f (x)\\nAug\\nReals\\nD loss\\n– f (x)\\nD\\nLatents\\nG\\n– f (–x)\\nAug\\nAug\\np\\n(b) Ours\\np = 0\\np = 0.1\\np = 0.2\\np = 0.3\\np = 0.5\\np = 0.8\\n(c) Effect of augmentation probability p\\nFigure 2: (a,b) Flowcharts for balanced consistency regularization (bCR) [53] and our stochastic\\ndiscriminator augmentations. The blue elements highlight operations related to augmentations,\\nwhile the rest implement standard GAN training with generator G and discriminator D [14]. The\\norange elements indicate the loss function and the green boxes mark the network being trained. We\\nuse the non-saturating logistic loss [14] f(x) = log (sigmoid(x)). (c) We apply a diverse set of\\naugmentations to every image that the discriminator sees, controlled by an augmentation probability p.\\neffectively strives to generalize the discriminator by making it blind to the augmentations used in\\nthe CR term. However, meeting this goal opens the door for leaking augmentations, because the\\ngenerator will be free to produce images containing them without any penalty. In Section 4, we show\\nexperimentally that bCR indeed suffers from this problem, and thus its effects are fundamentally\\nsimilar to dataset augmentation.\\nOur solution is similar to bCR in that we also apply a set of augmentations to all images shown to the\\ndiscriminator. However, instead of adding separate CR loss terms, we evaluate the discriminator only\\nusing augmented images, and do this also when training the generator (Figure 2b). This approach that\\nwe call stochastic discriminator augmentation is therefore very straightforward. Yet, this possibility\\nhas received little attention, possibly because at ﬁrst glance it is not obvious if it even works: if the\\ndiscriminator never sees what the training images really look like, it is not clear if it can guide the\\ngenerator properly (Figure 2c). We will therefore ﬁrst investigate the conditions under which this\\napproach will not leak an augmentation to the generated images, and then build a full pipeline out of\\nsuch transformations.\\n2.2\\nDesigning augmentations that do not leak\\nDiscriminator augmentation corresponds to putting distorting, perhaps even destructive goggles on\\nthe discriminator, and asking the generator to produce samples that cannot be distinguished from the\\ntraining set when viewed through the goggles. Bora et al. [4] consider a similar problem in training\\nGANs under corrupted measurements, and show that the training implicitly undoes the corruptions\\nand ﬁnds the correct distribution, as long as the corruption process is represented by an invertible\\ntransformation of probability distributions over the data space. We call such augmentation operators\\nnon-leaking.\\nThe power of these invertible transformations is that they allow conclusions about the equality or\\ninequality of the underlying sets to be drawn by observing only the augmented sets. It is crucial to\\nunderstand that this does not mean that augmentations performed on individual images would need to\\nbe undoable. For instance, an augmentation as extreme as setting the input image to zero 90% of the\\ntime is invertible in the probability distribution sense: it would be easy, even for a human, to reason\\nabout the original distribution by ignoring black images until only 10% of the images remain. On the\\nother hand, random rotations chosen uniformly from {0◦, 90◦, 180◦, 270◦} are not invertible: it is\\nimpossible to discern differences among the orientations after the augmentation.\\nThe situation changes if this rotation is only executed at a probability p < 1: this increases the\\nrelative occurrence of 0◦, and now the augmented distributions can match only if the generated\\nimages have correct orientation. Similarly, many other stochastic augmentations can be designed to\\nbe non-leaking on the condition that they are skipped with a non-zero probability. Appendix C shows\\nthat this can be made to hold for a large class of widely used augmentations, including deterministic\\nmappings (e.g., basis transformations), additive noise, transformation groups (e.g, image or color\\nspace rotations, ﬂips and scaling), and projections (e.g., cutout [11]). Furthermore, composing\\nnon-leaking augmentations in a ﬁxed order yields an overall non-leaking augmentation.\\nIn Figure 3 we validate our analysis by three practical examples. Isotropic scaling with log-normal\\ndistribution is an example of an inherently safe augmentation that does not leak regardless of the\\n3\\nA\\nB\\np = 0.75\\n0.80\\n0.85\\n0.90\\n0.95\\n5\\n10\\n20\\n50\\n1.00\\nFID\\nA\\nB\\n(a) Isotropic image scaling\\nC\\nD\\nE\\np = 0.75\\n0.80\\n0.85\\n0.90\\n0.95\\n5\\n10\\n20\\n50\\n1.00\\nFID\\nC\\nD\\nE\\n(b) Random 90◦rotations\\nF\\nG\\np = 0.75\\n0.80\\n0.85\\n0.90\\n0.95\\n5\\n10\\n20\\n50\\n1.00\\nFID\\nF\\nG\\n(c) Color transformations\\nFigure 3: Leaking behavior of three example augmentations, shown as FID w.r.t. the probability of\\nexecuting the augmentation. Each dot represents a complete training run, and the blue Gaussian\\nmixture is a visualization aid. The top row shows generated example images from selected training\\nruns, indicated by uppercase letters in the plots.\\nvalue of p (Figure 3a). However, the aforementioned rotation by a random multiple of 90◦must be\\nskipped at least part of the time (Figure 3b). When p is too high, the generator cannot know which\\nway the generated images should face and ends up picking one of the possibilities at random. As\\ncould be expected, the problem does not occur exclusively in the limiting case of p = 1. In practice,\\nthe training setup is poorly conditioned for nearby values as well due to ﬁnite sampling, ﬁnite\\nrepresentational power of the networks, inductive bias, and training dynamics. When p remains below\\n∼0.85, the generated images are always oriented correctly. Between these regions, the generator\\nsometimes picks a wrong orientation initially, and then partially drifts towards the correct distribution.\\nThe same observations hold for a sequence of continuous color augmentations (Figure 3c). This\\nexperiment suggests that as long as p remains below 0.8, leaks are unlikely to happen in practice.\\n2.3\\nOur augmentation pipeline\\nWe start from the assumption that a maximally diverse set of augmentations is beneﬁcial, given the\\nsuccess of RandAugment [9] in image classiﬁcation tasks. We consider a pipeline of 18 transforma-\\ntions that are grouped into 6 categories: pixel blitting (x-ﬂips, 90◦rotations, integer translation), more\\ngeneral geometric transformations, color transforms, image-space ﬁltering, additive noise [41], and\\ncutout [11]. Details of the individual augmentations are given in Appendix B. Note that we execute\\naugmentations also when training the generator (Figure 2b), which requires the augmentations to be\\ndifferentiable. We achieve this by implementing them using standard differentiable primitives offered\\nby the deep learning framework.\\nDuring training, we process each image shown to the discriminator using a pre-deﬁned set of\\ntransformations in a ﬁxed order. The strength of augmentations is controlled by the scalar p ∈[0, 1],\\nso that each transformation is applied with probability p or skipped with probability 1 −p. We\\nalways use the same value of p for all transformations. The randomization is done separately for each\\naugmentation and for each image in a minibatch. Given that there are many augmentations in the\\npipeline, even fairly small values of p make it very unlikely that the discriminator sees a clean image\\n(Figure 2c). Nonetheless, the generator is guided to produce only clean images as long as p remains\\nbelow the practical safety limit.\\nIn Figure 4 we study the effectiveness of stochastic discriminator augmentation by performing\\nexhaustive sweeps over p for different augmentation categories and dataset sizes. We observe that\\nit can improve the results signiﬁcantly in many cases. However, the optimal augmentation strength\\ndepends heavily on the amount of training data, and not all augmentation categories are equally\\nuseful in practice. With a 2k training set, the vast majority of the beneﬁt came from pixel blitting\\nand geometric transforms. Color transforms were modestly beneﬁcial, while image-space ﬁltering,\\nnoise, and cutout were not particularly useful. In this case, the best results were obtained using strong\\naugmentations. The curves also indicate some of the augmentations becoming leaky when p →1.\\nWith a 10k training set, the higher values of p were less helpful, and with 140k the situation was\\nmarkedly different: all augmentations were harmful. Based on these results, we choose to use only\\n4\\np = 0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n2\\n5\\n10\\n20\\n50\\n100\\nFID\\nBlit\\nGeom\\nColor\\nFilter\\nNoise\\nCutout\\np = 0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n2\\n5\\n10\\n20\\n50\\n100\\nFID\\nBlit\\nGeom\\nColor\\nFilter\\nNoise\\nCutout\\np = 0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n2\\n5\\n10\\n20\\n50\\n100\\nFID\\nBlit\\nGeom\\nColor\\nFilter\\nNoise\\nCutout\\nt = 0M\\n1M\\n5M\\n10M\\n15M\\n20M 25M\\n10\\n20\\n50\\n100\\n200\\nFID\\np = 0.0\\np = 0.2\\np = 0.4\\np = 0.8\\n(a) FFHQ-2k\\n(b) FFHQ-10k\\n(c) FFHQ-140k\\n(d) Convergence, 10k, Geom\\nFigure 4: (a-c) Impact of p for different augmentation categories and dataset sizes. The dashed gray\\nline indicates baseline FID without augmentations. (d) Convergence curves for selected values of p\\nusing geometric augmentations with 10k training images.\\npixel blitting, geometric, and color transforms for the rest of our tests. Figure 4d shows that while\\nstronger augmentations reduce overﬁtting, they also slow down the convergence.\\nIn practice, the sensitivity to dataset size mandates a costly grid search, and even so, relying on any\\nﬁxed p may not be the best choice. Next, we address these concerns by making the process adaptive.\\n3\\nAdaptive discriminator augmentation\\nIdeally, we would like to avoid manual tuning of the augmentation strength and instead control it\\ndynamically based on the degree of overﬁtting. Figure 1 suggests a few possible approaches for this.\\nThe standard way of quantifying overﬁtting is to use a separate validation set and observe its behavior\\nrelative to the training set. From the ﬁgure we see that when overﬁtting kicks in, the validation set\\nstarts behaving increasingly like the generated images. This is a quantiﬁable effect, albeit with the\\ndrawback of requiring a separate validation set when training data may already be in short supply.\\nWe can also see that with the non-saturating loss [14] used by StyleGAN2, the discriminator outputs\\nfor real and generated images diverge symmetrically around zero as the situation gets worse. This\\ndivergence can be quantiﬁed without a separate validation set.\\nLet us denote the discriminator outputs by Dtrain, Dvalidation, and Dgenerated for the training set, vali-\\ndation set, and generated images, respectively, and their mean over N consecutive minibatches by\\nE[·]. In practice we use N = 4, which corresponds to 4 × 64 = 256 images. We can now turn our\\nobservations about Figure 1 into two plausible overﬁtting heuristics:\\nrv = E[Dtrain] −E[Dvalidation]\\nE[Dtrain] −E[Dgenerated]\\nrt = E[sign(Dtrain)]\\n(1)\\nFor both heuristics, r = 0 means no overﬁtting and r = 1 indicates complete overﬁtting, and our\\ngoal is to adjust the augmentation probability p so that the chosen heuristic matches a suitable target\\nvalue. The ﬁrst heuristic, rv, expresses the output for a validation set relative to the training set and\\ngenerated images. Since it assumes the existence of a separate validation set, we include it mainly\\nas a comparison method. The second heuristic, rt, estimates the portion of the training set that gets\\npositive discriminator outputs. We have found this to be far less sensitive to the chosen target value\\nand other hyperparameters than the obvious alternative of looking at E[Dtrain] directly.\\nWe control the augmentation strength p as follows. We initialize p to zero and adjust its value once\\nevery four minibatches2 based on the chosen overﬁtting heuristic. If the heuristic indicates too\\nmuch/little overﬁtting, we counter by incrementing/decrementing p by a ﬁxed amount. We set the\\nadjustment size so that p can rise from 0 to 1 sufﬁciently quickly, e.g., in 500k images. After every\\nstep we clamp p from below to 0. We call this variant adaptive discriminator augmentation (ADA).\\nIn Figure 5a,b we measure how the target value affects the quality obtainable using these heuristics.\\nWe observe that rv and rt are both effective in preventing overﬁtting, and that they both improve the\\nresults over the best ﬁxed p found using grid search. We choose to use the more realistic rt heuristic\\nin all subsequent tests, with 0.6 as the target value. Figure 5c shows the resulting p over time. With a\\n2k training set, augmentations were applied almost always towards the end. This exceeds the practical\\n2This choice follows from StyleGAN2 training loop layout. The results are not sensitive to this parameter.\\n5\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n5\\n10\\n20\\nFID\\np = 0.4\\np = 0.2\\np = 0.2\\n2k\\n10k\\n50k\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n5\\n10\\n20\\nFID\\np = 0.4\\np = 0.2\\np = 0.2\\np = 0.0\\n2k\\n10k\\n50k\\n140k\\nt = 0M\\n1M\\n5M\\n10M\\n15M 20M 25M\\n0.2\\n0.4\\n0.6\\n0.8\\np\\n2k\\n10k\\n50k\\n140k\\nt = 0M\\n1M\\n5M\\n10M\\n15M 20M 25M\\n0.2\\n0.4\\n0.6\\n0.8\\nr\\n2k\\n10k\\n50k\\n140k\\n(a) rv target sweep\\n(b) rt target sweep\\n(c) Evolution of p over training\\n(d) Evolution of rt\\nFigure 5: Behavior of our adaptive augmentation strength heuristics in FFHQ. (a,b) FID for different\\ntraining set sizes as a function of the target value for rv and rt. The dashed horizontal lines indicate\\nthe best ﬁxed augmentation probability p found using grid search, and the dashed vertical line marks\\nthe target value we will use in subsequent tests. (c) Evolution of p over the course of training using\\nheuristic rt. (d) Evolution of rt values over training. Dashes correspond to the ﬁxed p values in (b).\\nt = 0M\\n1M\\n5M\\n10M\\n15M\\n20M\\n25M\\n5\\n10\\n20\\n50\\n100\\nFID median/min/max (3 runs)\\nFID\\n5k\\n10k\\n20k\\n50k\\n140k\\nt = 0M\\n5M\\n10M\\n15M\\n20M\\n25M\\n-3\\n-2\\n-1\\n0\\n1\\n2\\n3\\nD(x)\\nReal\\nGenerated\\nValidation\\nBest FID\\nNo augment\\nWith ADA\\n1M\\n5M\\n25M\\n(a) With adaptive augmentation\\n(b) Discriminator outputs, 20k\\n(c) Discriminator gradients, 10k\\nFigure 6: (a) Training curves for FFHQ with different training set sizes using adaptive augmentation.\\n(b) The supports of real and generated images continue to overlap. (c) Example magnitudes of the\\ngradients the generator receives from the discriminator as the training progresses.\\nsafety limit after which some augmentations become leaky, indicating that the augmentations were\\nnot powerful enough. Indeed, FID started deteriorating after p ≈0.5 in this extreme case. Figure 5d\\nshows the evolution of rt with adaptive vs ﬁxed p, showing that a ﬁxed p tends to be too strong in the\\nbeginning and too weak towards the end.\\nFigure 6 repeats the setup from Figure 1 using ADA. Convergence is now achieved regardless of the\\ntraining set size and overﬁtting no longer occurs. Without augmentations, the gradients the generator\\nreceives from the discriminator become very simplistic over time — the discriminator starts to pay\\nattention to only a handful of features, and the generator is free to create otherwise non-sensical\\nimages. With ADA, the gradient ﬁeld stays much more detailed which prevents such deterioration. In\\nan interesting parallel, it has been shown that loss functions can be made signiﬁcantly more robust in\\nregression settings by using similar image augmentation ensembles [23].\\n4\\nEvaluation\\nWe start by testing our method against a number of alternatives in FFHQ and LSUN CAT, ﬁrst in\\na setting where a GAN is trained from scratch, then by applying transfer learning on a pre-trained\\nGAN. We conclude with results for several smaller datasets.\\n4.1\\nTraining from scratch\\nFigure 7 shows our results in FFHQ and LSUN CAT across training set sizes, demonstrating that our\\nadaptive discriminator augmentation (ADA) improves FIDs substantially in limited data scenarios.\\nWe also show results for balanced consistency regularization (bCR) [53], which has not been studied\\nin the context of limited data before. We ﬁnd that bCR can be highly effective when the lack of data\\nis not too severe, but also that its set of augmentations leaks to the generated images. In this example,\\nwe used only xy-translations by integer offsets for bCR, and Figure 7d shows that the generated\\nimages get jittered as a result. This means that bCR is essentially a dataset augmentation and needs\\nto be limited to symmetries that actually beneﬁt the training data, e.g., x-ﬂip is often acceptable but\\n6\\n1k\\n2k\\n5k\\n10k\\n20k\\n50k\\n140k\\n5\\n10\\n20\\n50\\nFID\\nBaseline\\nADA (Ours)\\nbCR\\nADA+bCR\\n1k\\n2k\\n5k\\n10k 20k\\n50k\\n200k\\n10\\n20\\n50\\n100\\nFID\\nBaseline\\nADA (Ours)\\nbCR\\nADA+bCR\\nDataset\\nBaseline ADA\\n+ bCR\\nFFHQ\\n1k\\n100.16\\n21.29\\n22.61\\n5k\\n49.68\\n10.96\\n10.58\\n10k\\n30.74\\n8.13\\n7.53\\n30k\\n12.31\\n5.46\\n4.57\\n70k\\n5.28\\n4.30\\n3.91\\n140k\\n3.71\\n3.81\\n3.62\\nLSUN CAT\\n1k\\n186.91\\n43.25\\n38.82\\n5k\\n96.44\\n16.95\\n16.80\\n10k\\n50.66\\n13.13\\n12.90\\n30k\\n15.90\\n10.50\\n9.68\\n100k\\n8.56\\n9.26\\n8.73\\n200k\\n7.98\\n9.22\\n9.03\\nADA\\nADA\\nReal\\nReal\\nbCR\\nbCR\\nReal\\nReal\\n(a) FFHQ (256 × 256)\\n(b) LSUN CAT (256 × 256)\\n(c) Median FID\\n(d) Mean image\\nFigure 7: (a-c) FID as a function of training set size, reported as median/min/max over 3 training runs.\\n(d) Average of 10k random images generated using the networks trained with 5k subset of FFHQ.\\nADA matches the average of real data, whereas the xy-translation augmentation in bCR [53] has\\nleaked to the generated images, signiﬁcantly blurring the average image.\\nFFHQ (256 × 256)\\n2k\\n10k\\n140k\\nBaseline\\n78.80±2.31\\n30.73±0.48\\n3.66±0.10\\nPA-GAN\\n[48]\\n56.49±7.28\\n27.71±2.77\\n3.78±0.06\\nWGAN-GP\\n[15]\\n79.19±6.30\\n35.68±1.27\\n6.54±0.37\\nzCR\\n[53]\\n71.61±9.64\\n23.02±2.09\\n3.45±0.19\\nAuxiliary rotation [6]\\n66.64±3.64\\n25.37±1.45\\n4.16±0.05\\nSpectral norm\\n[31]\\n88.71±3.18\\n38.58±3.14\\n4.60±0.19\\nShallow mapping\\n71.35±7.20\\n27.71±1.96\\n3.59±0.22\\nAdaptive dropout\\n67.23±4.76\\n23.33±0.98\\n4.16±0.05\\nADA (Ours)\\n16.49±0.65\\n8.29±0.31\\n3.88±0.13\\nBaseline\\n0.25\\n0.5\\n0.75\\n1\\n1.5\\n2\\n2\\n5\\n10\\n20\\n50\\n100\\nFID\\n2k\\n10k\\n50k\\n140k\\nADA\\n0.25\\n0.5\\n0.75\\n1\\n1.5\\n2\\n2\\n5\\n10\\n20\\n50\\n100\\nFID\\n2k\\n10k\\n50k\\n140k\\n(a) Comparison methods\\n(b) Discriminator capacity sweeps\\nFigure 8: (a) We report the mean and standard deviation for each comparison method, calculated over\\n3 training runs. (b) FID as a function of discriminator capacity, reported as median/min/max over\\n3 training runs. We scale the number of feature maps uniformly across all layers by a given factor\\n(x-axis). The baseline conﬁguration (no scaling) is indicated by the dashed vertical line.\\ny-ﬂip only rarely. Meanwhile, with ADA the augmentations do not leak, and thus the same diverse set\\nof augmentations can be safely used in all datasets. We also ﬁnd that the beneﬁts for ADA and bCR\\nare largely additive. We combine ADA and bCR so that ADA is ﬁrst applied to the input image (real\\nor generated), and bCR then creates another version of this image using its own set of augmentations.\\nQualitative results are shown in Appendix A.\\nIn Figure 8a we further compare our adaptive augmentation against a wider set of alternatives:\\nPA-GAN [48], WGAN-GP [15], zCR [53], auxiliary rotations [6], and spectral normalization [31].\\nWe also try modifying our baseline to use a shallower mapping network, which can be trained with\\nless data, borrowing intuition from DeLiGAN [16]. Finally, we try replacing our augmentations with\\nmultiplicative dropout [42], whose per-layer strength is driven by our adaptation algorithm. We spent\\nconsiderable effort tuning the parameters of all these methods, see Appendix D. We can see that ADA\\ngave signiﬁcantly better results than the alternatives. While PA-GAN is somewhat similar to our\\nmethod, its checksum task was not strong enough to prevent overﬁtting in our tests. Figure 8b shows\\nthat reducing the discriminator capacity is generally harmful and does not prevent overﬁtting.\\n4.2\\nTransfer learning\\nTransfer learning reduces the training data requirements by starting from a model trained using\\nsome other dataset, instead of a random initialization. Several authors have explored this in the\\ncontext of GANs [44, 45, 34], and Mo et al. [33] recently showed strong results by freezing the\\nhighest-resolution layers of the discriminator during transfer (Freeze-D).\\nWe explore several transfer learning setups in Figure 9, using the best Freeze-D conﬁguration found\\nfor each case with grid search. Transfer learning gives signiﬁcantly better results than from-scratch\\ntraining, and its success seems to depend primarily on the diversity of the source dataset, instead of\\nthe similarity between subjects. For example, FFHQ (human faces) can be trained equally well from\\n7\\nt = 0M\\n1M\\n2M\\n3M\\n4M\\n5M\\n5\\n10\\n20\\n50\\nFID\\nFFHQ-1k\\n+ Freeze-D\\nFFHQ-5k\\n+ Freeze-D\\nFFHQ-20k\\n+ Freeze-D\\nt = 0M\\n1M\\n2M\\n3M\\n4M\\n5M\\n5\\n10\\n20\\n50\\nFID\\nFFHQ-1k\\n+ Freeze-D\\nFFHQ-5k\\n+ Freeze-D\\nFFHQ-20k\\n+ Freeze-D\\n1k\\n2k\\n5k\\n10k\\n20k\\n2\\n5\\n10\\n20\\n50\\nFID\\nBaseline\\n+ Transfer\\n+ Freeze-D\\nADA (Ours)\\n+ Transfer\\n+ Freeze-D\\n1k\\n2k\\n5k\\n10k\\n20k\\n2\\n5\\n10\\n20\\nFID\\nLSUN Cat from CelebA-HQ\\nLSUN Cat from FFHQ\\nLSUN Cat from LSUN Dog\\nFFHQ from CelebA-HQ\\nFFHQ from LSUN Dog\\n(a) Without ADA\\n(b) With ADA\\n(c) Dataset sizes\\n(d) Datasets\\nFigure 9: Transfer learning FFHQ starting from a pre-trained CELEBA-HQ model, both 256 × 256.\\n(a) Training convergence for our baseline method and Freeze-D [33]. (b) The same conﬁgurations\\nwith ADA. (c) FIDs as a function of dataset size. (d) Effect of source and target datasets.\\nMETFACES (new dataset)\\nBRECAHAD\\nAFHQ CAT, DOG, WILD (5122)\\nCIFAR-10\\n1336 img, 10242, transfer learning from FFHQ\\n1944 img, 5122\\n5153 img\\n4739 img\\n4738 img\\n50k, 10 cls, 322\\nFigure 10: Example generated images for several datasets with limited amount of training data, trained\\nusing ADA. We use transfer learning with METFACES and train other datasets from scratch. See\\nAppendix A for uncurated results and real images, and Appendix D for our training conﬁgurations.\\nCELEBA-HQ (human faces, low diversity) or LSUN DOG (more diverse). LSUN CAT, however,\\ncan only be trained from LSUN DOG, which has comparable diversity, but not from the less diverse\\ndatasets. With small target dataset sizes, our baseline achieves reasonable FID quickly, but the\\nprogress soon reverts as training continues. ADA is again able to prevent the divergence almost\\ncompletely. Freeze-D provides a small but reliable improvement when used together with ADA but is\\nnot able to prevent the divergence on its own.\\n4.3\\nSmall datasets\\nWe tried our method with several datasets that consist of a limited number of training images\\n(Figure 10). METFACES is our new dataset of 1336 high-quality faces extracted from the collection of\\nMetropolitan Museum of Art (https://metmuseum.github.io/). BRECAHAD [1] consists of only\\n162 breast cancer histopathology images (1360 × 1024); we reorganized these into 1944 partially\\noverlapping crops of 5122. Animal faces (AFHQ) [7] includes ∼5k closeups per category for dogs,\\ncats, and wild life; we treated these as three separate datasets and trained a separate network for each\\nof them. CIFAR-10 includes 50k tiny images in 10 categories [25].\\nFigure 11 reveals that FID is not an ideal metric for small datasets, because it becomes dominated\\nby the inherent bias when the number of real images is insufﬁcient. We ﬁnd that kernel inception\\ndistance (KID) [3] — that is unbiased by design — is more descriptive in practice and see that ADA\\nprovides a dramatic improvement over baseline StyleGAN2. This is especially true when training\\nfrom scratch, but transfer learning also beneﬁts from ADA. In the widely used CIFAR-10 benchmark,\\nwe improve the SOTA FID from 5.59 to 2.42 and inception score (IS) [37] from 9.58 to 10.24 in the\\nclass-conditional setting (Figure 11b). This large improvement portrays CIFAR-10 as a limited data\\nbenchmark. We also note that CIFAR-speciﬁc architecture tuning had a signiﬁcant effect.\\n8\\nDataset\\nMethod\\nScratch\\nTransfer\\n+ Freeze-D\\nFID\\nKID\\nKID\\nKID\\n×103\\n×103\\n×103\\nMETFACES\\nBaseline\\n57.26\\n35.66\\n3.16\\n2.05\\nADA\\n18.22\\n2.41\\n0.81\\n1.33\\nBRECAHAD\\nBaseline\\n97.72\\n89.76\\n18.07\\n6.94\\nADA\\n15.71\\n2.88\\n3.36\\n1.91\\nAFHQ CAT\\nBaseline\\n5.13\\n1.54\\n1.09\\n1.00\\nADA\\n3.55\\n0.66\\n0.44\\n0.35\\nAFHQ DOG\\nBaseline\\n19.37\\n9.62\\n4.63\\n2.80\\nADA\\n7.40\\n1.16\\n1.40\\n1.12\\nAFHQ WILD\\nBaseline\\n3.48\\n0.77\\n0.31\\n0.12\\nADA\\n3.05\\n0.45\\n0.15\\n0.14\\nMethod\\nUnconditional\\nConditional\\nFID ↓\\nIS ↑\\nFID ↓\\nIS ↑\\nProGAN\\n[19]\\n15.52\\n8.56±0.06\\n–\\n–\\nAutoGAN [13]\\n12.42\\n8.55±0.10\\n–\\n–\\nBigGAN\\n[5]\\n–\\n–\\n14.73\\n9.22\\n+ Tuning\\n[22]\\n–\\n–\\n8.47\\n9.07±0.13\\nMultiHinge [22]\\n–\\n–\\n6.40\\n9.58±0.09\\nFQ-GAN\\n[52]\\n–\\n–\\n5.59±0.12\\n8.48\\nBaseline\\n8.32±0.09\\n9.21±0.09\\n6.96±0.41\\n9.53±0.06\\n+ ADA (Ours)\\n5.33±0.35 10.02±0.07\\n3.49±0.17 10.24±0.07\\n+ Tuning (Ours)\\n2.92±0.05\\n9.83±0.04\\n2.42±0.04 10.14±0.09\\n(a) Small datasets\\n(b) CIFAR-10\\nFigure 11: (a) Several small datasets trained with StyleGAN2 baseline (conﬁg F) and ADA, from\\nscratch and using transfer learning. We used FFHQ-140K with matching resolution as a starting\\npoint for all transfers. We report the best KID, and compute FID using the same snapshot. (c) Mean\\nand standard deviation for CIFAR-10, computed from the best scores of 5 training runs. For the\\ncomparison methods we report the average scores when available, and the single best score otherwise.\\nThe best IS and FID were searched separately [22], and often came from different snapshots. We\\ncomputed the FID for Progressive GAN [19] using the publicly available pre-trained network.\\n5\\nConclusions\\nWe have shown that our adaptive discriminator augmentation reliably stabilizes training and vastly\\nimproves the result quality when training data is in short supply. Of course, augmentation is not a\\nsubstitute for real data — one should always try to collect a large, high-quality set of training data\\nﬁrst, and only then ﬁll the gaps using augmentation. As future work, it would be worthwhile to search\\nfor the most effective set of augmentations, and to see if recently published techniques, such as the\\nU-net discriminator [38] or multi-modal generator [39], could also help with limited data.\\nEnabling ADA has a negligible effect on the energy consumption of training a single model. As such,\\nusing it does not increase the cost of training models for practical use or developing methods that\\nrequire large-scale exploration. For reference, Appendix E provides a breakdown of all computation\\nthat we performed related to this paper; the project consumed a total of 325 MWh of electricity, or\\n135 single-GPU years, the majority of which can be attributed to extensive comparisons and sweeps.\\nInterestingly, the core idea of discriminator augmentations was independently discovered by three\\nother research groups in parallel work: Z. Zhao et al. [54], Tran et al. [43], and S. Zhao et al. [51].\\nWe recommend these papers as they all offer a different set of intuition, experiments, and theoret-\\nical justiﬁcations. While two of these papers [54, 51] propose essentially the same augmentation\\nmechanism as we do, they study the absence of leak artifacts only empirically. The third paper\\n[43] presents a theoretical justiﬁcation based on invertibility, but arrives at a different argument\\nthat leads to a more complex network architecture, along with signiﬁcant restrictions on the set of\\npossible augmentations. None of these works consider the possibility of tuning augmentation strength\\nadaptively. Our experiments in Section 3 show that the optimal augmentation strength not only varies\\nbetween datasets of different content and size, but also over the course of training — even an optimal\\nset of ﬁxed augmentation parameters is likely to leave performance on the table.\\nA direct comparison of results between the parallel works is difﬁcult because the only dataset used\\nin all papers is CIFAR-10. Regrettably, the other three papers compute FID using 10k generated\\nimages and 10k validation images (FID-10k), while we use follow the original recommendation of\\nHeusel et al. [18] and use 50k generated images and all training images. Their FID-10k numbers are\\nthus not comparable to the FIDs in Figure 11b. For this reason we also computed FID-10k for our\\nmethod, obtaining 7.01 ± 0.06 for unconditional and 6.54 ± 0.06 for conditional. These compare\\nfavorably to parallel work’s unconditional 9.89 [51] or 10.89 [43], and conditional 8.30 [54] or 8.49\\n[51]. It seems likely that some combination of the ideas from all four papers could further improve\\nour results. For example, more diverse set of augmentations or contrastive regularization [54] might\\nbe worth testing.\\nAcknowledgements\\nWe thank David Luebke for helpful comments; Tero Kuosmanen and Sabu\\nNadarajan for their support with compute infrastructure; and Edgar Schönfeld for guidance on setting\\nup unconditional BigGAN.\\n9\\nBroader impact\\nData-driven generative modeling means learning a computational recipe for generating complicated\\ndata based purely on examples. This is a foundational problem in machine learning. In addition\\nto their fundamental nature, generative models have several uses within applied machine learning\\nresearch as priors, regularizers, and so on. In those roles, they advance the capabilities of computer\\nvision and graphics algorithms for analyzing and synthesizing realistic imagery.\\nThe methods presented in this work enable high-quality generative image models to be trained using\\nsigniﬁcantly less data than required by existing approaches. It thereby primarily contributes to the\\ndeep technical question of how much data is enough for generative models to succeed in picking up\\nthe necessary commonalities and relationships in the data.\\nFrom an applied point of view, this work contributes to efﬁciency; it does not introduce fundamental\\nnew capabilities. Therefore, it seems likely that the advances here will not substantially affect the\\noverall themes — surveillance, authenticity, privacy, etc. — in the active discussion on the broader\\nimpacts of computer vision and graphics.\\nSpeciﬁcally, generative models’ implications on image and video authenticity is a topic of active\\ndiscussion. Most attention revolves around conditional models that allow semantic control and\\nsometimes manipulation of existing images. Our algorithm does not offer direct controls for high-\\nlevel attributes (e.g., identity, pose, expression of people) in the generated images, nor does it enable\\ndirect modiﬁcation of existing images. However, over time and through the work of other researchers,\\nour advances will likely lead to improvements in these types of models as well.\\nThe contributions in this work make it easier to train high-quality generative models with custom sets\\nof images. By this, we eliminate, or at least signiﬁcantly lower, the barrier for applying GAN-type\\nmodels in many applied ﬁelds of research. We hope and believe that this will accelerate progress in\\nseveral such ﬁelds. For instance, modeling the space of possible appearance of biological specimens\\n(tissues, tumors, etc.) is a growing ﬁeld of research that appears to chronically suffer from limited\\nhigh-quality data. Overall, generative models hold promise for increased understanding of the\\ncomplex and hard-to-pinpoint relationships in many real-world phenomena; our work hopefully\\nincreases the breadth of phenomena that can be studied.\\nReferences\\n[1] A. Aksac, D. J. Demetrick, T. Ozyer, and R. Alhajj. BreCaHAD: A dataset for breast cancer histopatholog-\\nical annotation and diagnosis. BMC Research Notes, 12, 2019.\\n[2] M. Arjovsky and L. Bottou. Towards principled methods for training generative adversarial networks. In\\nProc. ICLR, 2017.\\n[3] M. Bi´nkowski, D. J. Sutherland, M. Arbel, and A. Gretton. Demystifying MMD GANs. In Proc. ICLR,\\n2018.\\n[4] A. Bora, E. Price, and A. Dimakis. AmbientGAN: Generative models from lossy measurements. In Proc.\\nICLR, 2018.\\n[5] A. Brock, J. Donahue, and K. Simonyan. Large scale GAN training for high ﬁdelity natural image synthesis.\\nIn Proc. ICLR, 2019.\\n[6] T. Chen, X. Zhai, M. Ritter, M. Lucic, and N. Houlsby. Self-supervised GANs via auxiliary rotation loss.\\nIn Proc. CVPR, 2019.\\n[7] Y. Choi, Y. Uh, J. Yoo, and J.-W. Ha. StarGAN v2: Diverse image synthesis for multiple domains. In Proc.\\nCVPR, 2020.\\n[8] E. D. Cubuk, B. Zoph, D. Mané, V. Vasudevan, and Q. V. Le. AutoAugment: Learning augmentation\\npolicies from data. In Proc. CVPR, 2019.\\n[9] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le. RandAugment: Practical automated data augmentation\\nwith a reduced search space. CoRR, abs/1909.13719, 2019.\\n[10] I. Daubechies. Ten lectures on wavelets, volume 61. Siam, 1992.\\n[11] T. De Vries and G. Taylor. Improved regularization of convolutional neural networks with cutout. CoRR,\\nabs/1708.04552, 2017.\\n[12] R. Ge, X. Feng, H. Pyla, K. Cameron, and W. Feng. Power measurement tutorial for the Green500 list.\\nhttps://www.top500.org/green500/resources/tutorials/, Accessed March 1, 2020.\\n[13] X. Gong, S. Chang, Y. Jiang, and Z. Wang. AutoGAN: Neural architecture search for generative adversarial\\nnetworks. In Proc. ICCV, 2019.\\n10\\n[14] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio.\\nGenerative adversarial networks. In Proc. NIPS, 2014.\\n[15] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville. Improved training of Wasserstein\\nGANs. In Proc. NIPS, pages 5769–5779, 2017.\\n[16] S. Gurumurthy, R. K. Sarvadevabhatla, and V. B. Radhakrishnan. DeLiGAN: Generative adversarial\\nnetworks for diverse and limited data. In Proc. CVPR, 2017.\\n[17] T. He, Z. Zhang, H. Zhang, Z. Zhang, J. Xie, and M. Li. Bag of tricks for image classiﬁcation with\\nconvolutional neural networks. In Proc. CVPR, 2019.\\n[18] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. GANs trained by a two time-scale\\nupdate rule converge to a local Nash equilibrium. In Proc. NIPS, 2017.\\n[19] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive growing of GANs for improved quality, stability,\\nand variation. In Proc. ICLR, 2018.\\n[20] T. Karras, S. Laine, and T. Aila. A style-based generator architecture for generative adversarial networks.\\nIn Proc. CVPR, 2018.\\n[21] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila. Analyzing and improving the image\\nquality of StyleGAN. In Proc. CVPR, 2020.\\n[22] I. Kavalerov, W. Czaja, and R. Chellappa. cGANs with multi-hinge loss. CoRR, abs/1912.04216, 2019.\\n[23] M. Kettunen, E. Härkönen, and J. Lehtinen. E-LPIPS: robust perceptual image similarity via random\\ntransformation ensembles. CoRR, abs/1906.03973, 2019.\\n[24] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Proc. ICLR, 2015.\\n[25] A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of\\nToronto, 2009.\\n[26] T. Kynkäänniemi, T. Karras, S. Laine, J. Lehtinen, and T. Aila. Improved precision and recall metric for\\nassessing generative models. In Proc. NeurIPS, 2019.\\n[27] S. Laine and T. Aila. Temporal ensembling for semi-supervised learning. In Proc. ICLR, 2017.\\n[28] X. Mao, Q. Li, H. Xie, R. Y. K. Lau, and Z. Wang. Least squares generative adversarial networks. In Proc.\\nICCV, 2017.\\n[29] M. Marchesi. Megapixel size image creation using generative adversarial networks. CoRR, abs/1706.00082,\\n2017.\\n[30] L. Mescheder, A. Geiger, and S. Nowozin. Which training methods for GANs do actually converge? In\\nProc. ICML, 2018.\\n[31] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral normalization for generative adversarial\\nnetworks. In Proc. ICLR, 2018.\\n[32] T. Miyato and M. Koyama. cGANs with projection discriminator. In Proc. ICLR, 2018.\\n[33] S. Mo, M. Cho, and J. Shin. Freeze the discriminator: a simple baseline for ﬁne-tuning GANs. CoRR,\\nabs/2002.10964, 2020.\\n[34] A. Noguchi and T. Harada. Image generation from small datasets via batch statistics adaptation. In Proc.\\nICCV, 2019.\\n[35] M. Sajjadi, M. Javanmardi, and T. Tasdizen. Regularization with stochastic transformations and perturba-\\ntions for deep semi-supervised learning. In Proc. NIPS, 2016.\\n[36] M. S. M. Sajjadi, O. Bachem, M. Lucic, O. Bousquet, and S. Gelly. Assessing generative models via\\nprecision and recall. In Proc. NIPS, 2018.\\n[37] T. Salimans, I. J. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques for\\ntraining GANs. In Proc. NIPS, 2016.\\n[38] E. Schönfeld, B. Schiele, and A. Khoreva. A U-net based discriminator for generative adversarial networks.\\nCoRR, abs/2002.12655, 2020.\\n[39] O. Sendik, D. Lischinski, and D. Cohen-Or. Unsupervised multi-modal styled content generation. CoRR,\\nabs/2001.03640, 2020.\\n[40] C. Shorten and T. M. Khoshgoftaar. A survey on image data augmentation for deep learning. Journal of\\nBig Data, 6, 2019.\\n[41] C. Sønderby, J. Caballero, L. Theis, W. Shi, and F. Huszár. Amortised MAP inference for image super-\\nresolution. In Proc. ICLR, 2017.\\n[42] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to\\nprevent neural networks from overﬁtting. Journal of Machine Learning Research, 15:1929–1958, 2014.\\n[43] N.-T. Tran, V.-H. Tran, N.-B. Nguyen, T.-K. Nguyen, and N.-M. Cheung. On data augmentation for GAN\\ntraining. CoRR, abs/2006.05338, 2020.\\n[44] Y. Wang, A. Gonzalez-Garcia, D. Berga, L. Herranz, F. S. Khan, and J. van de Weijer. MineGAN: Effective\\nknowledge transfer from GANs to target domains with few images. In Proc. CVPR, 2020.\\n[45] Y. Wang, C. Wu, L. Herranz, J. van de Weijer, A. Gonzalez-Garcia, and B. Raducanu. Transferring GANs:\\nGenerating images from limited data. In Proc. ECCV, 2018.\\n[46] J. Wishart and M. S. Bartlett. The distribution of second order moment statistics in a normal system.\\nMathematical Proceedings of the Cambridge Philosophical Society, 28(4):455–459, 1932.\\n11\\n[47] X. Yi, E. Walia, and P. S. Babyn. Generative adversarial network in medical imaging: A review. Medical\\nImage Analysis, 58, 2019.\\n[48] D. Zhang and A. Khoreva. PA-GAN: Improving GAN training by progressive augmentation. In Proc.\\nNeurIPS, 2019.\\n[49] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena. Self-attention generative adversarial networks. In\\nProc. ICML, 2019.\\n[50] H. Zhang, Z. Zhang, A. Odena, and H. Lee. Consistency regularization for generative adversarial networks.\\nIn Proc. ICLR, 2019.\\n[51] S. Zhao, Z. Liu, J. Lin, J.-Y. Zhu, and S. Han. Differentiable augmentation for data-efﬁcient GAN training.\\nCoRR, abs/2006.10738, 2020.\\n[52] Y. Zhao, C. Li, P. Yu, J. Gao, and C. Chen. Feature quantization improves GAN training. CoRR,\\nabs/2004.02088, 2020.\\n[53] Z. Zhao, S. Singh, H. Lee, Z. Zhang, A. Odena, and H. Zhang. Improved consistency regularization for\\nGANs. CoRR, abs/2002.04724, 2020.\\n[54] Z. Zhao, Z. Zhang, T. Chen, S. Singh, and H. Zhang. Image augmentations for GAN training. CoRR,\\nabs/2006.02595, 2020.\\nA\\nAdditional results\\nIn Figures 12, 13, 14, 15, and 16, we show generated images for METFACES, BRECAHAD, and\\nAFHQ CAT, DOG, WILD, respectively, along with real images from the respective training sets\\n(Section 4.3 and Figure 11a). The images were selected at random; we did not perform any cherry-\\npicking besides choosing one global random seed. We can see that ADA yields excellent results in all\\ncases, and with slight truncation [29, 20], virtually all of the images look convincing. Without ADA,\\nthe convergence is hampered by discriminator overﬁtting, leading to inferior image quality for the\\noriginal StyleGAN2, especially in METFACES, AFHQ DOG, and BRECAHAD.\\nFigure 17 shows examples of the generated CIFAR-10 images in both unconditional and class-\\nconditional setting (See Appendix D.1 for details on the conditional setup).\\nFigure 18 shows\\nqualitative results for different methods using subsets of FFHQ at 256×256 resolution. Methods\\nthat do not employ augmentation (BigGAN, StyleGAN2, and our baseline) degrade noticeably as\\nthe size of the training set decreases, generally yielding poor image quality and diversity with fewer\\nthan 30k training images. With ADA, the degradation is much more graceful, and the results remain\\nreasonable even with a 5k training set.\\nFigure 19 compares our results with unconditional BigGAN [5, 38] and StyleGAN2 conﬁg F [21].\\nBigGAN was very unstable in our experiments: while some of the results were quite good, ap-\\nproximately 50% of the training runs failed to converge. StyleGAN2, on the other hand, behaved\\npredictably, with different training runs resulting in nearly identical FID. We note that FID has a\\ngeneral tendency to increase as the training set gets smaller — not only because of the lower image\\nquality, but also due to inherent bias in FID itself [3]. In our experiments, we minimize the impact\\nof this bias by always computing FID between 50k generated images and all available real images,\\nregardless of which subset was used for training. To estimate the magnitude of bias in FID, we\\nsimulate a hypothetical generator that replicates the training set as-is, and compute the average FID\\nover 100 random trials with different subsets of training data; the standard deviation was ≤2% in all\\ncases. We can see that the bias remains negligible with ≥20k training images but starts to dominate\\nwith ≤2k. Interestingly, ADA reaches the same FID as the best-case generator with FFHQ-1k,\\nindicating that FID is no longer able to differentiate between the two in this case.\\nFigure 20 shows additional examples of bCR leaking to generated images and compares bCR with\\ndataset augmentation. In particular, rotations in range [−45◦, +45◦] (denoted ±45◦) serve as a very\\nclear example that attempting to make the discriminator blind to certain transformations opens up the\\npossibility for the generator to produce similarly transformed images with no penalty. In applications\\nwhere such leaks are acceptable, one can employ either bCR or dataset augmentation — we ﬁnd that\\nit is difﬁcult to predict which method is better. For example, with translation augmentations bCR\\nwas signiﬁcantly better than dataset augmentation, whereas x-ﬂip was much more effective when\\nimplemented as a dataset augmentation.\\nFinally, Figure 21 shows an extended version of Figure 4, illustrating the effect of different augmenta-\\ntion categories with increasing augmentation probability p. Blit + Geom + Color yielded the best\\nresults with a 2k training set and remained competitive with larger training sets as well.\\n12\\nADA (Ours), truncated (ψ = 0.7)\\nReal images from the training set\\nADA (Ours), untruncated\\nOriginal StyleGAN2 conﬁg F, untruncated\\nFID 15.34 – KID 0.81×103 – Recall 0.261\\nFID 19.47 – KID 3.16×103 – Recall 0.350\\nFigure 12: Uncurated 1024×1024 results generated for METFACES (1336 images) with and without\\nADA, along with real images from the training set. Both generators were trained using transfer\\nlearning, starting from the pre-trained StyleGAN2 for FFHQ. We recommend zooming in.\\n13\\nADA (Ours), truncated (ψ = 0.7)\\nReal images from the training set\\nADA (Ours), untruncated\\nOriginal StyleGAN2 conﬁg F, untruncated\\nFID 15.71 – KID 2.88×103 – Recall 0.340\\nFID 97.72 – KID 89.76×103 – Recall 0.027\\nFigure 13: Uncurated 512×512 results generated for BRECAHAD [1] (1944 images) with and\\nwithout ADA, along with real images from the training set. Both generators were trained from scratch.\\nWe recommend zooming in to inspect the image quality in detail.\\n14\\nADA (Ours), truncated (ψ = 0.7)\\nReal images from the training set\\nADA (Ours), untruncated\\nOriginal StyleGAN2 conﬁg F, untruncated\\nFID 3.55 – KID 0.66×103 – Recall 0.430\\nFID 5.13 – KID 1.54×103 – Recall 0.215\\nFigure 14: Uncurated 512×512 results generated for AFHQ CAT [7] (5153 images) with and without\\nADA, along with real images from the training set. Both generators were trained from scratch. We\\nrecommend zooming in to inspect the image quality in detail.\\n15\\nADA (Ours), truncated (ψ = 0.7)\\nReal images from the training set\\nADA (Ours), untruncated\\nOriginal StyleGAN2 conﬁg F, untruncated\\nFID 7.40 – KID 1.16×103 – Recall 0.454\\nFID 19.37 – KID 9.62×103 – Recall 0.196\\nFigure 15: Uncurated 512×512 results generated for AFHQ DOG [7] (4739 images) with and without\\nADA, along with real images from the training set. Both generators were trained from scratch. We\\nrecommend zooming in to inspect the image quality in detail.\\n16\\nADA (Ours), truncated (ψ = 0.7)\\nReal images from the training set\\nADA (Ours), untruncated\\nOriginal StyleGAN2 conﬁg F, untruncated\\nFID 3.05 – KID 0.45×103 – Recall 0.147\\nFID 3.48 – KID 0.77×103 – Recall 0.143\\nFigure 16: Uncurated 512×512 results generated for AFHQ WILD [7] (4738 images) with and\\nwithout ADA, along with real images from the training set. Both generators were trained from scratch.\\nWe recommend zooming in to inspect the image quality in detail.\\n17\\nGenerator with best FID\\nReal images\\nGenerator with best IS\\nUnconditional\\nFID 2.85 – IS 9.74\\nIS 11.24\\nFID 5.70 – IS 10.08\\nPlane\\nCar\\nBird\\nCat\\nDeer\\nDog\\nFrog\\nHorse\\nShip\\nTruck\\nFID 2.38 – IS 10.00\\nIS 11.24\\nFID 3.62 – IS 10.33\\nFigure 17: Generated and real images for CIFAR-10 in the unconditional setting (top) and each class\\nin the conditional setting (bottom). We show the results for the best generators trained in the context\\nof Figure 11b, selected according to either FID or IS. The numbers refer to the single best model and\\nare therefore slightly better than the averages quoted in the result table. It can be seen that the model\\nwith the lowest FID produces images with a wider variation in coloring and poses compared to the\\nmodel with highest IS. This is in line with the common approximation (e.g., [5]) that FID roughly\\ncorresponds to Recall and IS to Precision, two independent aspects of result quality [36, 26].\\n18\\n2k training set\\n5k training set\\n30k training set\\n140k training set\\nBigGAN\\nFID 60.47\\nFID 32.34\\nFID 15.84\\nFID 11.08\\nStyleGAN2\\nFID 66.77 – Recall 0.002\\nFID 39.42 – Recall 0.030\\nFID 8.80 – Recall 0.283\\nFID 3.81 – Recall 0.452\\nOur baseline\\nFID 76.61 – Recall 0.000\\nFID 43.72 – Recall 0.010\\nFID 11.40 – Recall 0.258\\nFID 3.54 – Recall 0.452\\nADA (Ours)\\nFID 15.76 – Recall 0.135\\nFID 10.78 – Recall 0.185\\nFID 5.40 – Recall 0.354\\nFID 3.79 – Recall 0.440\\nADA + bCR\\nFID 17.05 – Recall 0.076\\nFID 10.21 – Recall 0.155\\nFID 4.55 – Recall 0.327\\nFID 3.55 – Recall 0.412\\nFigure 18: Images generated for different subsets of FFHQ at 256×256 resolution using the training\\nsetups from Figures 7 and 19. We show the best snapshot of the best training run for each case,\\nselected according to FID, so the numbers are slightly better than the medians reported in Figure 7c. In\\naddition to FID, we also report the Recall metric [26] as a more direct way to estimate image diversity.\\nThe bolded numbers indicate the lowest FID and highest Recall for each training set size. “BigGAN”\\ncorresponds to the unconditional variant of BigGAN [5] proposed by Schönfeld et al. [38], and\\n“StyleGAN2” corresponds to conﬁg F of the ofﬁcial TensorFlow implementation by Karras et al. [21].\\n19\\n1k\\n2k\\n5k\\n10k\\n20k\\n30k\\n50k 70k\\n140k\\n2\\n5\\n10\\n20\\n50\\n100\\n200\\nFID\\nBigGAN\\n(5 runs)\\nStyleGAN2 (3 runs)\\nOur baseline (3 runs)\\n+ ADA\\n(3 runs)\\nApproximate\\nbias in FID\\n1k\\n2k\\n5k\\n10k\\n20k\\n30k\\n50k\\n100k\\n200k\\n2\\n5\\n10\\n20\\n50\\n100\\n200\\nFID\\nBigGAN\\n(5 runs)\\nStyleGAN2 (3 runs)\\nOur baseline (3 runs)\\n+ ADA\\n(3 runs)\\nApproximate\\nbias in FID\\n(a) Different subsets of FFHQ at 256×256\\n(b) Different subsets of LSUN CAT at 256×256\\nFigure 19: Comparison of our results with unconditional BigGAN [5, 38] and StyleGAN2 con-\\nﬁg F [21]. We report the median/min/max FID as a function of training set size, calculated over\\nmultiple independent training runs. The dashed red line illustrates the expected bias of the FID metric,\\ncomputed using a hypothetical generator that outputs random images from the training set as-is.\\nInteger translation\\n±0px\\n±4px\\n±8px\\n±16px\\n±16px samples\\nArbitrary rotation\\n±0◦\\n±10◦\\n±20◦\\n±45◦\\n±45◦samples\\n1k\\n2k\\n5k\\n10k\\n30k\\n70k\\n2\\n5\\n10\\n20\\n50\\nFID\\nBaseline\\nData trans.\\nData x-flip\\nbCR trans.\\nbCR trans. + bCR x-flip\\nbCR trans. + data x-flip\\n1k\\n2k\\n5k\\n10k\\n30k\\n70k\\n2\\n5\\n10\\n20\\n50\\nFID\\nBaseline\\n+ data x-flip\\nADA\\n+ data x-flip\\n(a) Mean images for bCR with FFHQ-5k\\n(b) bCR vs. dataset augment\\n(c) Effect of dataset x-ﬂips\\nFigure 20: (a) Examples of bCR leaking to generated images. (b) Comparison between dataset\\naugmentation and bCR using ±8px translations and x-ﬂips. (c) In general, dataset x-ﬂips can provide\\na signiﬁcant boost to FID in cases where they are appropriate. For baseline, the effect is almost equal\\nto doubling the size of training set, as evidenced by the consistent 2× horizontal offset between the\\nblue curves. With ADA the effect is somewhat weaker.\\n2k training set\\n10k training set\\n50k training set\\n140k training set\\nIndividual\\np = 0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n5\\n10\\n20\\n50\\n100\\nFID\\nBlit\\nGeom\\nColor\\nFilter\\nNoise\\nCutout\\np = 0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n5\\n10\\n20\\n50\\n100\\nFID\\nBlit\\nGeom\\nColor\\nFilter\\nNoise\\nCutout\\np = 0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n5\\n10\\n20\\n50\\n100\\nFID\\nBlit\\nGeom\\nColor\\nFilter\\nNoise\\nCutout\\np = 0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n5\\n10\\n20\\n50\\n100\\nFID\\nBlit\\nGeom\\nColor\\nFilter\\nNoise\\nCutout\\nCumulative\\np = 0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n5\\n10\\n20\\n50\\n100\\nFID\\nBlit\\n+ Geom\\n+ Color\\n+ Filter\\n+ Noise\\n+ Cutout\\np = 0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n5\\n10\\n20\\n50\\n100\\nFID\\nBlit\\n+ Geom\\n+ Color\\n+ Filter\\n+ Noise\\n+ Cutout\\np = 0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n5\\n10\\n20\\n50\\n100\\nFID\\nBlit\\n+ Geom\\n+ Color\\n+ Filter\\n+ Noise\\n+ Cutout\\np = 0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n5\\n10\\n20\\n50\\n100\\nFID\\nBlit\\n+ Geom\\n+ Color\\n+ Filter\\n+ Noise\\n+ Cutout\\nFigure 21: Extended version of Figure 4, illustrating the individual and cumulative effect of different\\naugmentation categories with increasing augmentation probability p.\\n20\\nB\\nOur augmentation pipeline\\nWe designed our augmentation pipeline based on three goals. First, the entire pipeline must be strictly\\nnon-leaking (Appendix C). Second, we aim for a maximally diverse set of augmentations, inspired by\\nthe success of RandAugment [9]. Third, we strive for the highest possible image quality to reduce\\nunintended artifacts such as aliasing. In total, our pipeline consists of 18 transformations: geometric\\n(7), color (5), ﬁltering (4), and corruption (2). We implement it entirely on the GPU in a differentiable\\nfashion, with full support for batching. All parameters are sampled independently for each image.\\nB.1\\nGeometric and color transformations\\nFigure 22 shows pseudocode for our geometric and color transformations, along with example images.\\nIn general, geometric transformations tend to lose high-frequency details of the input image due to\\nuneven resampling, which may reduce the capability of the discriminator to detect pixel-level errors\\nin the generated images. We alleviate this by introducing a dedicated sub-category, pixel blitting,\\nthat only copies existing pixels as-is, without blending between neighboring pixels. Furthermore,\\nwe avoid gradual image degradation from multiple consecutive transformations by collapsing all\\ngeometric transformations into a single combined operation.\\nThe parameters for pixel blitting are selected on lines 5–15, consisting of x-ﬂips (line 7), 90◦rotations\\n(line 10), and integer translations (line 13). The transformations are accumulated into a homogeneous\\n3 × 3 matrix G, deﬁned so that input pixel (xi, yi) is placed at [xo, yo, 1]T = G · [xi, yi, 1]T in the\\noutput. The origin is located at the center of the image and neighboring pixels are spaced at unit\\nintervals. We apply each transformation with probability p by sampling its parameters from uniform\\ndistribution, either discrete U{·} or continuous U(·), and updating G using elementary transforms:\\nSCALE2D(sx, sy) =\\n\"sx\\n0\\n0\\n0\\nsy\\n0\\n0\\n0\\n1\\n#\\n, ROTATE2D(θ) =\\n\"cos θ\\n−sin θ\\n0\\nsin θ\\ncos θ\\n0\\n0\\n0\\n1\\n#\\n, TRANSLATE2D(tx, ty) =\\n\"1\\n0\\ntx\\n0\\n1\\nty\\n0\\n0\\n1\\n#\\n(2)\\nGeneral geometric transformations are handled in a similar way on lines 16–32, consisting of isotropic\\nscaling (line 17), arbitrary rotation (lines 21 and 27), anisotropic scaling (line 24), and fractional\\ntranslation (line 30). Since both of the scaling transformations are multiplicative in nature, we sample\\ntheir parameter, s, from a log-normal distribution so that ln s ∼N\\n\\x000, (0.2 · ln 2)2\\x01\\n. In practice, this\\ncan be done by ﬁrst sampling t ∼N(0, 1) and then calculating s = exp2(0.2t). We allow anisotropic\\nscaling to operate in other directions besides the coordinate axes by breaking the rotation into two\\nindependent parts, one applied before the scaling (line 21) and one after it (line 27). We apply the\\nrotations slightly less frequently than other transformations, so that the probability of applying at\\nleast one rotation is equal to p. Note that we also have two translations in our pipeline (lines 13 and\\n30), one applied at the beginning and one at the end. To increase the diversity of our augmentations,\\nwe use U(·) for the former and N(·) for the latter.\\nOnce the parameters are settled, the combined geometric transformation is executed on lines 33–47.\\nWe avoid undesirable effects at image borders by ﬁrst padding the image with reﬂection. The amount\\nof padding is calculated dynamically based on G so that none of the output pixels are affected by\\nregions outside the image (line 35). We then upsample the image to a higher resolution (line 40) and\\ntransform it using bilinear interpolation (line 45). Operating at a higher resolution is necessary to\\nreduce aliasing when the image is miniﬁed, e.g., as a result of isotropic scaling — interpolating at\\nthe original resolution would fail to correctly ﬁlter out frequencies above Nyquist in this case, no\\nmatter which interpolation ﬁlter was used. The choice of the upsampling ﬁlter requires some care,\\nhowever, because we must ensure that an identity transform does not modify the image in any way\\n(e.g., when p = 0). In other words, we need to use a lowpass ﬁlter H(z) with cutoff fc = π\\n2 that\\nsatisﬁes DOWNSAMPLE2D\\n\\x00UPSAMPLE2D\\n\\x00Y, H(z−1)\\n\\x01\\n, H(z)\\n\\x01\\n= Y . Luckily, existing literature\\non wavelets [10] offers a wide selection of such ﬁlters; we choose 12-tap symlets (SYM6) to strike a\\nbalance between resampling quality and computational cost.\\nFinally, color transformations are applied to the resulting image on lines 48–70. The overall oper-\\nation is similar to geometric transformations: we collect the parameters of each individual trans-\\nformation into a homogeneous 4 × 4 matrix C that we then apply to each pixel by computing\\n[ro, go, bo, 1]T = C · [ri, gi, bi, 1]T . The transformations include adjusting brightness (line 50), con-\\ntrast (line 53), and saturation (line 63), as well as ﬂipping the luma axis while keeping the chroma\\nunchanged (line 57) and rotating the hue axis by an arbitrary amount (line 60).\\n21\\n1:\\ninput: original image X, augmentation probability p\\n2:\\noutput: augmented image Y\\n3:\\n(w, h) ←SIZE(X)\\n4:\\nY ←CONVERT(X, FLOAT) ▷Yx,y ∈[−1, +1]3\\n5:\\n▷Select parameters for pixel blitting\\n6:\\nG ←I3\\n▷Homogeneous 2D transformation matrix\\n7:\\napply x-ﬂip with probability p\\n8:\\nsample i ∼U{0, 1}\\n9:\\nG ←SCALE2D(1 −2i, 1) · G\\n10:\\napply 90◦rotations with probability p\\n11:\\nsample i ∼U{0, 3}\\n12:\\nG ←ROTATE2D\\n\\x00−π\\n2 · i\\n\\x01\\n· G\\n13:\\napply integer translation with probability p\\n14:\\nsample tx, ty ∼U(−0.125, +0.125)\\n15:\\nG ←TRANSLATE2D\\n\\x00round(txw), round(tyh)\\n\\x01\\n· G\\n16:\\n▷Select parameters for general geometric transformations\\n17:\\napply isotropic scaling with probability p\\n18:\\nsample s ∼Lognormal\\n\\x000, (0.2 · ln 2)2\\x01\\n19:\\nG ←SCALE2D(s, s) · G\\n20:\\nprot ←1 −√1 −p\\n▷P (pre ∪post) = p\\n21:\\napply pre-rotation with probability prot\\n22:\\nsample θ ∼U(−π, +π)\\n23:\\nG ←ROTATE2D(−θ) · G ▷Before anisotropic scaling\\n24:\\napply anisotropic scaling with probability p\\n25:\\nsample s ∼Lognormal\\n\\x000, (0.2 · ln 2)2\\x01\\n26:\\nG ←SCALE2D\\n\\x00s, 1\\ns\\n\\x01\\n· G\\n27:\\napply post-rotation with probability prot\\n28:\\nsample θ ∼U(−π, +π)\\n29:\\nG ←ROTATE2D(−θ) · G ▷After anisotropic scaling\\n30:\\napply fractional translation with probability p\\n31:\\nsample tx, ty ∼N\\n\\x000, (0.125)2\\x01\\n32:\\nG ←TRANSLATE2D(txw, tyh) · G\\n33:\\n▷Pad image and adjust origin\\n34:\\nH(z) ←WAVELET(SYM6) ▷Orthogonal lowpass ﬁlter\\n35:\\n(mlo, mhi) ←CALCULATEPADDING\\n\\x00G, w, h, H(z)\\n\\x01\\n36:\\nY ←PAD(Y, mlo, mhi, REFLECT)\\n37:\\nT ←TRANSLATE2D\\n\\x00 1\\n2 w−1\\n2 +mlo,x, 1\\n2 h−1\\n2 +mlo,y\\n\\x01\\n38:\\nG ←T · G · T −1 ▷Place origin at image center\\n39:\\n▷Execute geometric transformations\\n40:\\nY ′ ←UPSAMPLE2X2\\n\\x00Y, H(z−1)\\n\\x01\\n41:\\nS ←SCALE2D(2, 2)\\n42:\\nG ←S · G · S−1\\n▷Account for the upsampling\\n43:\\nfor each pixel (xo, yo) ∈Y ′ do\\n44:\\n[xi, yi, zi]T ←G−1 · [xo, yo, 1]T\\n45:\\nYxo,yo ←BILINEARLOOKUP(Y ′, xi, yi)\\n46:\\nY ←DOWNSAMPLE2X2\\n\\x00Y, H(z)\\n\\x01\\n47:\\nY ←CROP(Y, mlo, mhi)\\n▷Undo the padding\\n48:\\n▷Select parameters for color transformations\\n49:\\nC ←I4\\n▷Homogeneous 3D transformation matrix\\n50:\\napply brightness with probability p\\n51:\\nsample b ∼N\\n\\x000, (0.2)2\\x01\\n52:\\nC ←TRANSLATE3D(b, b, b) · C\\n53:\\napply contrast with probability p\\n54:\\nsample c ∼Lognormal\\n\\x000, (0.5 · ln 2)2\\x01\\n55:\\nC ←SCALE3D(c, c, c) · C\\n56:\\nv ←[1, 1, 1, 0] / √\\n3\\n▷Luma axis\\n57:\\napply luma ﬂip with probability p\\n58:\\nsample i ∼U{0, 1}\\n59:\\nC ←\\n\\x00I4 −2vT v · i\\n\\x01\\n· C ▷Householder reﬂection\\n60:\\napply hue rotation with probability p\\n61:\\nsample θ ∼U(−π, +π)\\n62:\\nC ←ROTATE3D(v, θ) · C\\n▷Rotate around v\\n63:\\napply saturation with probability p\\n64:\\nsample s ∼Lognormal\\n\\x000, (1 · ln 2)2\\x01\\n65:\\nC ←\\n\\x10\\nvT v +\\n\\x00I4 −vT v\\n\\x01\\n· s\\n\\x11\\n· C\\n66:\\n▷Execute color transformations\\n67:\\nfor each pixel (x, y) ∈Y do\\n68:\\n(ri, gi, bi) ←Yx,y\\n69:\\n[ro, go, bo, ao]T ←C · [ri, gi, bi, 1]T\\n70:\\nYx,y ←(ro, go, bo)\\n71:\\nreturn Y\\nPercentile:\\n5th\\n35th\\n65th\\n95th\\nPixel blitting\\nx-ﬂip\\n90◦\\nrotations\\nInteger\\ntranslation\\nGeneral geometric transformations\\nIsotropic\\nscaling\\nArbitrary\\nrotation\\nAnisotropic\\nscaling\\nFractional\\ntranslation\\nColor transformations\\nBrightness\\nContrast\\nLuma\\nﬂip\\nHue\\nrotation\\nSaturation\\nFigure 22: Pseudocode and example images for geometric and color transformations (Appendix B.1).\\nWe illustrate the effect of each individual transformation (apply) using four sets of parameter values,\\nrepresenting the 5th, 35th, 65th, and 95th percentiles of their corresponding distributions (sample).\\n22\\n1:\\ninput: original image X, augmentation probability p\\n2:\\noutput: augmented image Y\\n3:\\n(w, h) ←SIZE(X)\\n4:\\nY ←CONVERT(X, FLOAT) ▷Yx,y ∈[−1, +1]3\\n5:\\n▷Select parameters for image-space ﬁltering\\n6:\\nb ←\\nh\\x02\\n0, π\\n8\\n\\x03\\n,\\n\\x02 π\\n8 , π\\n4\\n\\x03\\n,\\n\\x02 π\\n4 , π\\n2\\n\\x03\\n,\\n\\x02 π\\n2 , π\\n\\x03i\\n▷Freq. bands\\n7:\\ng ←[1, 1, 1, 1]\\n▷Global gain vector (identity)\\n8:\\nλ ←[10, 1, 1, 1] / 13 ▷Expected power spectrum (1/f)\\n9:\\nfor i = 1, 2, 3, 4 do\\n10:\\napply ampliﬁcation for bi with probability p\\n11:\\nt ←[1, 1, 1, 1]\\n▷Temporary gain vector\\n12:\\nsample ti ∼Lognormal\\n\\x000, (1 · ln 2)2\\x01\\n13:\\nt ←t\\n\\x0eqP\\nj λjt2\\nj\\n▷Normalize power\\n14:\\ng ←g ⊙t\\n▷Accumulate into global gain\\n15:\\n▷Execute image-space ﬁltering\\n16:\\nH(z) ←WAVELET(SYM2) ▷Orthogonal 4-tap ﬁlter bank\\n17:\\nH′(z) ←0\\n▷Combined ampliﬁcation ﬁlter\\n18:\\nfor i = 1, 2, 3, 4 do\\n19:\\nH′(z) ←H′(z) + BANDPASS\\n\\x00H(z), bi\\n\\x01\\n· gi\\n20:\\n(mlo, mhi) ←CALCULATEPADDING\\n\\x00H′(z)\\n\\x01\\n21:\\nY ←PAD(Y, mlo, mhi, REFLECT)\\n22:\\nY ←SEPARABLECONV2D\\n\\x00Y, H′(z)\\n\\x01\\n23:\\nY ←CROP(Y, mlo, mhi)\\n24:\\n▷Additive RGB noise\\n25:\\napply noise with probability p\\n26:\\nsample σ ∼Halfnormal\\n\\x00(0.1)2\\x01\\n27:\\nfor each pixel (x, y) ∈Y do\\n28:\\nsample nr, ng, nb ∼N(0, σ2)\\n29:\\nYx,y ←Yx,y + [nr, ng, nb]\\n30:\\n▷Cutout\\n31:\\napply cutout with probability p\\n32:\\nsample cx, cy ∼U(0, 1)\\n33:\\nrlo ←round\\n\\x10h\\x00cx −1\\n4\\n\\x01\\n· w,\\n\\x00cy −1\\n4\\n\\x01\\n· h\\ni\\x11\\n34:\\nrhi ←round\\n\\x10h\\x00cx + 1\\n4\\n\\x01\\n· w,\\n\\x00cy + 1\\n4\\n\\x01\\n· h\\ni\\x11\\n35:\\nY ←Y ⊙\\n\\x001 −RECTANGULARMASK(rlo, rhi)\\n\\x01\\n36:\\nreturn Y\\nPercentile:\\n5th\\n35th\\n65th\\n95th\\nImage-space ﬁltering\\nFrequency\\nband b1\\n\\x02\\n0, π\\n8\\n\\x03\\nFrequency\\nband b2\\n\\x02 π\\n8 , π\\n4\\n\\x03\\nFrequency\\nband b3\\n\\x02 π\\n4 , π\\n2\\n\\x03\\nFrequency\\nband b4\\n\\x02 π\\n2 , π\\n\\x03\\nImage-space corruptions\\nAdditive\\nRGB noise\\nCutout\\nFigure 23: Pseudocode and example images for image-space ﬁltering and corruptions (Appendix B.2).\\nx ⊙y denotes element-wise multiplication.\\nB.2\\nImage-space ﬁltering and corruptions\\nFigure 23 shows pseudocode for our image-space ﬁltering and corruptions. The parameters for image-\\nspace ﬁltering are selected on lines 5–14. The idea is to divide the frequency content of the image into\\n4 non-overlapping bands and amplify/weaken each band in turn via a sequence of 4 transformations,\\nso that each transformation is applied independently with probability p (lines 9–10). Frequency\\nbands b2, b3, and b4 correspond to the three highest octaves, respectively, while the remaining low\\nfrequencies are attributed to b1 (line 6). We track the overall gain of each band using vector g (line 7)\\nthat we update after each transformation (line 14). We sample the ampliﬁcation factor for a given\\nband from log-normal distribution (line 12), similar to geometric scaling, and normalize the overall\\ngain so that the total energy is retained on expectation. For the normalization, we assume that the\\nfrequency content obeys 1/f power spectrum typically seen in natural images (line 8). While this\\nassumption is not strictly true in our case, especially when some of the previous frequency bands\\nhave already been ampliﬁed, it is sufﬁcient to keep the output pixel values within reasonable bounds.\\nThe ﬁltering is executed on lines 15–23. We ﬁrst construct a combined ampliﬁcation ﬁlter H′(z)\\n(lines 17–19) and then perform separable convolution for the image using reﬂection padding (lines 21–\\n23). We use a zero-phase ﬁlter bank derived from 4-tap symlets (SYM2) [10]. Denoting the wavelet\\nscaling ﬁlter by H(z), the corresponding bandpass ﬁlters are obtained as follows (line 19):\\nBANDPASS\\n\\x00H(z), b1\\n\\x01\\n=\\nH(z)H(z−1)H(z2)H(z−2)H(z4)H(z−4)/8\\n(3)\\nBANDPASS\\n\\x00H(z), b2\\n\\x01\\n=\\nH(z)H(z−1)H(z2)H(z−2)H(−z4)H(−z−4)/8\\n(4)\\nBANDPASS\\n\\x00H(z), b3\\n\\x01\\n=\\nH(z)H(z−1)H(−z2)H(−z−2)/4\\n(5)\\nBANDPASS\\n\\x00H(z), b4\\n\\x01\\n=\\nH(−z)H(−z−1)/2\\n(6)\\n23\\nFinally, we apply additive RGB noise on lines 24–29 and cutout on lines 30–35. We vary the\\nstrength of the noise by sampling its standard deviation from half-normal distribution, i.e., N(·)\\nrestricted to non-negative values (line 26). For cutout, we match the original implementation of\\nDeVries and Taylor [11] by setting pixels to zero within a rectangular area of size\\n\\x00 w\\n2 , h\\n2\\n\\x01\\n, with the\\ncenter point selected from uniform distribution over the entire image.\\nC\\nNon-leaking augmentations\\nThe goal of GAN training is to ﬁnd a generator function G whose output probability distribution x\\n(under suitable stochastic input) matches a given target distribution y.\\nWhen augmenting both the dataset and the generator output, the key safety principle is that if x and\\ny do not match, then their augmented versions must not match either. If the augmentation pipeline\\nviolates this principle, the generator is free to learn some different output distribution than the dataset,\\nas these look identical after the augmentations – we say that the augmentations leak. Conversely, if\\nthe principle holds, then the only option for the generator is to learn the correct distribution: no other\\nchoice results in a post-augmentation match.\\nIn this section, we study the conditions on the augmentation pipeline under which this holds and\\ndemonstrate the safety and caveats of various common augmentations and their compositions.\\nNotation\\nThroughout this section, we denote probability distributions (and their generalizations)\\nwith lowercase bold-face letters (e.g., x), operators acting on them by calligraphic letters (T ), and\\nvariates sampled from probability distributions by upper-case letters (X).\\nC.1\\nAugmentation operator\\nA very general model for augmentations is as follows. Assume a ﬁxed but arbitrarily complicated non-\\nlinear and stochastic augmentation pipeline. To any image X, it assigns a distribution of augmented\\nimages, such as demonstrated in Figure 2c. This idea is captured by an augmentation operator T\\nthat maps probability distributions to probability distributions (or, informally, datasets to augmented\\ndatasets). A distribution with the lone image X is the Dirac point mass δX, which is mapped to some\\ndistribution T δX of augmented images.3 In general, applying T to an arbitrary distribution x yields\\nthe linear superposition T x of such augmented distributions.\\nIt is important to understand that T is different from a function f(X; φ) that actually applies the\\naugmentation on any individual image X sampled from x (parametrized by some φ, e.g., angle in case\\nof a rotation augmentation). It captures the aggregate effect of applying this function on all images\\nin the distribution and subsumes the randomization of the function parameters. T is always linear\\nand deterministic, regardless of non-linearity of the function f and stochasticity of its parameters\\nφ. We will later discuss invertibility of T . Here it is also critical to note that its invertibility is not\\nequivalent with the invertibility of the function f it is based on; for an example, refer to the discussion\\nin Section 2.2.\\nSpeciﬁcally, T is a (Markov) transition operator.\\nIntuitively, it is an (uncountably) inﬁnite-\\ndimensional generalization of a Markov transition matrix (i.e. a stochastic matrix), with nonnegative\\nentries that sum to 1 along columns. In this analogy, probability distributions upon which T operates\\nare vectors, with nonnegative entries summing to 1. More generally, the distributions have a vector\\nspace structure and they can be arbitrarily linearly combined (in which case they may lose their\\nvalidity as probability distributions and are viewed as arbitrary signed measures). Similarly, we can\\ndo algebra with the with the operators by linearly combining and composing them like matrices.\\nConcepts such as null space and invertibility carry over to this setting, with suitable technical care. In\\nthe following, we will be somewhat informal with the measure theoretical and functional analytic\\ndetails of the problem, and draw upon this analogy as appropriate.4\\n3These distributions are probability measures over a non-discrete high dimensional space: for example, in\\nour experiments with 256 × 256 RGB images, this space is R256∗256∗3 = R196608.\\n4The addition and scalar multiplication of measures is taken to mean that for any set S to which x and y\\nassign a measure, [αx + βy](S) = αx(S) + βy(S). When the measures are represented by density functions,\\nthis simpliﬁes to the usual pointwise linear combination of the functions. We always mean addition and scalar\\n24\\nC.2\\nInvertibility implies non-leaking augmentations\\nWithin this framework, our question can be stated as follows. Given a target distribution y and an\\naugmentation operator T , we train for a generated distribution x such that the augmented distributions\\nmatch, namely\\nT x = T y.\\n(7)\\nThe desired outcome is that this equation is satisﬁed only by the correct target distribution, namely\\nx = y. We say that T leaks if there exist distributions x ̸= y that satisfy the above equation, and the\\ngoal is to ﬁnd conditions that guarantee the absence of leaks.\\nThere are obviously no such leaks in classical non-augmented training, where T is the identity I,\\nwhence T x = T y ⇒Ix = Iy ⇒x = y. For arbitrary augmentations, the desired outcome x = y\\ndoes always satisfy Eq. 7; however, if also other choices of x satisfy it, then it cannot be guaranteed\\nthat the training lands on the desired solution. A trivial example is an augmentation that maps every\\nimage to black (in other words, T z = δ0 for any z). Then, T x = T y does not imply that x = y, as\\nindeed any choice of x produces the same set of black images that satisﬁes Eq. 7. In this case, it is\\nvanishingly unlikely that the training ﬁnds the solution x = y.\\nMore generally, assume that T has a non-trivial null space, namely there exists a signed measure\\nn ̸= 0 such that T n = 0, that is, n is in the null space of T . Equivalently, T is not invertible, because\\nn cannot be recovered from T n. Then, x = y + αn for any α ∈R satisﬁes Eq. 7. Therefore non-\\ninvertibility of T implies that measures in its null space may freely leak into the learned distribution\\n(as long as the sum remains a valid probability distribution that assigns non-negative mass to all sets).\\nConversely, assume that some x ̸= y satisﬁes Eq. 7. Then T (x −y) = T y −T y = 0, so x −y is\\nin null space of T and therefore T is not invertible.\\nTherefore, leaking augmentations imply non-invertibility of the augmentation operator, which con-\\nversely implies the central principle: if the augmentation operator T is invertible, it does not\\nleak. Such a non-leaking operator further satisﬁes the requirements of Lemma 5.1. of Bora et al. [4],\\nwhere the invertibility is shown to imply that a GAN learns the correct distribution.\\nThe invertibility has an intuitive interpretation: the training process can implicitly “undo” the\\naugmentations, as long as probability mass is merely shifted around and not squashed ﬂat.\\nC.3\\nCompositions and mixtures\\nWe only access the operator T indirectly: it is implemented as a procedure, rather than a matrix-like\\nentity whose null space we could study directly (even if we know that such a thing exists in principle).\\nShowing invertibility for an arbitrary procedure is likely to be impossible. Rather, we adopt a\\nconstructive approach, and build our augmentation pipeline from combinations of simple known-safe\\naugmentations, in a way that can be shown to not leak. This calls for two components: a set of\\ncombination rules that preserve the non-leaking guarantee, and a set of elementary augmentations\\nthat have this property. In this subsection we address the former.\\nBy elementary linear algebra: assume T and U are invertible. Then the composition T U is invert-\\nible, as is any ﬁnite chain of such compositions. Hence, sequential composition of non-leaking\\naugmentations is non-leaking. We build our pipeline on this observation.\\nThe other obvious combination of augmentations is obtained by probabilistic mixtures: given\\ninvertible augmentations T and U, perform T with probability α and U with probability 1 −α.\\nThe operator corresponding to this augmentation is the “pointwise” convex blend αT + (1 −α)U.\\nMore generally, one can mix e.g. a continuous family of augmentations Tφ with weights given by\\na non-negative unit-sum function α(φ), as\\nR\\nα(φ)Tφ dφ. Unfortunately, stochastically choosing\\namong a set of augmentations is not guaranteed to preserve the non-leaking property, and must\\nbe analyzed case by case (which is the content of the next subsection). To see this, consider an\\nmultiplication of probability distributions in this sense (as opposed to e.g. addition of random variables), unless\\notherwise noted.\\nTechnically, one can consider the vector space of ﬁnite signed measures on RN, which is a Banach space\\nunder the Total Variation norm. Markov operators form a convex subset of linear operators acting on this space,\\nand general linear combinations thereof form a subspace (and a subalgebra). The exact mathematical conditions\\nunder which some of the following ﬁndings apply may be intricate but have limited practical signiﬁcance given\\nthe approximate nature of GAN training.\\n25\\nextremely simple discrete probability space with only two elements. The augmentation operator\\nT =\\n\\x00 0 1\\n1 0\\n\\x01\\nﬂips the elements. Mixed with probability α = 1\\n2 with the identity augmentation I\\n(which keeps the distribution unchanged), we obtain the augmentation 1\\n2T + 1\\n2I = 1\\n2\\n\\x00 1 1\\n1 1\\n\\x01\\nwhich\\nis a singular matrix and therefore not invertible. Intuitively, this operator smears any probability\\ndistribution into a degenerate equidistribution, from which the original can no longer be recovered.\\nSimilar considerations carry over to arbitrarily complicated linear operators.\\nC.4\\nNon-leaking elementary augmentations\\nIn the following, we construct several examples of relatively large classes of elementary augmentations\\nthat do not leak and can therefore be used to form a chain of augmentations. Importantly, most of\\nthese classes are not inherently safe, as they are stochastic mixtures of even simpler augmentations,\\nas discussed above. However, in many cases we can show that the degenerate situation only arises\\nwith speciﬁc choices of mixture distribution, which we can then avoid.\\nSpeciﬁcally, for every type of augmentation, we identify a conﬁguration where applying it with\\nprobability strictly less than 1 results in an invertible transformation. From the standpoint of this\\nanalysis, we interpret this stochastic skipping as modifying the augmentation operator itself, in a\\nway that boosts the probability of leaving the input unchanged and reduces the probability of other\\noutcomes.\\nC.4.1\\nDeterministic mappings\\nThe simplest form of augmentation is a deterministic mapping, where the operator Tf assigns to\\nevery image X a unique image f(X). In the most general setting f is any measurable function and\\nTfx is the corresponding pushforward measure. When f is a diffeomorphism, Tf acts by the usual\\nchange of variables formula with a density correction by a Jacobian determinant. These mappings are\\ninvertible as long as f itself is invertible. Conversely, if f is not invertible, then neither is Tf.\\nHere it may be instructive to highlight the difference between f and Tf. The former transforms the\\nunderlying space on which the probability distributions live – for example, if we are dealing with\\nimages of just two pixels (with continuous and unconstrained values), f is a nonlinear “warp” of the\\ntwo-dimensional plane. In contrast, Tf operates on distributions deﬁned on this space – think of a\\ncontinuous 2-dimensional function (density) on the aforementioned plane. The action of Tf is to\\nmove the density around according to f, while compensating for thinning and concentration of the\\nmass due to stretching. As long as f maps every distinct point to a distinct point, this warp can be\\nreversed.\\nAn important special case is that where f is a linear transformation of the space. Then the invertibility\\nof Tf becomes a simpler question of the invertibility of a ﬁnite-dimensional matrix that represents f.\\nNote that when an invertible deterministic transformation is skipped probabilistically, the determin-\\nism is lost, and very speciﬁc choices of transformation could result in non-invertibility (see e.g.\\nthe example of ﬂipping above). We only use deterministic mappings as building blocks of other\\naugmentations, and never apply them in isolation with stochastic skipping.\\nC.4.2\\nTransformation group augmentations\\nMany commonly used augmentations are built from transformations that act as a group under\\nsequential composition. Examples of this are ﬂips, translations, rotations, scalings, shears, and many\\ncolor and intensity transformations. We show that a stochastic mixture of transformations within\\na ﬁnitely generated abelian group is non-leaking as long as the mixture weights are chosen from a\\nnon-degenerate distribution.\\nAs an example, the four deterministic augmentations {R0, R90, R180, R270} that rotate the images\\nto every one of the 90-degree increment orientations constitute a group. This is seen by checking\\nthat the set satisﬁes the axiomatic deﬁnition of a group. Speciﬁcally, the set is closed, as composing\\ntwo of elements always results in an element of the same set, e.g. R270R180 = R90. It is also\\nobviously associative, and has an identity element R0 = I. Finally, every element has an inverse, e.g.\\nR−1\\n90 = R270. We can now simply speak of powers of the single generator element, whereby the four\\ngroup elements are written as {R0\\n90, R1\\n90, R2\\n90, R3\\n90} and further (as well as negative) powers “wrap\\nover” to the same elements. This group is isomorphic to Z4, the additive group of integers modulo 4.\\n26\\nA group of rotations is compact due to the wrap-over effect. An example of a non-compact group is\\nthat of translations (with non-periodic boundary conditions): compositions of translations are still\\ntranslations, but one cannot wrap over. Furthermore, more than one generator element can be present\\n(e.g. y-translation in addition to x-translation), but we require that these commute, i.e. the order of\\napplying the transformations must not matter (in which case the group is called abelian).\\nSimilar considerations extend to continuous Lie groups, e.g. that of rotations by any angle; here the\\ngenerating element is replaced by an inﬁnitesimal generator from the corresponding Lie algebra,\\nand the discrete powers by the continuous exponential mapping. For example, continuous rotation\\ntransformations are isomorphic to the group SO(2), or U(1).\\nIn the following subsections show that for ﬁnitely generated abelian groups whose identity el-\\nement matches the identity augmentation, stochastic mixtures of augmentations within the\\ngroup are invertible, as long as the appropriate Fourier transform of the probability distribu-\\ntion over the elements has no zeros.\\nDiscrete compact one-parameter groups\\nWe demonstrate the key points in detail with the simple\\nbut relevant case of a discrete compact one-parameter group and generalize later. Let G be a\\ndeterministic augmentation that generates the ﬁnite cyclic group {Gi}N−1\\ni=0 of order N (e.g. the four\\n90-degree rotations above), such that the element G0 is the identity mapping that leaves its input\\nunchanged.\\nConsider a stochastic augmentation T that randomly applies an element of the group, with the\\nprobability of choosing each element given by the probability vector p ∈RN (where p is nonnegative\\nand sums to 1):\\nT =\\nN−1\\nX\\ni=0\\npiGi\\n(8)\\nTo show the conditions for invertibility of T , we build an operator U that explicitly inverts T , namely\\nUT = I = G0. Whenever this is possible, T is invertible and non-leaking. We build U from the\\nsame group elements with a different weighting5 vector q ∈RN:\\nU =\\nN−1\\nX\\nj=0\\nqjGj\\n(9)\\nWe now seek a vector q for which UT = I, that is, for which U is the desired inverse. Now,\\nUT\\n=\\n N−1\\nX\\ni=0\\npiGi\\n! \\uf8eb\\n\\uf8ed\\nN−1\\nX\\nj=0\\nqjGj\\n\\uf8f6\\n\\uf8f8\\n(10)\\n=\\nN−1\\nX\\ni,j=0\\npiqjGi+j\\n(11)\\nThe powers of the group operation, as well as the indices of the weight vectors, are taken as modulo\\nN due to the cyclic wrap-over of the group element. Collecting the terms that correspond to each Gk\\nin this range and changing the indexing accordingly, we arrive at:\\n=\\nN−1\\nX\\nk=0\\n\"N−1\\nX\\nl=0\\nplqk−l\\n#\\nGk\\n(12)\\n=\\nN−1\\nX\\nk=0\\n[p ⊗q]kGk\\n(13)\\n5Unlike with p, there is no requirement for q to represent a nonnegative probability density that sums to 1, as\\nwe are establishing the general invertibility of T without regard to its probabilistic interpretation. Note that U is\\nnever actually constructed or evaluated when applying our method in practice, and does not need to represent\\nan operation that can be algorithmically implemented; our interest is merely to identify the conditions for its\\nexistence.\\n27\\nwhere we observe that the multiplier in front of each Gk is given by the cyclic convolution of the\\nelements of the vectors p and q. This can be written as a pointwise product in terms of the Discrete\\nFourier Transform F, denoting the DFT’s of p and q by a hat:\\n=\\nN−1\\nX\\nk=0\\n[F−1(ˆp ⊙ˆq)]kGk\\n(14)\\nTo recover the sought after inverse, assuming every element of ˆp is nonzero, we set ˆqi =\\n1\\nˆpi for all i:\\n=\\nN−1\\nX\\nk=0\\n[F−1(ˆp ⊙ˆp−1)]kGk\\n(15)\\n=\\nN−1\\nX\\nk=0\\n[F−11]kGk\\n(16)\\n=\\nG0\\n(17)\\n=\\nI\\n(18)\\nHere, we take advantage of the fact that the inverse DFT of a constant vector of ones is the vector\\n[1, 0, ..., 0].\\nIn summary, the product of U and T effectively computes a convolution between their respective\\ngroup element weights. This convolution assigns all of the weight to the identity element precisely\\nwhen one has ˆqi =\\n1\\nˆpi , for all i, whereby U is the inverse of T . This inverse only exists when the\\nFourier transform ˆpi of the augmentation probability weights has no zeros.\\nThe intuition is that the mixture of group transformations “smears” probability mass among the\\ndifferent transformed versions of the distribution. Analogously to classical deconvolution, this\\nsmearing can be undone (“deconvolved”) as long as the convolution does not destroy any frequencies\\nby scaling them to zero.\\nSome noteworthy consequences of this are:\\n• Assume p is a constant vector 1\\nN 1, that is, the augmentation applies the group elements with\\nuniform probability. In this case ˆp = δ0 and convolution with any zero-mean weight vector\\nis zero. This case is almost certain to cause leaks of the group elements themselves. To see\\nthis directly, the mixed augmentation operator is now T := 1\\nN\\nPN−1\\nj=0 Gj. Consider the true\\ndistribution of training samples y, and a version y′ = Gky into which some element of the\\ntransformation group has leaked. Now,\\nT y′ = T (Gky) = 1\\nN\\nN−1\\nX\\nj=0\\nGjGky = 1\\nN\\nN−1\\nX\\nj=0\\nGj+ky = 1\\nN\\nN−1\\nX\\nj=0\\nGjy = T y\\n(19)\\n(recalling the modulo arithmetic in the group powers). By Eq. 7, this is a leak, and the\\ntraining may equally well learn the distribution Gky rather than y. By the same reasoning,\\nany mixture of transformed elements may be learned (possibly even a different one for each\\nimage).\\n• Similarly, if p is periodic (with period that is some integer factor of N, other than N itself),\\nthe Fourier transform is a sparse sequence of spikes separated by zeros. Another viewpoint\\nto this is that the group has a subgroup, whose elements are chosen uniformly. Similar to\\nabove, this is almost certain to cause leaks with elements of that subgroup.\\n• With more sporadic zero patterns, the leaks can be seen as “conditional”: while the augmen-\\ntation operator has a null space, it is not generally possible to write an equivalent of Eq. 19\\nwithout setting conditions on the distribution y itself. In these cases, leaks only occur for\\nspeciﬁc kinds of distributions, e.g., when a sufﬁcient amount of group symmetry is already\\npresent in the distribution itself.\\nFor example, consider a dataset where all four 90 degree orientations of any image are\\nequally likely, and an augmentation that performs either a 0 or 90 degree rotation at equal\\nprobability. This corresponds to the probability vector p = [0.5, 0.5, 0, 0] over the four\\n28\\nelements of the 90-degree rotation group. This distribution has a single zero in its Fourier\\ntransform. The associated leak might manifest as the generator only learning to produce\\nimages in orientations 0 and 180 degrees, and relying on the augmentation to ﬁll the gaps.\\nSuch a leak could not happen in e.g. a dataset depicting upright faces, and the failure of\\ninvertibility would be harmless in this case. However, this may no longer hold when the\\naugmentation is a part of a composed pipeline, as other augmentations may have introduced\\npartial invariances that were not present in the original data.\\nIn our augmentations involving compact groups (rotations and ﬂips), we always choose the elements\\nwith a uniform probability, but importantly, only perform the augmentation with some probability\\nless than one. This combination can be viewed as increasing the probability of choosing the group\\nidentity element. The probability vector p is then constant, except for having a higher value at p0; the\\nFourier transform of such a vector has no zeros.\\nNon-compact discrete one-parameter groups\\nThe above reasoning can be extended to groups\\nwhich are not compact, in particular translations by integer offsets (without periodic boundaries).\\nIn the discrete case, such a group is necessarily isomorphic to the additive group Z of all integers, and\\nno modulo integer arithmetic is performed. The mixture density is then a two-sided sequence {pi}\\nwith i ∈Z, and the appropriate Fourier transform maps this to a periodic function. By an analogous\\nreasoning with the previous subsection, the invertibility holds as long as this spectrum has no zeros.\\nContinuous one-parameter groups\\nWith suitable technical care, these arguments can be extended\\nto continuous groups with elements Gφ indexed by a continuous parameter φ. In the compact case\\n(e.g. continuous rotation), the group elements wrap over at some period L, such that Gφ+L = Gφ.\\nIn the non-compact case (e.g. translation (addition) and scaling (multiplication) by real-valued\\namounts) no such wrap-over occurs. The compact and non-compact groups are isomorphic to U(1),\\nand the additive group R, respectively. Stochastic mixtures of these group elements are expressed by\\nprobability density functions p(φ), with φ ∈[0, L) if the group is compact, and φ ∈R otherwise.\\nThe Fourier transforms are replaced by the appropriate generalizations, and the invertibility holds\\nwhen the spectrum has no zeros.\\nHere it is important to use the correct parametrization of the group. Note that one could in principle\\nparametrize e.g. rotations in arbitrary ways, and it may seem ambiguous as to what parametrization\\nto use, which would appear to render concepts like uniform distribution meaningless. The issue\\narises when replacing the sums in the earlier formulas with integrals, whereby one needs to choose\\na measure of integration. These ﬁndings apply speciﬁcally to the natural Haar measure and the\\nassociated parametrization – essentially, the measure that accumulates at constant rate when taking\\nsmall steps in the group by applying the inﬁnitesimal generator. For rotation groups, the usual “area”\\nmeasure over the angular parametrization coincides with the Haar measure, and therefore e.g. uniform\\ndistribution is taken to mean that all angles are chosen equally likely. For translation, the natural\\nEuclidian distance is the correct parametrization. For other groups, such as scaling, the choice is a\\nbit more nuanced: when composing scaling operations, the scale factor combines by multiplication\\ninstead of addition, so the natural parametrization is the logarithm of the scale factor.\\nFor continuous compact groups (rotation), we use the same scheme as in the discrete case: uniform\\nprobability mixed with identity at a probability greater than zero.\\nFor continuous non-compact groups, the Fourier transform of the normal distribution has no zeros\\nand results in an invertible augmentation when used to choose among the group elements. Other\\ndistributions with this property are at least the α-stable and more generally the inﬁnitely divisible\\nfamily of distributions. When the parametrization is logarithmic, we may instead use exponentiated\\nvalues from these distributions (e.g. the log-normal distribution). Finally, stochastically mixing\\nzero-mean normal distributed variables with identity does not introduce zeros to the FT, as it merely\\nlifts the already positive values of the spectrum.\\nMulti-parameter abelian groups\\nFinally, these ﬁndings generalize to groups that are products of a\\nﬁnite number of single-parameter groups, provided that the elements of the different groups commute\\n29\\namong each other (in other words, ﬁnitely generated abelian groups). An example of this is the group\\nof 2-dimensional translations obtained by considering x- and y-translations simultaneously.6\\nThe Fourier transforms are replaced with suitable multi-dimensional generalizations, and the proba-\\nbility distributions and their Fourier transforms obtain multidimensional domains accordingly.\\nDiscussion\\nInvertibility is a sufﬁcient condition to ensure the absence of leaks. However, it may\\nnot always be necessary: in the case of non-compact groups, a hypothesis could be made that even a\\ntechnically non-invertible operator does not leak. For example, a shift augmentation with uniform\\ndistributed offset on a continuous interval is not invertible, as the Fourier transform of its density is a\\nsinc function with periodic zeros (except at 0). This only allows for leaks of zero-mean functions\\nwhose FT is supported on this evenly spaced set of frequencies – in other words, inﬁnitely periodic\\nfunctions. Even though such functions are in the null space of the augmentation operator, they\\ncannot be added to any density in an inﬁnite domain without violating non-negativity, and so we\\nmay hypothesize that no leak can in fact occur. In practice, however, the near-zero spectrum values\\nmight allow for a periodic leak modulated by a wide window function to occur for very speciﬁc (and\\npossibly contrived) data distributions.\\nIn contrast, straightforward examples and practical demonstrations of leaks are easily found for\\ncompact groups, e.g. with uniform or periodic rotations.\\nC.4.3\\nNoise and image ﬁlter augmentations\\nWe refer to Theorem 5.3. of Bora et al. [4], where it is shown that in a setting effectively identical to\\nours, addition of noise that is independent of the image is an invertible operation as long as the\\nFourier spectrum of the noise distribution does not contain zeros. The reason is that addition of\\nmutually independent random variables results in a convolution of their probability distributions.\\nSimilar to groups, this is a multiplication in the Fourier domain, and the zeros correspond to\\nirrevocable loss of information, making the inversion impossible. The inverse can be realized by\\n“deconvolution”, or division in the Fourier domain.\\nA potential source of confusion is that the Fourier transform is commonly used to describe spatial\\ncorrelations of noise in signal processing. We refer to a different concept, namely the Fourier\\ntransform of the probability density of the noise, often called the characteristic function in probability\\nliterature (although correlated noise is also subsumed by this analysis).\\nGaussian product noise\\nIn our setting, we also randomize the magnitude parameter of the noise,\\nin effect stochastically mixing between different noise distributions. The above analysis subsumes\\nthis case, as the mixture is also a random noise, with a density that is a weighted blend between the\\ndensities of the base noises. However, the noise is no longer independent across points, so its joint\\ndistribution is no longer separable to a product of marginals, and one must consider the joint Fourier\\ntransform in full dimension.\\nSpeciﬁcally, we draw the per-pixel noise from a normal distribution and modulate this entire noise\\nﬁeld by a multiplication with a single (half-)normal random number. The resulting distribution has\\nan everywhere nonzero Fourier transform and hence is invertible. To see this, ﬁrst consider two\\nstandard normal distributed random scalars X and Y , and their product Z = XY (taken in the\\nsense of multiplying the random variables, not the densities). Then Z is distributed according to\\nthe density pZ(Z) = K0(|Z|)\\nπ\\n, where K0 is a modiﬁed Bessel function, and has the characteristic\\nfunction (Fourier transform) ˆpZ(ω) =\\n1\\n√\\nω2+1, which is everywhere positive [46].\\nThen, considering our situation with a product of a normal distributed scalar X and an independent\\nnormal distributed vector Y ∈RN, the N entries of the product Z = XY become mutually\\ndependent. The marginal distribution of each entry is nevertheless exactly the above product\\ndistribution pZ. By Fourier slice theorem, all one-dimensional slices through the main axes of the\\ncharacteristic function of Z must then coincide with the characteristic function ˆpZ of this marginal\\n6However, for example the non-abelian group of 3-dimensional rotations, SO(3), is not obtained as a product\\nof the single-parameter “Euler angle” rotations along three axes, and therefore is not covered by the present\\nformulation of our theory. The reason is that the three different rotations do not commute. One may of course\\nstill freely compose the three single-parameter rotation augmentations in sequence, but note that the combined\\neffect can only induce a subset of possible probability distributions on SO(3).\\n30\\ndistribution. Finally, because the joint distribution is radially symmetric, so is the characteristic\\nfunction, and this must apply to all slices through the origin, yielding the everywhere positive Fourier\\ntransform ˆpZ(ω) =\\n1\\n√\\n|ω|2+1. When stochastically mixed with identity (as is our random skipping\\nprocedure), the Fourier Transform values are merely lifted towards 1 and no new zero-crossings are\\nintroduced.\\nAdditive noise in transformed bases\\nSimilar notes apply to additive noise in a different basis: one\\ncan consider the noise augmentation as being ﬂanked by an invertible deterministic (possibly also\\nnonlinear) basis transformation and its inverse. It then sufﬁces to show that the additive noise has a\\nnon-zero spectrum in isolation. In particular, multiplicative noise with a non-negative distribution\\ncan be viewed as additive noise in logarithmic space and is invertible if the logarithmic version of the\\nnoise distribution has no zeros in its Fourier transform. The image-space ﬁlters are a combination\\nof a linear basis transformation to the wavelet basis, and additive Gaussian noise under a non-linear\\nlogarithmic transformation.\\nC.4.4\\nRandom projection augmentations\\nThe cutout augmentation (as well as e.g. the pixel and patch blocking in AmbientGAN [4]) can be\\ninterpreted as projecting a random subset of the dimensions to zero.\\nLet P1, P2, ..., PN be a set of deterministic projection augmentation operators with the deﬁning\\nproperty that P2\\nj = Pj. For example, each one of these operators can set a different ﬁxed rectangular\\nregion to zero. Clearly the individual projections have a null space (unless they are the identity\\nprojection) and they are not invertible in isolation.\\nConsider a stochastic augmentation that randomly applies one of these projections, or the identity.\\nLet p0, p1, ..., pN denote the discrete probabilities of choosing the identity operator I for p0, and\\nPk for the remaining pk. Deﬁne the mixture of the projections as:\\nT = p0I +\\nN\\nX\\nj=1\\npjPj\\n(20)\\nAgain, T is a mixture of operators, however unlike in earlier examples, some (but not all) of the\\noperators are non-invertible. Under what conditions on the probability distribution p is T invertible?\\nAssume that T is not invertible, i.e. there exists a probability distribution x ̸= 0 such that T x = 0.\\nThen\\n0 = T x = p0x +\\nN\\nX\\nj=1\\npjPjx\\n(21)\\nand rearranging,\\nN\\nX\\nj=1\\npjPjx = −p0x\\n(22)\\nUnder reasonable technical assumptions (e.g. discreteness of the pixel intensity values, such as\\njustiﬁed in Theorem 5.4. of Bora et al. [4]), we can consider the inner product of both sides of this\\nequation with x:\\nN\\nX\\nj=1\\npj⟨x, Pjx⟩= −p0⟨x, x⟩\\n(23)\\nThe right side of this equation is strictly negative if the probability p0 of identity is greater than\\nzero, as x ̸= 0. The left side is a non-negative sum of non-negative terms, as the inner product\\nof a vector with its projection is non-negative. Therefore, the assumption leads to a contradiction\\nunless p0 = 0; conversely, random projection augmentation does not leak if there is a non-zero\\nprobability that it produces the identity.\\n31\\nC.5\\nPractical considerations\\nC.5.1\\nConditioning\\nIn practical numerical computation, an operator that is technically invertible may nevertheless be so\\nclose to a non-invertible conﬁguration that inversion fails in practice. Assuming a ﬁnite state space,\\nthis notion is captured by the condition number, which is inﬁnite when the matrix is singular, and\\nlarge when it is singular for all practical purposes. The same consideration applies to inﬁnite state\\nspaces, but the appropriate technical notion of conditioning is less clear.\\nThe practical value of the analysis in this section is in identifying the conditions where exact non-\\ninvertibility happens, so that appropriate safety margin can be kept. We achieve this by regulating the\\nprobability p of performing a given augmentation, and keeping it at a safe distance from p = 1 which\\nfor many of the augmentations corresponds to a non-invertible condition (e.g. uniform distribution\\nover compact group elements).\\nFor example, consider applying transformations from a ﬁnite group with a uniform probability\\ndistribution, where the augmentation is applied with probability p. In a ﬁnite state space, a matrix\\ncorresponding to this augmentation has 1 −p for its smallest singular value, and 1 for the largest,\\nresulting in condition number 1/(1 −p) which approaches inﬁnity as p approaches one.\\nC.5.2\\nPixel-level effects and boundaries\\nWhen dealing with images represented on ﬁnite pixel grids, naive practical implementations of some\\nof the group transformations do not strictly speaking form groups. For example, a composition of\\ntwo continuous rotations of an image with angles φ and θ does not generally reproduce the same\\nimage as a single rotation by angle φ + θ, if the transformed image is resampled to the rectangular\\npixel grid twice. Furthermore, parts of the image may fall outside the boundaries of the grid, whereby\\ntheir values are lost and cannot be restored even if a reverse transformation is made afterwards,\\nunless special care is taken. These effects may become signiﬁcant when multiple transformations are\\ncomposed.\\nIn our implementation, we mitigate these issues as much as possible by accumulating the chain of\\ntransformations into a matrix and a vector representing the total afﬁne transformation implemented\\nby all the grouped augmentations, and only then applying it on the image. This is possible because\\nall the augmentations we use are afﬁne transformations in the image (or color) space. Furthermore,\\nprior to applying the geometric transformations, the images are reﬂection padded and scaled to\\ndouble resolution (and conversely, cropped and downscaled afterwards). Effectively the image is\\nthen treated as an inﬁnite tiling of suitably reﬂected ﬁner-resolution copies of itself, and a practical\\ntarget-resolution crop is only sampled at augmentation time.\\nD\\nImplementation details\\nWe implemented our techniques on top of the StyleGAN2 ofﬁcial TensorFlow implementation7. We\\nkept most of the details unchanged, including network architectures [21], weight demodulation [21],\\npath length regularization [21], lazy regularization [21], style mixing regularization [20], bilinear\\nﬁltering in all up/downsampling layers [20], equalized learning rate for all trainable parameters [19],\\nminibatch standard deviation layer at the end of the discriminator [19], exponential moving average\\nof generator weights [19], non-saturating logistic loss [14] with R1 regularization [30], and Adam\\noptimizer [24] with β1 = 0, β2 = 0.99, and ϵ = 10−8.\\nWe ran our experiments on a computing cluster with a few dozen NVIDIA DGX-1s, each containing\\n8 Tesla V100 GPUs, using TensorFlow 1.14.0, PyTorch 1.1.0 (for comparison methods), CUDA 10.0,\\nand cuDNN 7.6.3. We used the ofﬁcial pre-trained Inception network8 to compute FID, KID, and\\nInception score.\\n32\\nParameter\\nStyleGAN2\\nconﬁg F\\nOur\\nbaseline\\nBreCaHAD,\\nAFHQ\\nMetFaces\\nCIFAR-10\\n+ Tuning\\nResolution\\n1024×1024\\n256×256\\n512×512\\n1024×1024\\n32×32\\n32×32\\nNumber of GPUs\\n8\\n8\\n8\\n8\\n2\\n2\\nTraining length\\n25M\\n25M\\n25M\\n25M\\n100M\\n100M\\nMinibatch size\\n32\\n64\\n64\\n32\\n64\\n64\\nMinibatch stddev\\n4\\n8\\n8\\n4\\n32\\n32\\nDataset x-ﬂips\\n✓/ –\\n–\\n✓\\n✓\\n–\\n–\\nFeature maps\\n1×\\n1\\n2×\\n1×\\n1×\\n512\\n512\\nLearning rate η×103\\n2\\n2.5\\n2.5\\n2\\n2.5\\n2.5\\nR1 regularization γ\\n10\\n1\\n0.5\\n2\\n0.01\\n0.01\\nG moving average\\n10k\\n20k\\n20k\\n10k\\n500k\\n500k\\nMixed-precision\\n–\\n✓\\n✓\\n✓\\n✓\\n✓\\nMapping net depth\\n8\\n8\\n8\\n8\\n8\\n2\\nStyle mixing reg.\\n✓\\n✓\\n✓\\n✓\\n✓\\n–\\nPath length reg.\\n✓\\n✓\\n✓\\n✓\\n✓\\n–\\nResnet D\\n✓\\n✓\\n✓\\n✓\\n✓\\n–\\nFigure 24: Hyperparameters used in each experiment.\\nD.1\\nHyperparameters and training conﬁgurations\\nFigure 24 shows the hyperparameters that we used in our experiments, as well as the original\\nStyleGAN2 conﬁg F [21]. We performed all training runs using 8 GPUs and continued the training\\nuntil the discriminator had seen a total of 25M real images, except for CIFAR-10, where we used\\n2 GPUs and 100M images. We used minibatch size of 64 when possible, but reverted to 32 for\\nMETFACES in order to avoid running out of GPU memory. Similar to StyleGAN2, we evaluated the\\nminibatch standard deviation layer independently over the images processed by each GPU.\\nDataset augmentation\\nWe did not use dataset augmentation in any of our experiments with\\nFFHQ, LSUN CAT, or CIFAR-10, except for the FFHQ-140k case and in Figure 20. In particular,\\nwe feel that leaky augmentations are inappropriate for CIFAR-10 given its status as a standard\\nbenchmark dataset, where dataset/leaky augmentations would unfairly inﬂate the results. METFACES,\\nBRECAHAD, and AFHQ DOG are horizontally symmetric in nature, so we chose to enable dataset\\nx-ﬂips for these datasets to maximize result quality.\\nNetwork capacity\\nWe follow the original StyleGAN2 conﬁguration for high-resolution datasets\\n(≥5122): a layer operating on N = w × h pixels uses min\\n\\x00216/\\n√\\nN, 512\\n\\x01\\nfeature maps. With\\nCIFAR-10 we use 512 feature maps for all layers. In the 256 × 256 conﬁguration used with FFHQ\\nand LSUN CAT, we facilitate extensive sweeps over dataset sizes by decreasing the number of\\nfeature maps to min\\n\\x00215/\\n√\\nN, 512\\n\\x01\\n.\\nLearning rate and weight averaging\\nWe selected the optimal learning rates using grid search and\\nfound that it is generally beneﬁcial to use the highest learning rate that does not result in training\\ninstability. We also found that larger minibatch size allows for a slightly higher learning rate. For the\\nmoving average of generator weights [19], the natural choice is to parameterize the decay rate with\\nrespect to minibatches — not individual images — so that increasing the minibatch size results in a\\nlonger decay. Furthermore, we observed that a very long moving average consistently gave the best\\nresults on CIFAR-10. To reduce startup bias, we linearly ramp up the length parameter from 0 to\\n500k over the ﬁrst 10M images.\\nR1 regularization\\nKarras et al. [21] postulated that the best choice for the R1 regularization weight\\nγ is highly dependent on the dataset. We thus performed extensive grid search for each column\\n7https://github.com/NVlabs/stylegan2\\n8http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\\n33\\nin Figure 24, considering γ ∈{0.001, 0.002, 0.005, . . . , 20, 50, 100}. Although the optimal γ does\\nvary wildly, from 0.01 to 10, it seems to scale almost linearly with the resolution of the dataset. In\\npractice, we have found that a good initial guess is given by γ0 = 0.0002 · N/M, where N = w × h\\nis the number of pixels and M is the minibatch size. Nevertheless, the optimal value of γ tends to\\nvary depending on the dataset, so we recommend experimenting with different values in the range\\nγ ∈[γ0/5, γ0 · 5].\\nMixed-precision training\\nWe utilize the high-performance Tensor Cores available in Volta-class\\nGPUs by employing mixed-precision FP16/FP32 training in all of our experiments (with two ex-\\nceptions, discussed in Appendix D.2). We store the trainable parameters with full FP32 precision\\nfor the purposes of optimization but cast them to FP16 before evaluating G and D. The main\\nchallenge with mixed-precision training is that the numerical range of FP16 is limited to ∼±216, as\\nopposed to ∼±2128 for FP32. Thus, any unexpected spikes in signal magnitude — no matter how\\ntransient — will immediately collapse the training dynamics. We found that the risk of such spikes\\ncan be reduced drastically using three tricks: ﬁrst, by limiting the use of FP16 to only the 4 highest\\nresolutions, i.e., layers for which Nlayer ≥Ndataset/(2 × 2)4; second, by pre-normalizing the style\\nvector s and each row of the weight tensor w before applying weight modulation and demodulation9;\\nand third, by clamping the output of every convolutional layer to ±28, i.e., an order of magnitude\\nwider range than is needed in practice. We observed about 60% end-to-end speedup from using FP16\\nand veriﬁed that the results were virtually identical to FP32 on our baseline conﬁguration.\\nCIFAR-10\\nWe enable class-conditional image generation on CIFAR-10 by extending the original\\nStyleGAN2 architecture as follows. For the generator, we embed the class identiﬁer into a 512-\\ndimensional vector that we concatenate with the original latent code after normalizing each, i.e.,\\nz′ = concat\\n\\x00norm(z), norm(embed(c))\\n\\x01\\n, where c is the class identiﬁer. For the discriminator, we\\nfollow the approach of Miyato and Koyama [32] by evaluating the ﬁnal discriminator output as\\nD(x) = norm\\n\\x00embed(c)\\n\\x01\\n· D′(x)T , where D′(x) corresponds to the feature vector produced by the\\nlast layer of D. To compute FID, we generate 50k images using randomly selected class labels and\\ncompare their statistics against the 50k images from the training set. For IS, we compute the mean\\nover 10 independent trials using 5k generated images per trial. As illustrated in Figures 11b and 24,\\nwe found that we can improve the FID considerably by disabling style mixing regularization [20],\\npath length regularization [21], and residual connections in D [21]. Note that all of these features are\\nhighly beneﬁcial on higher-resolution datasets such as FFHQ. We ﬁnd it somewhat alarming that\\nthey have precisely the opposite effect on CIFAR-10 — this suggests that some previous conclusions\\nreached in the literature using CIFAR-10 may fail to generalize to other datasets.\\nD.2\\nComparison methods\\nWe implemented the comparison methods shown in Figures 8a on top of our baseline conﬁgura-\\ntion, identifying the best-performing hyperparameters for each method via extensive grid search.\\nFurthermore, we inspected the resulting network weights and training dynamics in detail to verify\\ncorrect behavior, e.g., that with the discriminator indeed learns to correctly handle the auxiliary tasks\\nwith PA-GAN and auxiliary rotations. We found zCR and WGAN-GP to be inherently incompatible\\nwith our mixed-precision training setup due to their large variation in gradient magnitudes. We\\nthus reverted to full-precision FP32 for these methods. Similarly, we found lazy regularization\\nto be incompatible with bCR, zCR, WGAN-GP, and auxiliary rotations. Thus, we included their\\ncorresponding loss terms directly into our main training loss, evaluated on every minibatch.\\nbCR\\nWe implement balanced consistency regularization proposed by Zhao et al. [53] by introducing\\ntwo new loss terms as shown in Figure 2a. We set λreal = λfake = 10 and use integer translations on\\nthe range of [−8, +8] pixels. In Figure 20, we also perform experiments with x-ﬂips and arbitrary\\nrotations.\\nzCR\\nIn addition to bCR, Zhao et al. [53] also propose latent consistency regularization (zCR) to\\nimprove the diversity of the generated images. We implement zCR by perturbing each component of\\nthe latent z by σnoise = 0.1 and encouraging the generator to maximize the L2 difference between the\\n9Note that our pre-normalization only affects the intermediate results; it has no effect on the ﬁnal output of\\nthe convolution layer due to the subsequent post-normalization performed by weight demodulation.\\n34\\ngenerated images, measured as an average over the pixels, with weight λgen = 0.02. Similarly, we\\nencourage the discriminator to minimize the L2 difference in D(x) with weight λdis = 0.2.\\nPA-GAN\\nZhang and Khoreva [48] propose to reduce overﬁtting by requiring the discriminator\\nto learn an auxiliary checksum task. This is done by providing a random bit string as additional\\ninput to D, requiring that the sign of the output is ﬂipped based on the parity of bits that were set,\\nand dynamically increasing the number of bits when overﬁtting is detected. We select the number\\nof bits using our rt heuristic with target 0.95. Given the value of p produced by the heuristic, we\\ncalculate the number of bits as k = ⌈p · 16⌉. Similar to Zhang and Khoreva, we fade in the effect\\nof newly added bits smoothly over the course of training. In practice, we use a ﬁxed string of\\n16 bits, where the ﬁrst k −1 bits are sampled from Bernoulli(0.5), the kth bit is sampled from\\nBernoulli\\n\\x00min(p · 16 −k + 1, 0.5)\\n\\x01\\n, and the remaining 16 −k bits are set to zero.\\nWGAN-GP\\nFor WGAN-GP, proposed by Gulrajani et al. [15], we reuse the existing implementa-\\ntion included in the StyleGAN2 codebase with λ = 10. We found WGAN-GP to be quite unstable\\nin our baseline conﬁguration, which necessitated us to disable mixed-precision training and lazy\\nregularization, as well as to settle for a considerably lower learning rate η = 0.0010.\\nAuxiliary rotations\\nChen et al. [6] propose to improve GAN training by introducing an auxiliary\\nrotation loss for G and D. In addition the main training objective, the discriminator is shown real\\nimages augmented with 90◦rotations and asked to detect their correct orientation. Similarly, the\\ngenerator is encouraged to produce images whose orientation is easy for the discriminator to detect\\ncorrectly. We implement this method by introducing two new loss terms that are evaluated on a 4×\\nlarger minibatch, consisting of rotated versions of the images shown to the discriminator as a part of\\nthe main loss. We extend the last layer of D to output 5 scalar values instead of one and interpret the\\nlast 4 components as raw logits for softmax cross-entropy loss. We weight the additional loss terms\\nusing α = 10 for G, and β = 5 for D.\\nSpectral normalization\\nMiyato et al. [31] propose to regularize the discriminator by explicitly\\nenforcing an upper bound for its Lipschitz constant, and several follow-up works [49, 5, 53, 38] have\\nfound it to be beneﬁcial. Given that spectral normalization is effectively a no-op when applied to\\nthe StyleGAN2 generator [21], we apply it only to the discriminator. We ported the original Chainer\\nimplementation10 to TensorFlow, and applied it to the main convolution layers of D. We found it\\nbeneﬁcial to not use spectral normalization with the FromRGB layer, residual skip connections, or the\\nlast fully-connected layer.\\nFreeze-D\\nMo et al. [33] propose to freeze the ﬁrst k layers of the discriminator to improve results\\nwith transfer learning. We tested several different choices for k; the best results were given by k = 10\\nin Figure 9 and by k = 13 in Figure 11b. In practice, this corresponds to freezing all layers operating\\nat the 3 or 4 highest resolutions, respectively.\\nBigGAN\\nBigGAN results in Figures 19 and 18 were run on a modiﬁed version of the original\\nBigGAN PyTorch implementation11. The implementation was adapted for unconditional operation\\nfollowing Schönfeld et al. [38] by matching their hyperparameters, replacing class-conditional\\nBatchNorm with self-modulation, where the BatchNorm parameters are conditioned only on the\\nlatent vector z, and not using class projection in the discriminator.\\nMapping network depth\\nFor the “Shallow mapping” case in Figure 8a, we reduced the depth of\\nthe mapping network from 8 to 2. Reducing the depth further than 2 yielded consistently inferior\\nresults, conﬁrming the usefulness of the mapping network. In general, we found depth 2 to yield\\nslightly better results than depth 8, making it a good default choice for future work.\\nAdaptive dropout\\nDropout [42] is a well-known technique for combating overﬁtting in practically\\nall areas of machine learning. In Figure 8a, we employ multiplicative Gaussian dropout for all layers\\nof the discriminator, similar to the approach employed by Karras et al. [19] in the context of LSGAN\\n10https://github.com/pfnet-research/sngan_projection\\n11https://github.com/ajbrock/BigGAN-PyTorch\\n35\\nloss [28]. We adjust the standard deviation dynamically using our rt heuristic with target 0.6, so that\\nthe resulting p is used directly as the value for σ.\\nD.3\\nMetFaces dataset\\nWe have collected a new dataset, MetFaces, by extracting images of human faces from the Metropoli-\\ntan Museum of Art online collection. Dataset images were searched using terms such as ‘paintings’,\\n‘watercolor’ and ‘oil on canvas’, and downloaded via the https://metmuseum.github.io/ API. This\\nresulted in a set of source images that depicted paintings, drawings, and statues. Various automated\\nheuristics, such as face detection and image quality metrics, were used to narrow down the set\\nof images to contain only human faces. A manual selection pass over the remaining images was\\nperformed to weed out poor quality images not caught by automated ﬁltering. Finally, faces were\\ncropped and aligned to produce 1,336 high quality images at 10242 resolution.\\nThe whole dataset, including the unprocessed images, is available at\\nhttps://github.com/NVlabs/metfaces-dataset\\nE\\nEnergy consumption\\nComputation is a core resource in any machine learning project: its availability and cost, as well as\\nthe associated energy consumption, are key factors in both choosing research directions and practical\\nadoption. We provide a detailed breakdown for our entire project in Table 25 in terms of both GPU\\ntime and electricity consumption. We report expended computational effort as single-GPU years\\n(Volta class GPU). We used a varying number of NVIDIA DGX-1s for different stages of the project,\\nand converted each run to single-GPU equivalents by simply scaling by the number of GPUs used.\\nWe followed the Green500 power measurements guidelines [12] similarly to Karras et al. [21]. The\\nentire project consumed approximately 300 megawatt hours (MWh) of electricity. Almost half of\\nthe total energy was spent on exploration and shaping the ideas before the actual paper production\\nstarted. Subsequently the majority of computation was targeted towards the extensive sweeps shown\\nin various ﬁgures. Given that ADA does not signiﬁcantly affect the cost of training a single model,\\ne.g., training StyleGAN2 [21] with 1024 × 1024 FFHQ still takes approximately 0.7 MWh.\\n36\\nItem\\nNumber of\\nGPU years\\nElectricity\\ntraining runs\\n(Volta)\\n(MWh)\\nEarly exploration\\n253\\n22.65\\n52.05\\nPaper exploration\\n1116\\n36.54\\n87.39\\nSetting up the baselines\\n251\\n12.19\\n30.70\\nPaper ﬁgures\\n960\\n50.53\\n108.02\\nFig.1\\nBaseline convergence\\n21\\n1.01\\n2.27\\nFig.3\\nLeaking behavior\\n78\\n3.62\\n7.93\\nFig.4\\nAugmentation categories\\n90\\n4.45\\n9.40\\nFig.5\\nADA heuristics\\n61\\n3.16\\n6.87\\nFig.6\\nADA convergence\\n15\\n0.78\\n1.70\\nFig.7\\nTraining set sweeps\\n174\\n10.82\\n22.70\\nFig.8a\\nComparison methods\\n69\\n4.18\\n8.64\\nFig.8b\\nDiscriminator capacity\\n144\\n7.70\\n15.93\\nFig.9\\nTransfer learning\\n40\\n0.71\\n1.67\\nFig.11a\\nSmall datasets\\n30\\n1.71\\n4.15\\nFig.11b\\nCIFAR-10\\n30\\n0.93\\n2.71\\nFig.19\\nBigGAN comparison\\n54\\n3.34\\n7.12\\nFig.20\\nbCR leaks\\n40\\n2.19\\n4.57\\nFig.21\\nCumulative augmentations\\n114\\n5.93\\n12.36\\nResults intentionally left out\\n177\\n5.51\\n11.78\\nWasted due to technical issues\\n255\\n3.86\\n8.39\\nCode release\\n375\\n12.49\\n26.71\\nTotal\\n3387\\n143.76\\n325.06\\nFigure 25: Computational effort expenditure and electricity consumption data for this project. The unit\\nfor computation is GPU-years on a single NVIDIA V100 GPU — it would have taken approximately\\n135 years to execute this project using a single GPU. See the text for additional details about the\\ncomputation and energy consumption estimates. Early exploration includes all training runs that\\naffected our decision to start this project. Paper exploration includes all training runs that were done\\nspeciﬁcally for this project, but were not intended to be used in the paper as-is. Setting up the baselines\\nincludes all hyperparameter tuning for the baselines. Figures provides a per-ﬁgure breakdown, and\\nunderlines that just reproducing all the ﬁgures would require over 50 years of computation on a single\\nGPU. Results intentionally left out includes additional results that were initially planned, but then left\\nout to improve focus and clarity. Wasted due to technical issues includes computation wasted due\\nto code bugs and infrastructure issues. Code release covers testing and benchmarking related to the\\npublic release.\\n37\\n')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articleloader = ArxivLoader(query=\"2006.06676\")\n",
    "articledoc = articleloader.load()\n",
    "articledoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOAD WIKIPEDIA DOC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_loader = WikipediaLoader(query=\"Nigeria economy\", load_max_docs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Economy of Nigeria', 'summary': \"The economy of Nigeria is a middle-income, mixed economy and emerging market with expanding manufacturing, financial, service, communications, technology, and entertainment sectors. It is ranked as the 53rd-largest economy in the world in terms of nominal GDP, the fourth largest in Africa and the 27th-largest in terms of purchasing power parity.\\nThe country's re-emergent manufacturing sector became the largest on the continent in 2013, and it produces a large proportion of goods and services for the region of West Africa. Nigeria's debt-to-GDP ratio was 36.63% in 2021 according to the IMF.\\nAlthough oil revenues contributed 2/3 of state revenues, oil only contributes about 9% to the GDP. Nigeria produces only about 2.7% of the world's oil supply. Although the petroleum sector is important, as government revenues still heavily rely on this sector, it remains a small part of the country's overall economy. The largely subsistence agricultural sector has not kept up with the country's rapid population growth. Nigeria was once a large net exporter of food, but currently imports some of its food products. Mechanization has led to a resurgence in the manufacturing and exporting of food products, and there was consequently a move towards food sufficiency. In 2006, Nigeria came to an agreement with the Paris Club to buy back the bulk of its owed debts from them, in exchange for a cash payment of roughly US$12 billion.\\nAccording to a Citigroup report published in February 2011, Nigeria would have the highest average GDP growth in the world between 2010 and 2050. Nigeria is one of two countries from Africa among the 11 Global Growth Generators countries.\", 'source': 'https://en.wikipedia.org/wiki/Economy_of_Nigeria'}, page_content=\"The economy of Nigeria is a middle-income, mixed economy and emerging market with expanding manufacturing, financial, service, communications, technology, and entertainment sectors. It is ranked as the 53rd-largest economy in the world in terms of nominal GDP, the fourth largest in Africa and the 27th-largest in terms of purchasing power parity.\\nThe country's re-emergent manufacturing sector became the largest on the continent in 2013, and it produces a large proportion of goods and services for the region of West Africa. Nigeria's debt-to-GDP ratio was 36.63% in 2021 according to the IMF.\\nAlthough oil revenues contributed 2/3 of state revenues, oil only contributes about 9% to the GDP. Nigeria produces only about 2.7% of the world's oil supply. Although the petroleum sector is important, as government revenues still heavily rely on this sector, it remains a small part of the country's overall economy. The largely subsistence agricultural sector has not kept up with the country's rapid population growth. Nigeria was once a large net exporter of food, but currently imports some of its food products. Mechanization has led to a resurgence in the manufacturing and exporting of food products, and there was consequently a move towards food sufficiency. In 2006, Nigeria came to an agreement with the Paris Club to buy back the bulk of its owed debts from them, in exchange for a cash payment of roughly US$12 billion.\\nAccording to a Citigroup report published in February 2011, Nigeria would have the highest average GDP growth in the world between 2010 and 2050. Nigeria is one of two countries from Africa among the 11 Global Growth Generators countries.\\n\\n\\n== Overview ==\\nIn 2014, Nigeria rebased its GDP to account for fast-growing contributors such as telecommunications, banking, and its film industry to its economy.[1]  Human capital is underdeveloped, as Nigeria ranked 161 out of 189 countries in the United Nations Development Index in 2019—and non-energy-related infrastructure is inadequate.\\nNigeria has advanced efforts to provide universal primary education, and protect the environment.\\nA requirement for achieving many of its objectives is reducing endemic corruption, which obstructs development and stains Nigeria's business environment. However, while broad-based progress has been slow, these efforts are becoming visible in international surveys of corruption. Nigeria's ranking has mostly improved since 2001, ranking 154 out of 180 countries in Transparency International's 2021 Corruption Perceptions Index.\\nThe Nigerian economy suffers from an ongoing supply crisis in the power sector. Despite a rapidly growing economy, some of the world's largest deposits of coal, oil, and gas and the country's status as Africa's largest oil producer, power supply difficulties are frequently experienced by residents. Two-thirds of Nigerians expect living conditions to improve in the coming decades. According to the National Bureau of Statistics (NBS) Nigeria's GDP grew by 3.19% in Q2 2024. The non-oil sector drove growth, expanding by 4.13%, while the oil sector contracted by -3.83%.\\n\\n\\n== Economic history ==\\n\\nThe following table shows the main economic indicators in 1980–2021 (with IMF staff estimates in 2022–2027). Inflation under 10% is in green. The annual unemployment rate from 1991 to 2009 (in italic) is extracted from the World Bank, although the International Monetary Fund find them unreliable.\\n\\nNOTES:\\nThe US dollar exchange rate is an estimated average of the official rate throughout a year and does not reflect the parallel market rate at which the general population accesses foreign exchange. This rate ranged from a high of 520 in March 2017 to a low of 350 in August 2017, due to a scarcity of forex (oil earnings had dropped by half), and speculative activity as alleged by the Central Bank. All the while the official rate was pegged at 360.\\nPer capita income (as % of USA) is calculated using data from estimates in the PPP link above, and ce\"),\n",
       " Document(metadata={'title': 'Nigeria', 'summary': \"Nigeria, officially the Federal Republic of Nigeria, is a country in West Africa. It is situated between the Sahel to the north and the Gulf of Guinea in the Atlantic Ocean to the south. It covers an area of 923,769 square kilometres (356,669 sq mi). With a population of more than 230 million, it is the most populous country in Africa, and the world's sixth-most populous country. Nigeria borders Niger in the north, Chad in the northeast, Cameroon in the east, and Benin in the west. Nigeria is a federal republic comprising 36 states and the Federal Capital Territory, where its capital, Abuja, is located. The largest city in Nigeria is Lagos, one of the largest metropolitan areas in the world and the largest in Africa.\\nNigeria has been home to several indigenous material cultures, pre-colonial states and kingdoms since the second millennium BC. The Nok culture, c.\\u20091500 BC, marks one of the earliest known civilizations in the region. The Hausa Kingdoms inhabited the north, with the Edo Kingdom of Benin in the south and Igbo Kingdom of Nri in the southeast. In the southwest, the Yoruba Ife Empire was succeeded by the Oyo Empire. The present day territory of Nigeria was home to a vast array of city-states.:\\u200a136\\u200a In the early 19th century the Fula jihads culminated in the Sokoto Caliphate. The modern state originated with British colonialization in the 19th century, taking its present territorial shape with the merging of the Southern Nigeria Protectorate and the Northern Nigeria Protectorate in 1914. The British set up administrative and legal structures and incorporated traditional monarchs as a form of indirect rule. Nigeria became a formally independent federation on 1 October 1960. It experienced a civil war from 1967 to 1970, followed by a succession of military dictatorships and democratically elected civilian governments until achieving a stable government in the 1999 Nigerian presidential election. \\nNigeria is a multinational state inhabited by more than 250 ethnic groups speaking 500 distinct languages, all identifying with a wide variety of cultures. The three largest ethnic groups are the Hausa in the north, Yoruba in the west, and Igbo in the east, together constituting over 60% of the total population. The official language is English, chosen to facilitate linguistic unity at the national level. Nigeria's constitution ensures de jure freedom of religion, and it is home to some of the world's largest Muslim and Christian populations. Nigeria is divided roughly in half between Muslims, who live mostly in the north part of the country, and Christians, who live mostly in the south; indigenous religions, such as those native to the Igbo and Yoruba ethnicities, are in the minority.\\nNigeria is a regional power in Africa and a middle power in international affairs. Nigeria's economy is the fourth-largest in Africa, the 53rd-largest in the world by nominal GDP, and 27th-largest by PPP. Nigeria is often referred to as the Giant of Africa by its citizens due to its large population and formerly large economy, and is considered to be an emerging market by the World Bank. Nigeria is a founding member of the African Union and a member of many international organizations, including the United Nations, the Commonwealth of Nations, NAM, the Economic Community of West African States, Organisation of Islamic Cooperation and OPEC. It is also a member of the informal MINT group of countries and is one of the Next Eleven economies.\", 'source': 'https://en.wikipedia.org/wiki/Nigeria'}, page_content=\"Nigeria, officially the Federal Republic of Nigeria, is a country in West Africa. It is situated between the Sahel to the north and the Gulf of Guinea in the Atlantic Ocean to the south. It covers an area of 923,769 square kilometres (356,669 sq mi). With a population of more than 230 million, it is the most populous country in Africa, and the world's sixth-most populous country. Nigeria borders Niger in the north, Chad in the northeast, Cameroon in the east, and Benin in the west. Nigeria is a federal republic comprising 36 states and the Federal Capital Territory, where its capital, Abuja, is located. The largest city in Nigeria is Lagos, one of the largest metropolitan areas in the world and the largest in Africa.\\nNigeria has been home to several indigenous material cultures, pre-colonial states and kingdoms since the second millennium BC. The Nok culture, c.\\u20091500 BC, marks one of the earliest known civilizations in the region. The Hausa Kingdoms inhabited the north, with the Edo Kingdom of Benin in the south and Igbo Kingdom of Nri in the southeast. In the southwest, the Yoruba Ife Empire was succeeded by the Oyo Empire. The present day territory of Nigeria was home to a vast array of city-states.:\\u200a136\\u200a In the early 19th century the Fula jihads culminated in the Sokoto Caliphate. The modern state originated with British colonialization in the 19th century, taking its present territorial shape with the merging of the Southern Nigeria Protectorate and the Northern Nigeria Protectorate in 1914. The British set up administrative and legal structures and incorporated traditional monarchs as a form of indirect rule. Nigeria became a formally independent federation on 1 October 1960. It experienced a civil war from 1967 to 1970, followed by a succession of military dictatorships and democratically elected civilian governments until achieving a stable government in the 1999 Nigerian presidential election. \\nNigeria is a multinational state inhabited by more than 250 ethnic groups speaking 500 distinct languages, all identifying with a wide variety of cultures. The three largest ethnic groups are the Hausa in the north, Yoruba in the west, and Igbo in the east, together constituting over 60% of the total population. The official language is English, chosen to facilitate linguistic unity at the national level. Nigeria's constitution ensures de jure freedom of religion, and it is home to some of the world's largest Muslim and Christian populations. Nigeria is divided roughly in half between Muslims, who live mostly in the north part of the country, and Christians, who live mostly in the south; indigenous religions, such as those native to the Igbo and Yoruba ethnicities, are in the minority.\\nNigeria is a regional power in Africa and a middle power in international affairs. Nigeria's economy is the fourth-largest in Africa, the 53rd-largest in the world by nominal GDP, and 27th-largest by PPP. Nigeria is often referred to as the Giant of Africa by its citizens due to its large population and formerly large economy, and is considered to be an emerging market by the World Bank. Nigeria is a founding member of the African Union and a member of many international organizations, including the United Nations, the Commonwealth of Nations, NAM, the Economic Community of West African States, Organisation of Islamic Cooperation and OPEC. It is also a member of the informal MINT group of countries and is one of the Next Eleven economies.\\n\\n\\n== Etymology ==\\nThe name Nigeria derives from the Niger River running through the country. This name was coined on 8 January 1897, by the British journalist Flora Shaw. The neighboring Republic of Niger takes its name from the same river. The origin of the name Niger, which originally applied to only the middle reaches of the Niger River, is uncertain. The word is likely an alteration of the Tuareg name egerew n-igerewen used by inhabitants along the middle reaches of the river around Timbuktu before 19th-centu\")]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_result =wikipedia_loader.load()\n",
    "wikipedia_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
